{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"116a3jzRpm6h4-tHmCBRoBPLcSp_zdfgd","authorship_tag":"ABX9TyNt++XzQqRiodpk6RTqPv+k"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"zac9SDo3iucl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686412343539,"user_tz":-180,"elapsed":16363,"user":{"displayName":"YAKUP CAN KARACAOGLU","userId":"14801541947105482372"}},"outputId":"4f22bfec-65bf-4462-be33-72eac2568001"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount(\"/content/gdrive\")"]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","#from imblearn.over_sampling import RandomOverSampler\n","import numpy as np\n","from sklearn.utils import class_weight\n","\n","data = keras.preprocessing.image.ImageDataGenerator(\n","preprocessing_function=lambda x:x/255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/test\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","\n","# Calculate class weights\n","class_weights = np.array([1.3333, 0.8])\n","\n","model = keras.Sequential([\n","    keras.layers.Input((1600, 1200, 1)),\n","    keras.layers.Conv2D(8, (5, 5), strides=(3, 3), activation='relu'),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Conv2D(16, (7, 7), strides=(3, 3), activation='relu'),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Conv2D(32, (9, 9), strides=(2, 2), activation='relu'),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Flatten(),\n","    keras.layers.Dense(32, activation=\"relu\"),\n","    keras.layers.Dense(2, activation=\"softmax\"),\n","])\n","\n","model.compile(\n","    optimizer=keras.optimizers.SGD(learning_rate=0.01, nesterov=True, momentum=0.9),\n","    loss=tf.keras.losses.CategoricalCrossentropy(),\n","    metrics=[\"accuracy\"]\n",")\n","\n","model.summary()\n","\n","history = model.fit(data, verbose=1, epochs=4, class_weight=dict(enumerate(class_weights)))\n","\n","data_test = keras.preprocessing.image.ImageDataGenerator(\n","    preprocessing_function=lambda x: x / 255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/train\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","model.evaluate(data_test)\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Txke2voti4j4","executionInfo":{"status":"ok","timestamp":1685868694537,"user_tz":-180,"elapsed":246583,"user":{"displayName":"YAKUP CAN KARACAOGLU","userId":"14801541947105482372"}},"outputId":"947ccad2-9297-4a3e-f977-3265a536fb75"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 624 images belonging to 2 classes.\n","Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_5 (Conv2D)           (None, 532, 399, 8)       208       \n","                                                                 \n"," conv2d_6 (Conv2D)           (None, 264, 198, 8)       1608      \n","                                                                 \n"," dropout_1 (Dropout)         (None, 264, 198, 8)       0         \n","                                                                 \n"," conv2d_7 (Conv2D)           (None, 130, 97, 8)        1608      \n","                                                                 \n"," conv2d_8 (Conv2D)           (None, 42, 31, 16)        6288      \n","                                                                 \n"," max_pooling2d_2 (MaxPooling  (None, 21, 15, 16)       0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_9 (Conv2D)           (None, 7, 4, 32)          41504     \n","                                                                 \n"," max_pooling2d_3 (MaxPooling  (None, 3, 2, 32)         0         \n"," 2D)                                                             \n","                                                                 \n"," flatten_1 (Flatten)         (None, 192)               0         \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                6176      \n","                                                                 \n"," dense_3 (Dense)             (None, 2)                 66        \n","                                                                 \n","=================================================================\n","Total params: 57,458\n","Trainable params: 57,458\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/4\n","20/20 [==============================] - 20s 881ms/step - loss: 0.6931 - accuracy: 0.5465\n","Epoch 2/4\n","20/20 [==============================] - 18s 915ms/step - loss: 0.6598 - accuracy: 0.6699\n","Epoch 3/4\n","20/20 [==============================] - 19s 928ms/step - loss: 0.7051 - accuracy: 0.4263\n","Epoch 4/4\n","20/20 [==============================] - 18s 938ms/step - loss: 0.6821 - accuracy: 0.6795\n","Found 5232 images belonging to 2 classes.\n","164/164 [==============================] - 165s 1s/step - loss: 0.6563 - accuracy: 0.7422\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.6563021540641785, 0.7421635985374451]"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","from imblearn.over_sampling import RandomOverSampler\n","import numpy as np\n","\n","data = keras.preprocessing.image.ImageDataGenerator(\n","preprocessing_function=lambda x:x/255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/test\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","\n","model = keras.Sequential([\n","    keras.layers.Input((1600,1200,1)),\n","    keras.layers.Conv2D(8,(5,5),strides=(3,3), activation='relu'),\n","    keras.layers.Conv2D(8,(5,5),strides=(2,2), activation='relu'),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.Conv2D(8,(5,5),strides=(2,2), activation='relu'),\n","    keras.layers.Conv2D(16,(7,7),strides=(3,3), activation='relu'),\n","    keras.layers.MaxPool2D((2,2)),\n","    keras.layers.Conv2D(32,(9,9),strides=(2,2), activation='relu'),\n","    keras.layers.MaxPool2D((2,2)),\n","    keras.layers.Flatten(),\n","    keras.layers.Dense(32,activation=\"relu\"),\n","    keras.layers.Dense(2,activation=\"softmax\"),\n","    \n","])\n","\n","model.compile(optimizer=keras.optimizers.SGD(learning_rate=0.01,nesterov=True,momentum=0.9),loss=tf.keras.losses.CategoricalCrossentropy(),\n","             metrics=[\"accuracy\"])\n","\n","model.summary()\n","\n","history = model.fit(data,verbose=1,epochs=4)\n","\n","data_test = keras.preprocessing.image.ImageDataGenerator(\n","preprocessing_function=lambda x:x/255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/train\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","model.evaluate(data_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3LbpsLLwpC8i","executionInfo":{"status":"ok","timestamp":1685868990909,"user_tz":-180,"elapsed":284538,"user":{"displayName":"YAKUP CAN KARACAOGLU","userId":"14801541947105482372"}},"outputId":"d40907a2-81b3-4bd6-c0ee-bd14ea629e98"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 624 images belonging to 2 classes.\n","Model: \"sequential_2\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_10 (Conv2D)          (None, 532, 399, 8)       208       \n","                                                                 \n"," conv2d_11 (Conv2D)          (None, 264, 198, 8)       1608      \n","                                                                 \n"," dropout_2 (Dropout)         (None, 264, 198, 8)       0         \n","                                                                 \n"," conv2d_12 (Conv2D)          (None, 130, 97, 8)        1608      \n","                                                                 \n"," conv2d_13 (Conv2D)          (None, 42, 31, 16)        6288      \n","                                                                 \n"," max_pooling2d_4 (MaxPooling  (None, 21, 15, 16)       0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_14 (Conv2D)          (None, 7, 4, 32)          41504     \n","                                                                 \n"," max_pooling2d_5 (MaxPooling  (None, 3, 2, 32)         0         \n"," 2D)                                                             \n","                                                                 \n"," flatten_2 (Flatten)         (None, 192)               0         \n","                                                                 \n"," dense_4 (Dense)             (None, 32)                6176      \n","                                                                 \n"," dense_5 (Dense)             (None, 2)                 66        \n","                                                                 \n","=================================================================\n","Total params: 57,458\n","Trainable params: 57,458\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/4\n","20/20 [==============================] - 21s 923ms/step - loss: 0.6654 - accuracy: 0.6250\n","Epoch 2/4\n","20/20 [==============================] - 18s 901ms/step - loss: 0.6654 - accuracy: 0.6250\n","Epoch 3/4\n","20/20 [==============================] - 18s 918ms/step - loss: 0.6654 - accuracy: 0.6250\n","Epoch 4/4\n","20/20 [==============================] - 18s 898ms/step - loss: 0.6620 - accuracy: 0.6250\n","Found 5232 images belonging to 2 classes.\n","164/164 [==============================] - 164s 1s/step - loss: 0.6123 - accuracy: 0.7422\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.6122547388076782, 0.7421635985374451]"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","#from imblearn.over_sampling import RandomOverSampler\n","import numpy as np\n","from sklearn.utils import class_weight\n","\n","data = keras.preprocessing.image.ImageDataGenerator(\n","preprocessing_function=lambda x:x/255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/test\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","\n","# Calculate class weights\n","class_weights = np.array([13.333, 8])\n","\n","model = keras.Sequential([\n","    keras.layers.Input((1600, 1200, 1)),\n","    keras.layers.Conv2D(8, (5, 5), strides=(3, 3), activation='relu'),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Conv2D(16, (7, 7), strides=(3, 3), activation='relu'),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Conv2D(32, (9, 9), strides=(2, 2), activation='relu'),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Flatten(),\n","    keras.layers.Dense(32, activation=\"relu\"),\n","    keras.layers.Dense(2, activation=\"softmax\"),\n","])\n","\n","model.compile(\n","    optimizer=keras.optimizers.SGD(learning_rate=0.01, nesterov=True, momentum=0.9),\n","    loss=tf.keras.losses.CategoricalCrossentropy(),\n","    metrics=[\"accuracy\"]\n",")\n","\n","model.summary()\n","\n","history = model.fit(data, verbose=1, epochs=4, class_weight=dict(enumerate(class_weights)))\n","\n","data_test = keras.preprocessing.image.ImageDataGenerator(\n","    preprocessing_function=lambda x: x / 255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/train\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","model.evaluate(data_test)\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UuQWrfz6lWFV","executionInfo":{"status":"ok","timestamp":1685869340740,"user_tz":-180,"elapsed":285148,"user":{"displayName":"YAKUP CAN KARACAOGLU","userId":"14801541947105482372"}},"outputId":"ecbf143a-3e8a-4bdf-9080-679521b802c8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 624 images belonging to 2 classes.\n","Model: \"sequential_3\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_15 (Conv2D)          (None, 532, 399, 8)       208       \n","                                                                 \n"," conv2d_16 (Conv2D)          (None, 264, 198, 8)       1608      \n","                                                                 \n"," dropout_3 (Dropout)         (None, 264, 198, 8)       0         \n","                                                                 \n"," conv2d_17 (Conv2D)          (None, 130, 97, 8)        1608      \n","                                                                 \n"," conv2d_18 (Conv2D)          (None, 42, 31, 16)        6288      \n","                                                                 \n"," max_pooling2d_6 (MaxPooling  (None, 21, 15, 16)       0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_19 (Conv2D)          (None, 7, 4, 32)          41504     \n","                                                                 \n"," max_pooling2d_7 (MaxPooling  (None, 3, 2, 32)         0         \n"," 2D)                                                             \n","                                                                 \n"," flatten_3 (Flatten)         (None, 192)               0         \n","                                                                 \n"," dense_6 (Dense)             (None, 32)                6176      \n","                                                                 \n"," dense_7 (Dense)             (None, 2)                 66        \n","                                                                 \n","=================================================================\n","Total params: 57,458\n","Trainable params: 57,458\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/4\n","20/20 [==============================] - 19s 898ms/step - loss: 9.9630 - accuracy: 0.5657\n","Epoch 2/4\n","20/20 [==============================] - 19s 925ms/step - loss: 6.9722 - accuracy: 0.4455\n","Epoch 3/4\n","20/20 [==============================] - 19s 930ms/step - loss: 7.0378 - accuracy: 0.5032\n","Epoch 4/4\n","20/20 [==============================] - 19s 959ms/step - loss: 6.9579 - accuracy: 0.4712\n","Found 5232 images belonging to 2 classes.\n","164/164 [==============================] - 164s 999ms/step - loss: 0.6925 - accuracy: 0.7422\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.6925409436225891, 0.7421635985374451]"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","#from imblearn.over_sampling import RandomOverSampler\n","import numpy as np\n","from sklearn.utils import class_weight\n","\n","data = keras.preprocessing.image.ImageDataGenerator(\n","preprocessing_function=lambda x:x/255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/test\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","\n","# Calculate class weights\n","class_weights = np.array([133.3, 80])\n","\n","model = keras.Sequential([\n","    keras.layers.Input((1600, 1200, 1)),\n","    keras.layers.Conv2D(8, (5, 5), strides=(3, 3), activation='relu'),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Conv2D(16, (7, 7), strides=(3, 3), activation='relu'),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Conv2D(32, (9, 9), strides=(2, 2), activation='relu'),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Flatten(),\n","    keras.layers.Dense(32, activation=\"relu\"),\n","    keras.layers.Dense(2, activation=\"softmax\"),\n","])\n","\n","model.compile(\n","    optimizer=keras.optimizers.SGD(learning_rate=0.01, nesterov=True, momentum=0.9),\n","    loss=tf.keras.losses.CategoricalCrossentropy(),\n","    metrics=[\"accuracy\"]\n",")\n","\n","model.summary()\n","\n","history = model.fit(data, verbose=1, epochs=4, class_weight=dict(enumerate(class_weights)))\n","\n","data_test = keras.preprocessing.image.ImageDataGenerator(\n","    preprocessing_function=lambda x: x / 255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/train\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","model.evaluate(data_test)\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mKX5vQFipjJ4","executionInfo":{"status":"ok","timestamp":1685869710215,"user_tz":-180,"elapsed":249339,"user":{"displayName":"YAKUP CAN KARACAOGLU","userId":"14801541947105482372"}},"outputId":"c6dd17d4-8ef4-4320-8d9a-f2a7312a0f5b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 624 images belonging to 2 classes.\n","Model: \"sequential_4\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_20 (Conv2D)          (None, 532, 399, 8)       208       \n","                                                                 \n"," conv2d_21 (Conv2D)          (None, 264, 198, 8)       1608      \n","                                                                 \n"," dropout_4 (Dropout)         (None, 264, 198, 8)       0         \n","                                                                 \n"," conv2d_22 (Conv2D)          (None, 130, 97, 8)        1608      \n","                                                                 \n"," conv2d_23 (Conv2D)          (None, 42, 31, 16)        6288      \n","                                                                 \n"," max_pooling2d_8 (MaxPooling  (None, 21, 15, 16)       0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_24 (Conv2D)          (None, 7, 4, 32)          41504     \n","                                                                 \n"," max_pooling2d_9 (MaxPooling  (None, 3, 2, 32)         0         \n"," 2D)                                                             \n","                                                                 \n"," flatten_4 (Flatten)         (None, 192)               0         \n","                                                                 \n"," dense_8 (Dense)             (None, 32)                6176      \n","                                                                 \n"," dense_9 (Dense)             (None, 2)                 66        \n","                                                                 \n","=================================================================\n","Total params: 57,458\n","Trainable params: 57,458\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/4\n","20/20 [==============================] - 20s 914ms/step - loss: 1308667337667569443793428021248.0000 - accuracy: 0.4423\n","Epoch 2/4\n","20/20 [==============================] - 19s 961ms/step - loss: 71.8853 - accuracy: 0.5513\n","Epoch 3/4\n","20/20 [==============================] - 19s 916ms/step - loss: 72.0596 - accuracy: 0.4551\n","Epoch 4/4\n","20/20 [==============================] - 18s 916ms/step - loss: 70.6912 - accuracy: 0.4712\n","Found 5232 images belonging to 2 classes.\n","164/164 [==============================] - 163s 995ms/step - loss: 0.6285 - accuracy: 0.7422\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.6285456418991089, 0.7421635985374451]"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","#from imblearn.over_sampling import RandomOverSampler\n","import numpy as np\n","from sklearn.utils import class_weight\n","\n","data = keras.preprocessing.image.ImageDataGenerator(\n","preprocessing_function=lambda x:x/255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/test\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","\n","# Calculate class weights\n","class_weights = np.array([50, 1])\n","\n","model = keras.Sequential([\n","    keras.layers.Input((1600, 1200, 1)),\n","    keras.layers.Conv2D(8, (5, 5), strides=(3, 3), activation='relu'),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Conv2D(16, (7, 7), strides=(3, 3), activation='relu'),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Conv2D(32, (9, 9), strides=(2, 2), activation='relu'),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Flatten(),\n","    keras.layers.Dense(32, activation=\"relu\"),\n","    keras.layers.Dense(2, activation=\"softmax\"),\n","])\n","\n","model.compile(\n","    optimizer=keras.optimizers.SGD(learning_rate=0.01, nesterov=True, momentum=0.9),\n","    loss=tf.keras.losses.CategoricalCrossentropy(),\n","    metrics=[\"accuracy\"]\n",")\n","\n","model.summary()\n","\n","history = model.fit(data, verbose=1, epochs=4, class_weight=dict(enumerate(class_weights)))\n","\n","data_test = keras.preprocessing.image.ImageDataGenerator(\n","    preprocessing_function=lambda x: x / 255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/train\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","model.evaluate(data_test)\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lMeYIcE_pnwJ","executionInfo":{"status":"ok","timestamp":1685870340831,"user_tz":-180,"elapsed":246664,"user":{"displayName":"YAKUP CAN KARACAOGLU","userId":"14801541947105482372"}},"outputId":"af282b89-e038-460e-b99f-b9ecff8294fd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 624 images belonging to 2 classes.\n","Model: \"sequential_5\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_25 (Conv2D)          (None, 532, 399, 8)       208       \n","                                                                 \n"," conv2d_26 (Conv2D)          (None, 264, 198, 8)       1608      \n","                                                                 \n"," dropout_5 (Dropout)         (None, 264, 198, 8)       0         \n","                                                                 \n"," conv2d_27 (Conv2D)          (None, 130, 97, 8)        1608      \n","                                                                 \n"," conv2d_28 (Conv2D)          (None, 42, 31, 16)        6288      \n","                                                                 \n"," max_pooling2d_10 (MaxPoolin  (None, 21, 15, 16)       0         \n"," g2D)                                                            \n","                                                                 \n"," conv2d_29 (Conv2D)          (None, 7, 4, 32)          41504     \n","                                                                 \n"," max_pooling2d_11 (MaxPoolin  (None, 3, 2, 32)         0         \n"," g2D)                                                            \n","                                                                 \n"," flatten_5 (Flatten)         (None, 192)               0         \n","                                                                 \n"," dense_10 (Dense)            (None, 32)                6176      \n","                                                                 \n"," dense_11 (Dense)            (None, 2)                 66        \n","                                                                 \n","=================================================================\n","Total params: 57,458\n","Trainable params: 57,458\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/4\n","20/20 [==============================] - 19s 892ms/step - loss: 227.6060 - accuracy: 0.4423\n","Epoch 2/4\n","20/20 [==============================] - 19s 960ms/step - loss: 6.4683 - accuracy: 0.3750\n","Epoch 3/4\n","20/20 [==============================] - 19s 940ms/step - loss: 5.4766 - accuracy: 0.3750\n","Epoch 4/4\n","20/20 [==============================] - 19s 963ms/step - loss: 4.0461 - accuracy: 0.3750\n","Found 5232 images belonging to 2 classes.\n","164/164 [==============================] - 164s 1000ms/step - loss: 3.8115 - accuracy: 0.2578\n"]},{"output_type":"execute_result","data":{"text/plain":["[3.8115134239196777, 0.25783640146255493]"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","#from imblearn.over_sampling import RandomOverSampler\n","import numpy as np\n","from sklearn.utils import class_weight\n","\n","data = keras.preprocessing.image.ImageDataGenerator(\n","preprocessing_function=lambda x:x/255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/test\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","\n","# Calculate class weights\n","class_weights = np.array([10, 1])\n","\n","model = keras.Sequential([\n","    keras.layers.Input((1600, 1200, 1)),\n","    keras.layers.Conv2D(8, (5, 5), strides=(3, 3), activation='relu'),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Conv2D(16, (7, 7), strides=(3, 3), activation='relu'),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Conv2D(32, (9, 9), strides=(2, 2), activation='relu'),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Flatten(),\n","    keras.layers.Dense(32, activation=\"relu\"),\n","    keras.layers.Dense(2, activation=\"softmax\"),\n","])\n","\n","model.compile(\n","    optimizer=keras.optimizers.SGD(learning_rate=0.01, nesterov=True, momentum=0.9),\n","    loss=tf.keras.losses.CategoricalCrossentropy(),\n","    metrics=[\"accuracy\"]\n",")\n","\n","model.summary()\n","\n","history = model.fit(data, verbose=1, epochs=4, class_weight=dict(enumerate(class_weights)))\n","\n","data_test = keras.preprocessing.image.ImageDataGenerator(\n","    preprocessing_function=lambda x: x / 255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/train\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","model.evaluate(data_test)\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1hMxJVOksCXo","executionInfo":{"status":"ok","timestamp":1685870879759,"user_tz":-180,"elapsed":284181,"user":{"displayName":"YAKUP CAN KARACAOGLU","userId":"14801541947105482372"}},"outputId":"fb6556aa-0de8-4903-9de3-1edaaf8dbc04"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 624 images belonging to 2 classes.\n","Model: \"sequential_6\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_30 (Conv2D)          (None, 532, 399, 8)       208       \n","                                                                 \n"," conv2d_31 (Conv2D)          (None, 264, 198, 8)       1608      \n","                                                                 \n"," dropout_6 (Dropout)         (None, 264, 198, 8)       0         \n","                                                                 \n"," conv2d_32 (Conv2D)          (None, 130, 97, 8)        1608      \n","                                                                 \n"," conv2d_33 (Conv2D)          (None, 42, 31, 16)        6288      \n","                                                                 \n"," max_pooling2d_12 (MaxPoolin  (None, 21, 15, 16)       0         \n"," g2D)                                                            \n","                                                                 \n"," conv2d_34 (Conv2D)          (None, 7, 4, 32)          41504     \n","                                                                 \n"," max_pooling2d_13 (MaxPoolin  (None, 3, 2, 32)         0         \n"," g2D)                                                            \n","                                                                 \n"," flatten_6 (Flatten)         (None, 192)               0         \n","                                                                 \n"," dense_12 (Dense)            (None, 32)                6176      \n","                                                                 \n"," dense_13 (Dense)            (None, 2)                 66        \n","                                                                 \n","=================================================================\n","Total params: 57,458\n","Trainable params: 57,458\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/4\n","20/20 [==============================] - 20s 897ms/step - loss: 1.9977 - accuracy: 0.3750\n","Epoch 2/4\n","20/20 [==============================] - 18s 917ms/step - loss: 1.7998 - accuracy: 0.3750\n","Epoch 3/4\n","20/20 [==============================] - 19s 970ms/step - loss: 1.7965 - accuracy: 0.3750\n","Epoch 4/4\n","20/20 [==============================] - 18s 905ms/step - loss: 1.8001 - accuracy: 0.3750\n","Found 5232 images belonging to 2 classes.\n","164/164 [==============================] - 167s 1s/step - loss: 1.4532 - accuracy: 0.2578\n"]},{"output_type":"execute_result","data":{"text/plain":["[1.4532029628753662, 0.25783640146255493]"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","#from imblearn.over_sampling import RandomOverSampler\n","import numpy as np\n","from sklearn.utils import class_weight\n","\n","data = keras.preprocessing.image.ImageDataGenerator(\n","preprocessing_function=lambda x:x/255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/test\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","\n","# Calculate class weights\n","class_weights = np.array([1, 1])\n","\n","model = keras.Sequential([\n","    keras.layers.Input((1600, 1200, 1)),\n","    keras.layers.Conv2D(8, (5, 5), strides=(3, 3), activation='relu'),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Conv2D(16, (7, 7), strides=(3, 3), activation='relu'),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Conv2D(32, (9, 9), strides=(2, 2), activation='relu'),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Flatten(),\n","    keras.layers.Dense(32, activation=\"relu\"),\n","    keras.layers.Dense(2, activation=\"softmax\"),\n","])\n","\n","model.compile(\n","    optimizer=keras.optimizers.SGD(learning_rate=0.01, nesterov=True, momentum=0.9),\n","    loss=tf.keras.losses.CategoricalCrossentropy(),\n","    metrics=[\"accuracy\"]\n",")\n","\n","model.summary()\n","\n","history = model.fit(data, verbose=1, epochs=4, class_weight=dict(enumerate(class_weights)))\n","\n","data_test = keras.preprocessing.image.ImageDataGenerator(\n","    preprocessing_function=lambda x: x / 255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/train\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","model.evaluate(data_test)\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CJIbilHLt8yK","executionInfo":{"status":"ok","timestamp":1685871564818,"user_tz":-180,"elapsed":245148,"user":{"displayName":"YAKUP CAN KARACAOGLU","userId":"14801541947105482372"}},"outputId":"c778da43-c212-40e5-a693-77a42f507c84"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 624 images belonging to 2 classes.\n","Model: \"sequential_7\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_35 (Conv2D)          (None, 532, 399, 8)       208       \n","                                                                 \n"," conv2d_36 (Conv2D)          (None, 264, 198, 8)       1608      \n","                                                                 \n"," dropout_7 (Dropout)         (None, 264, 198, 8)       0         \n","                                                                 \n"," conv2d_37 (Conv2D)          (None, 130, 97, 8)        1608      \n","                                                                 \n"," conv2d_38 (Conv2D)          (None, 42, 31, 16)        6288      \n","                                                                 \n"," max_pooling2d_14 (MaxPoolin  (None, 21, 15, 16)       0         \n"," g2D)                                                            \n","                                                                 \n"," conv2d_39 (Conv2D)          (None, 7, 4, 32)          41504     \n","                                                                 \n"," max_pooling2d_15 (MaxPoolin  (None, 3, 2, 32)         0         \n"," g2D)                                                            \n","                                                                 \n"," flatten_7 (Flatten)         (None, 192)               0         \n","                                                                 \n"," dense_14 (Dense)            (None, 32)                6176      \n","                                                                 \n"," dense_15 (Dense)            (None, 2)                 66        \n","                                                                 \n","=================================================================\n","Total params: 57,458\n","Trainable params: 57,458\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/4\n","20/20 [==============================] - 21s 957ms/step - loss: 0.6735 - accuracy: 0.5946\n","Epoch 2/4\n","20/20 [==============================] - 19s 931ms/step - loss: 0.6369 - accuracy: 0.6506\n","Epoch 3/4\n","20/20 [==============================] - 19s 937ms/step - loss: 0.5071 - accuracy: 0.7532\n","Epoch 4/4\n","20/20 [==============================] - 19s 933ms/step - loss: 0.4064 - accuracy: 0.8381\n","Found 5232 images belonging to 2 classes.\n","164/164 [==============================] - 164s 997ms/step - loss: 0.3649 - accuracy: 0.8429\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.36489778757095337, 0.8428899049758911]"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","#from imblearn.over_sampling import RandomOverSampler\n","import numpy as np\n","from sklearn.utils import class_weight\n","\n","data = keras.preprocessing.image.ImageDataGenerator(\n","preprocessing_function=lambda x:x/255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/test\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","\n","# Calculate class weights\n","class_weights = np.array([1, 1])\n","\n","model = keras.Sequential([\n","    keras.layers.Input((1600, 1200, 1)),\n","    keras.layers.Conv2D(8, (5, 5), strides=(3, 3), activation='relu'),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Conv2D(16, (7, 7), strides=(3, 3), activation='relu'),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Conv2D(32, (9, 9), strides=(2, 2), activation='relu'),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Flatten(),\n","    keras.layers.Dense(32, activation=\"relu\"),\n","    keras.layers.Dense(2, activation=\"softmax\"),\n","])\n","\n","model.compile(\n","    optimizer=keras.optimizers.SGD(learning_rate=0.01, nesterov=True, momentum=0.9),\n","    loss=tf.keras.losses.CategoricalCrossentropy(),\n","    metrics=[\"accuracy\"]\n",")\n","\n","model.summary()\n","\n","history = model.fit(data, verbose=1, epochs=10, class_weight=dict(enumerate(class_weights)))\n","\n","data_test = keras.preprocessing.image.ImageDataGenerator(\n","    preprocessing_function=lambda x: x / 255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/train\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","model.evaluate(data_test)\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wf6BPIi6wtUg","executionInfo":{"status":"ok","timestamp":1685872110981,"user_tz":-180,"elapsed":405580,"user":{"displayName":"YAKUP CAN KARACAOGLU","userId":"14801541947105482372"}},"outputId":"88c18a40-6c8d-4fc1-c4b0-538fdd799437"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 624 images belonging to 2 classes.\n","Model: \"sequential_8\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_40 (Conv2D)          (None, 532, 399, 8)       208       \n","                                                                 \n"," conv2d_41 (Conv2D)          (None, 264, 198, 8)       1608      \n","                                                                 \n"," dropout_8 (Dropout)         (None, 264, 198, 8)       0         \n","                                                                 \n"," conv2d_42 (Conv2D)          (None, 130, 97, 8)        1608      \n","                                                                 \n"," conv2d_43 (Conv2D)          (None, 42, 31, 16)        6288      \n","                                                                 \n"," max_pooling2d_16 (MaxPoolin  (None, 21, 15, 16)       0         \n"," g2D)                                                            \n","                                                                 \n"," conv2d_44 (Conv2D)          (None, 7, 4, 32)          41504     \n","                                                                 \n"," max_pooling2d_17 (MaxPoolin  (None, 3, 2, 32)         0         \n"," g2D)                                                            \n","                                                                 \n"," flatten_8 (Flatten)         (None, 192)               0         \n","                                                                 \n"," dense_16 (Dense)            (None, 32)                6176      \n","                                                                 \n"," dense_17 (Dense)            (None, 2)                 66        \n","                                                                 \n","=================================================================\n","Total params: 57,458\n","Trainable params: 57,458\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/10\n","20/20 [==============================] - 21s 901ms/step - loss: 0.6709 - accuracy: 0.6250\n","Epoch 2/10\n","20/20 [==============================] - 19s 925ms/step - loss: 0.6379 - accuracy: 0.6554\n","Epoch 3/10\n","20/20 [==============================] - 20s 971ms/step - loss: 0.9186 - accuracy: 0.6667\n","Epoch 4/10\n","20/20 [==============================] - 19s 932ms/step - loss: 0.6638 - accuracy: 0.6250\n","Epoch 5/10\n","20/20 [==============================] - 20s 978ms/step - loss: 0.6628 - accuracy: 0.6250\n","Epoch 6/10\n","20/20 [==============================] - 18s 914ms/step - loss: 0.6621 - accuracy: 0.6250\n","Epoch 7/10\n","20/20 [==============================] - 19s 927ms/step - loss: 0.6623 - accuracy: 0.6250\n","Epoch 8/10\n","20/20 [==============================] - 18s 916ms/step - loss: 0.6622 - accuracy: 0.6250\n","Epoch 9/10\n","20/20 [==============================] - 19s 944ms/step - loss: 0.6621 - accuracy: 0.6250\n","Epoch 10/10\n","20/20 [==============================] - 19s 928ms/step - loss: 0.6623 - accuracy: 0.6250\n","Found 5232 images belonging to 2 classes.\n","164/164 [==============================] - 164s 1000ms/step - loss: 0.6038 - accuracy: 0.7422\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.6037914752960205, 0.7421635985374451]"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","#from imblearn.over_sampling import RandomOverSampler\n","import numpy as np\n","from sklearn.utils import class_weight\n","\n","data = keras.preprocessing.image.ImageDataGenerator(\n","preprocessing_function=lambda x:x/255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/train\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","\n","# Calculate class weights\n","class_weights = np.array([1, 1])\n","\n","model = keras.Sequential([\n","    keras.layers.Input((1600, 1200, 1)),\n","    keras.layers.Conv2D(8, (5, 5), strides=(3, 3), activation='relu'),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Conv2D(16, (7, 7), strides=(3, 3), activation='relu'),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Conv2D(32, (9, 9), strides=(2, 2), activation='relu'),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Flatten(),\n","    keras.layers.Dense(32, activation=\"relu\"),\n","    keras.layers.Dense(2, activation=\"softmax\"),\n","])\n","\n","model.compile(\n","    optimizer=keras.optimizers.SGD(learning_rate=0.01, nesterov=True, momentum=0.9),\n","    loss=tf.keras.losses.CategoricalCrossentropy(),\n","    metrics=[\"accuracy\"]\n",")\n","\n","model.summary()\n","\n","history = model.fit(data, verbose=1, epochs=4, class_weight=dict(enumerate(class_weights)))\n","\n","data_test = keras.preprocessing.image.ImageDataGenerator(\n","    preprocessing_function=lambda x: x / 255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/test\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","model.evaluate(data_test)\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2PTM0oGhyLvH","executionInfo":{"status":"ok","timestamp":1685872931894,"user_tz":-180,"elapsed":758331,"user":{"displayName":"YAKUP CAN KARACAOGLU","userId":"14801541947105482372"}},"outputId":"d6a78365-d9b3-4aad-82e1-075e3d54ec16"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 5232 images belonging to 2 classes.\n","Model: \"sequential_9\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_45 (Conv2D)          (None, 532, 399, 8)       208       \n","                                                                 \n"," conv2d_46 (Conv2D)          (None, 264, 198, 8)       1608      \n","                                                                 \n"," dropout_9 (Dropout)         (None, 264, 198, 8)       0         \n","                                                                 \n"," conv2d_47 (Conv2D)          (None, 130, 97, 8)        1608      \n","                                                                 \n"," conv2d_48 (Conv2D)          (None, 42, 31, 16)        6288      \n","                                                                 \n"," max_pooling2d_18 (MaxPoolin  (None, 21, 15, 16)       0         \n"," g2D)                                                            \n","                                                                 \n"," conv2d_49 (Conv2D)          (None, 7, 4, 32)          41504     \n","                                                                 \n"," max_pooling2d_19 (MaxPoolin  (None, 3, 2, 32)         0         \n"," g2D)                                                            \n","                                                                 \n"," flatten_9 (Flatten)         (None, 192)               0         \n","                                                                 \n"," dense_18 (Dense)            (None, 32)                6176      \n","                                                                 \n"," dense_19 (Dense)            (None, 2)                 66        \n","                                                                 \n","=================================================================\n","Total params: 57,458\n","Trainable params: 57,458\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/4\n","164/164 [==============================] - 170s 1s/step - loss: 0.4497 - accuracy: 0.7985\n","Epoch 2/4\n","164/164 [==============================] - 170s 1s/step - loss: 0.1607 - accuracy: 0.9413\n","Epoch 3/4\n","164/164 [==============================] - 165s 1s/step - loss: 0.1248 - accuracy: 0.9543\n","Epoch 4/4\n","164/164 [==============================] - 166s 1s/step - loss: 0.0985 - accuracy: 0.9671\n","Found 624 images belonging to 2 classes.\n","20/20 [==============================] - 19s 916ms/step - loss: 0.7247 - accuracy: 0.7388\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.7247006297111511, 0.7387820482254028]"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","#from imblearn.over_sampling import RandomOverSampler\n","import numpy as np\n","from sklearn.utils import class_weight\n","\n","data = keras.preprocessing.image.ImageDataGenerator(\n","preprocessing_function=lambda x:x/255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/train\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","\n","# Calculate class weights\n","class_weights = np.array([1, 1])\n","\n","model = keras.Sequential([\n","    keras.layers.Input((1600, 1200, 1)),\n","    keras.layers.Conv2D(8, (5, 5), strides=(3, 3), activation='relu'),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Conv2D(16, (7, 7), strides=(3, 3), activation='relu'),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Conv2D(32, (9, 9), strides=(2, 2), activation='relu'),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Flatten(),\n","    keras.layers.Dense(32, activation=\"relu\"),\n","    keras.layers.Dense(2, activation=\"softmax\"),\n","])\n","\n","model.compile(\n","    optimizer=keras.optimizers.SGD(learning_rate=0.01, nesterov=True, momentum=0.9),\n","    loss=tf.keras.losses.CategoricalCrossentropy(),\n","    metrics=[\"accuracy\"]\n",")\n","\n","model.summary()\n","\n","history = model.fit(data, verbose=1, epochs=10, class_weight=dict(enumerate(class_weights)))\n","\n","data_test = keras.preprocessing.image.ImageDataGenerator(\n","    preprocessing_function=lambda x: x / 255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/test\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","model.evaluate(data_test)\n","\n","\n","\n"],"metadata":{"id":"PeaZWi4Xz95y","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685903588524,"user_tz":-180,"elapsed":2135625,"user":{"displayName":"YAKUP CAN KARACAOGLU","userId":"14801541947105482372"}},"outputId":"a3b17112-9678-4c30-b2b8-f5f577b0be00"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 5232 images belonging to 2 classes.\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d (Conv2D)             (None, 532, 399, 8)       208       \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 264, 198, 8)       1608      \n","                                                                 \n"," dropout (Dropout)           (None, 264, 198, 8)       0         \n","                                                                 \n"," conv2d_2 (Conv2D)           (None, 130, 97, 8)        1608      \n","                                                                 \n"," conv2d_3 (Conv2D)           (None, 42, 31, 16)        6288      \n","                                                                 \n"," max_pooling2d (MaxPooling2D  (None, 21, 15, 16)       0         \n"," )                                                               \n","                                                                 \n"," conv2d_4 (Conv2D)           (None, 7, 4, 32)          41504     \n","                                                                 \n"," max_pooling2d_1 (MaxPooling  (None, 3, 2, 32)         0         \n"," 2D)                                                             \n","                                                                 \n"," flatten (Flatten)           (None, 192)               0         \n","                                                                 \n"," dense (Dense)               (None, 32)                6176      \n","                                                                 \n"," dense_1 (Dense)             (None, 2)                 66        \n","                                                                 \n","=================================================================\n","Total params: 57,458\n","Trainable params: 57,458\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/10\n","164/164 [==============================] - 440s 2s/step - loss: 0.5634 - accuracy: 0.7439\n","Epoch 2/10\n","164/164 [==============================] - 160s 973ms/step - loss: 0.5711 - accuracy: 0.7433\n","Epoch 3/10\n","164/164 [==============================] - 159s 968ms/step - loss: 0.5723 - accuracy: 0.7422\n","Epoch 4/10\n","164/164 [==============================] - 158s 961ms/step - loss: 0.5715 - accuracy: 0.7422\n","Epoch 5/10\n","164/164 [==============================] - 158s 964ms/step - loss: 0.5716 - accuracy: 0.7422\n","Epoch 6/10\n","164/164 [==============================] - 159s 967ms/step - loss: 0.5719 - accuracy: 0.7422\n","Epoch 7/10\n","164/164 [==============================] - 158s 963ms/step - loss: 0.5710 - accuracy: 0.7422\n","Epoch 8/10\n","164/164 [==============================] - 159s 968ms/step - loss: 0.5581 - accuracy: 0.7422\n","Epoch 9/10\n","164/164 [==============================] - 159s 969ms/step - loss: 0.3879 - accuracy: 0.8207\n","Epoch 10/10\n","164/164 [==============================] - 159s 971ms/step - loss: 0.2471 - accuracy: 0.8981\n","Found 624 images belonging to 2 classes.\n","20/20 [==============================] - 59s 3s/step - loss: 0.8215 - accuracy: 0.7260\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.8215276002883911, 0.7259615659713745]"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","#from imblearn.over_sampling import RandomOverSampler\n","import numpy as np\n","from sklearn.utils import class_weight\n","\n","data = keras.preprocessing.image.ImageDataGenerator(\n","preprocessing_function=lambda x:x/255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/train\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","\n","# Calculate class weights\n","class_weights = np.array([1, 1])\n","\n","model = keras.Sequential([\n","    keras.layers.Input((1600, 1200, 1)),\n","    keras.layers.Conv2D(8, (5, 5), strides=(3, 3), activation='relu'),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Conv2D(16, (7, 7), strides=(3, 3), activation='relu'),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Conv2D(32, (9, 9), strides=(2, 2), activation='relu'),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Flatten(),\n","    keras.layers.Dense(32, activation=\"relu\"),\n","    keras.layers.Dense(2, activation=\"softmax\"),\n","])\n","\n","model.compile(\n","    optimizer=keras.optimizers.SGD(learning_rate=0.01, nesterov=True, momentum=0.9),\n","    loss=tf.keras.losses.CategoricalCrossentropy(),\n","    metrics=[\"accuracy\"]\n",")\n","\n","model.summary()\n","\n","history = model.fit(data, verbose=1, epochs=4, class_weight=dict(enumerate(class_weights)))\n","\n","data_test = keras.preprocessing.image.ImageDataGenerator(\n","    preprocessing_function=lambda x: x / 255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/test\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","model.evaluate(data_test)\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9RF1IngpXV-F","executionInfo":{"status":"ok","timestamp":1685904760067,"user_tz":-180,"elapsed":384937,"user":{"displayName":"YAKUP CAN KARACAOGLU","userId":"14801541947105482372"}},"outputId":"23333d78-6159-4d75-fda3-363068183236"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 5232 images belonging to 2 classes.\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d (Conv2D)             (None, 532, 399, 8)       208       \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 264, 198, 8)       1608      \n","                                                                 \n"," dropout (Dropout)           (None, 264, 198, 8)       0         \n","                                                                 \n"," conv2d_2 (Conv2D)           (None, 130, 97, 8)        1608      \n","                                                                 \n"," conv2d_3 (Conv2D)           (None, 42, 31, 16)        6288      \n","                                                                 \n"," max_pooling2d (MaxPooling2D  (None, 21, 15, 16)       0         \n"," )                                                               \n","                                                                 \n"," conv2d_4 (Conv2D)           (None, 7, 4, 32)          41504     \n","                                                                 \n"," max_pooling2d_1 (MaxPooling  (None, 3, 2, 32)         0         \n"," 2D)                                                             \n","                                                                 \n"," flatten (Flatten)           (None, 192)               0         \n","                                                                 \n"," dense (Dense)               (None, 32)                6176      \n","                                                                 \n"," dense_1 (Dense)             (None, 2)                 66        \n","                                                                 \n","=================================================================\n","Total params: 57,458\n","Trainable params: 57,458\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/2\n","164/164 [==============================] - 163s 947ms/step - loss: 0.5718 - accuracy: 0.7464\n","Epoch 2/2\n","164/164 [==============================] - 157s 955ms/step - loss: 0.5714 - accuracy: 0.7422\n","Found 624 images belonging to 2 classes.\n","20/20 [==============================] - 18s 886ms/step - loss: 0.6961 - accuracy: 0.6250\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.6961091756820679, 0.625]"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","#from imblearn.over_sampling import RandomOverSampler\n","import numpy as np\n","from sklearn.utils import class_weight\n","\n","data = keras.preprocessing.image.ImageDataGenerator(\n","preprocessing_function=lambda x:x/255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/train\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","\n","# Calculate class weights\n","class_weights = np.array([1, 1])\n","\n","model = keras.Sequential([\n","    keras.layers.Input((1600, 1200, 1)),\n","    keras.layers.Conv2D(8, (5, 5), strides=(3, 3), activation='relu'),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Conv2D(16, (7, 7), strides=(3, 3), activation='relu'),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Conv2D(32, (9, 9), strides=(2, 2), activation='relu'),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Flatten(),\n","    keras.layers.Dense(32, activation=\"relu\"),\n","    keras.layers.Dense(2, activation=\"softmax\"),\n","])\n","\n","model.compile(\n","    optimizer=keras.optimizers.SGD(learning_rate=0.01, nesterov=True, momentum=0.9),\n","    loss=tf.keras.losses.CategoricalCrossentropy(),\n","    metrics=[\"accuracy\"]\n",")\n","\n","model.summary()\n","\n","history = model.fit(data, verbose=1, epochs=5, class_weight=dict(enumerate(class_weights)))\n","\n","data_test = keras.preprocessing.image.ImageDataGenerator(\n","    preprocessing_function=lambda x: x / 255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/test\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","model.evaluate(data_test)\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bL-LuuxdtNBA","executionInfo":{"status":"ok","timestamp":1685905918001,"user_tz":-180,"elapsed":949244,"user":{"displayName":"YAKUP CAN KARACAOGLU","userId":"14801541947105482372"}},"outputId":"c3ad2677-0127-4554-8a18-e99742242ef6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 5232 images belonging to 2 classes.\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d (Conv2D)             (None, 532, 399, 8)       208       \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 264, 198, 8)       1608      \n","                                                                 \n"," dropout (Dropout)           (None, 264, 198, 8)       0         \n","                                                                 \n"," conv2d_2 (Conv2D)           (None, 130, 97, 8)        1608      \n","                                                                 \n"," conv2d_3 (Conv2D)           (None, 42, 31, 16)        6288      \n","                                                                 \n"," max_pooling2d (MaxPooling2D  (None, 21, 15, 16)       0         \n"," )                                                               \n","                                                                 \n"," conv2d_4 (Conv2D)           (None, 7, 4, 32)          41504     \n","                                                                 \n"," max_pooling2d_1 (MaxPooling  (None, 3, 2, 32)         0         \n"," 2D)                                                             \n","                                                                 \n"," flatten (Flatten)           (None, 192)               0         \n","                                                                 \n"," dense (Dense)               (None, 32)                6176      \n","                                                                 \n"," dense_1 (Dense)             (None, 2)                 66        \n","                                                                 \n","=================================================================\n","Total params: 57,458\n","Trainable params: 57,458\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/5\n","164/164 [==============================] - 167s 960ms/step - loss: 0.5484 - accuracy: 0.7498\n","Epoch 2/5\n","164/164 [==============================] - 157s 959ms/step - loss: 0.2671 - accuracy: 0.8943\n","Epoch 3/5\n","164/164 [==============================] - 158s 965ms/step - loss: 0.1405 - accuracy: 0.9474\n","Epoch 4/5\n","164/164 [==============================] - 161s 982ms/step - loss: 0.1072 - accuracy: 0.9597\n","Epoch 5/5\n","164/164 [==============================] - 159s 968ms/step - loss: 0.0928 - accuracy: 0.9637\n","Found 624 images belonging to 2 classes.\n","20/20 [==============================] - 18s 909ms/step - loss: 0.8414 - accuracy: 0.7628\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.8414483666419983, 0.7628205418586731]"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","#from imblearn.over_sampling import RandomOverSampler\n","import numpy as np\n","from sklearn.utils import class_weight\n","\n","data = keras.preprocessing.image.ImageDataGenerator(\n","preprocessing_function=lambda x:x/255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/train\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","\n","# Calculate class weights\n","class_weights = np.array([1, 1])\n","\n","model = keras.Sequential([\n","    keras.layers.Input((1600, 1200, 1)),\n","    keras.layers.Conv2D(8, (5, 5), strides=(3, 3), activation='relu'),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Conv2D(16, (7, 7), strides=(3, 3), activation='relu'),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Conv2D(32, (9, 9), strides=(2, 2), activation='relu'),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Flatten(),\n","    keras.layers.Dense(32, activation=\"relu\"),\n","    keras.layers.Dense(2, activation=\"softmax\"),\n","])\n","\n","model.compile(\n","    optimizer=keras.optimizers.SGD(learning_rate=0.01, nesterov=True, momentum=0.9),\n","    loss=tf.keras.losses.CategoricalCrossentropy(),\n","    metrics=[\"accuracy\"]\n",")\n","\n","model.summary()\n","\n","history = model.fit(data, verbose=1, epochs=3, class_weight=dict(enumerate(class_weights)))\n","\n","data_test = keras.preprocessing.image.ImageDataGenerator(\n","    preprocessing_function=lambda x: x / 255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/test\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","model.evaluate(data_test)\n","\n","\n","\n"],"metadata":{"id":"dhEy4dZwxEqZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","#from imblearn.over_sampling import RandomOverSampler\n","import numpy as np\n","from sklearn.utils import class_weight\n","\n","data = keras.preprocessing.image.ImageDataGenerator(\n","preprocessing_function=lambda x:x/255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/train\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","\n","# Calculate class weights\n","class_weights = np.array([1, 1])\n","\n","model = keras.Sequential([\n","    keras.layers.Input((1600, 1200, 1)),\n","    keras.layers.Conv2D(8, (5, 5), strides=(3, 3), activation='relu'),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.BatchNormalization(),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Dropout(0.2),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Conv2D(16, (7, 7), strides=(3, 3), activation='relu'),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Conv2D(32, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Dropout(0.2),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Flatten(),\n","    keras.layers.Dense(32, activation=\"relu\"),\n","    keras.layers.Dense(2, activation=\"softmax\"),\n","])\n","\n","model.compile(\n","    optimizer=keras.optimizers.SGD(learning_rate=0.01, nesterov=True, momentum=0.90),\n","    loss=tf.keras.losses.CategoricalCrossentropy(),\n","    metrics=[\"accuracy\"]\n",")\n","\n","model.summary()\n","\n","history = model.fit(data, verbose=1, epochs=4)\n","\n","data_test = keras.preprocessing.image.ImageDataGenerator(\n","    preprocessing_function=lambda x: x / 255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/test\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","model.evaluate(data_test)\n","\n","\n","\n"],"metadata":{"id":"PB1D5XW7442j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.models import load_model\n","\n","loaded_model = load_model(\"/content/gdrive/MyDrive/chest/model2/model.h5\")"],"metadata":{"id":"iNmutFIIAQtg","executionInfo":{"status":"ok","timestamp":1686412522098,"user_tz":-180,"elapsed":7077,"user":{"displayName":"YAKUP CAN KARACAOGLU","userId":"14801541947105482372"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","#from imblearn.over_sampling import RandomOverSampler\n","import numpy as np\n","from sklearn.utils import class_weight\n","\n","data_test = keras.preprocessing.image.ImageDataGenerator(\n","    preprocessing_function=lambda x: x / 255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/test\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rf231LcoBGeW","executionInfo":{"status":"ok","timestamp":1686412571040,"user_tz":-180,"elapsed":3980,"user":{"displayName":"YAKUP CAN KARACAOGLU","userId":"14801541947105482372"}},"outputId":"e93ee9f3-d4e6-43da-dee5-60008a52f5f4"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 624 images belonging to 2 classes.\n"]}]},{"cell_type":"code","source":["loaded_model.evaluate(data_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wRUt-HzDBT81","executionInfo":{"status":"ok","timestamp":1686412755096,"user_tz":-180,"elapsed":148716,"user":{"displayName":"YAKUP CAN KARACAOGLU","userId":"14801541947105482372"}},"outputId":"f83a67c5-7c86-41b9-9020-0bc4fcd6378f"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["20/20 [==============================] - 88s 4s/step - loss: 0.3936 - accuracy: 0.8333\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.39356017112731934, 0.8333333134651184]"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":[],"metadata":{"id":"LWubG7nYBjjo"},"execution_count":null,"outputs":[]}]}