{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOGWnvg0oOJ/FRl3fS5Jixs"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"07muq02Zi9Jg","executionInfo":{"status":"ok","timestamp":1686140799183,"user_tz":-180,"elapsed":2830,"user":{"displayName":"YAKUP CAN KARACAOGLU","userId":"14801541947105482372"}},"outputId":"1554b11c-d09f-432d-d910-b687cf9a241a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount(\"/content/gdrive\")"]},{"cell_type":"code","source":["import numpy as np\n","lr_array = np.array([0.07, 0.09])\n","momentum_array = np.array([0.9, 0.92, 0.94])\n","import tensorflow as tf\n","from tensorflow import keras\n","#from imblearn.over_sampling import RandomOverSampler\n","import numpy as np\n","from sklearn.utils import class_weight"],"metadata":{"id":"pPJwPJXnjDoq","executionInfo":{"status":"ok","timestamp":1686140816292,"user_tz":-180,"elapsed":4063,"user":{"displayName":"YAKUP CAN KARACAOGLU","userId":"14801541947105482372"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["for lr in lr_array:\n","  for momentum_var in momentum_array:\n","    print(\"learning rate\",lr)\n","    print(\"momentum\", momentum_var)\n","    \n","\n","    data = keras.preprocessing.image.ImageDataGenerator(\n","    preprocessing_function=lambda x:x/255\n","    ).flow_from_directory(\n","        \"/content/gdrive/MyDrive/chest/datai2/chest_xray/train\",\n","        target_size=(1600, 1200),\n","        color_mode='grayscale',\n","        classes=None,\n","        class_mode='categorical',\n","        batch_size=32,\n","        shuffle=True,\n","        seed=None,\n","        save_to_dir=None,\n","        save_prefix='',\n","        save_format='png',\n","        follow_links=False,\n","        subset=None,\n","        interpolation='nearest',\n","        keep_aspect_ratio=False\n","    )\n","\n","\n","    # Calculate class weights\n","    class_weights = np.array([1, 1])\n","\n","    model = keras.Sequential([\n","        keras.layers.Input((1600, 1200, 1)),\n","        keras.layers.Conv2D(8, (5, 5), strides=(3, 3), activation='relu'),\n","        keras.layers.Dropout(0.1),\n","        keras.layers.BatchNormalization(),\n","        keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","        keras.layers.Dropout(0.2),\n","        keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","        keras.layers.Conv2D(16, (7, 7), strides=(3, 3), activation='relu'),\n","        keras.layers.Dropout(0.1),\n","        keras.layers.MaxPool2D((2, 2)),\n","        keras.layers.Conv2D(32, (5, 5), strides=(2, 2), activation='relu'),\n","        keras.layers.Dropout(0.2),\n","        keras.layers.MaxPool2D((2, 2)),\n","        keras.layers.Flatten(),\n","        keras.layers.Dense(32, activation=\"relu\"),\n","        keras.layers.Dense(2, activation=\"softmax\"),\n","    ])\n","\n","    model.compile(\n","        optimizer=keras.optimizers.SGD(learning_rate=lr, nesterov=True, momentum=momentum_var),\n","        loss=tf.keras.losses.CategoricalCrossentropy(),\n","        metrics=[\"accuracy\"]\n","    )\n","\n","    model.summary()\n","\n","    history = model.fit(data, verbose=1, epochs=3)\n","\n","    data_test = keras.preprocessing.image.ImageDataGenerator(\n","        preprocessing_function=lambda x: x / 255\n","    ).flow_from_directory(\n","        \"/content/gdrive/MyDrive/chest/datai2/chest_xray/test\",\n","        target_size=(1600, 1200),\n","        color_mode='grayscale',\n","        classes=None,\n","        class_mode='categorical',\n","        batch_size=32,\n","        shuffle=True,\n","        seed=None,\n","        save_to_dir=None,\n","        save_prefix='',\n","        save_format='png',\n","        follow_links=False,\n","        subset=None,\n","        interpolation='nearest',\n","        keep_aspect_ratio=False\n","    )\n","\n","    model.evaluate(data_test)\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"zITYyhURjgVK","executionInfo":{"status":"error","timestamp":1686140763624,"user_tz":-180,"elapsed":2479325,"user":{"displayName":"YAKUP CAN KARACAOGLU","userId":"14801541947105482372"}},"outputId":"6543dc0b-341d-4e37-8b60-c682b86a4a95"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["learning rate 0.07\n","momentum 0.9\n","Found 5232 images belonging to 2 classes.\n","Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_5 (Conv2D)           (None, 532, 399, 8)       208       \n","                                                                 \n"," dropout_4 (Dropout)         (None, 532, 399, 8)       0         \n","                                                                 \n"," batch_normalization_1 (Batc  (None, 532, 399, 8)      32        \n"," hNormalization)                                                 \n","                                                                 \n"," conv2d_6 (Conv2D)           (None, 264, 198, 8)       1608      \n","                                                                 \n"," dropout_5 (Dropout)         (None, 264, 198, 8)       0         \n","                                                                 \n"," conv2d_7 (Conv2D)           (None, 130, 97, 8)        1608      \n","                                                                 \n"," conv2d_8 (Conv2D)           (None, 42, 31, 16)        6288      \n","                                                                 \n"," dropout_6 (Dropout)         (None, 42, 31, 16)        0         \n","                                                                 \n"," max_pooling2d_2 (MaxPooling  (None, 21, 15, 16)       0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_9 (Conv2D)           (None, 9, 6, 32)          12832     \n","                                                                 \n"," dropout_7 (Dropout)         (None, 9, 6, 32)          0         \n","                                                                 \n"," max_pooling2d_3 (MaxPooling  (None, 4, 3, 32)         0         \n"," 2D)                                                             \n","                                                                 \n"," flatten_1 (Flatten)         (None, 384)               0         \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                12320     \n","                                                                 \n"," dense_3 (Dense)             (None, 2)                 66        \n","                                                                 \n","=================================================================\n","Total params: 34,962\n","Trainable params: 34,946\n","Non-trainable params: 16\n","_________________________________________________________________\n","Epoch 1/3\n","164/164 [==============================] - 181s 1s/step - loss: 0.6017 - accuracy: 0.7372\n","Epoch 2/3\n","164/164 [==============================] - 177s 1s/step - loss: 0.5730 - accuracy: 0.7422\n","Epoch 3/3\n","164/164 [==============================] - 176s 1s/step - loss: 0.5727 - accuracy: 0.7422\n","Found 624 images belonging to 2 classes.\n","20/20 [==============================] - 148s 8s/step - loss: 0.7075 - accuracy: 0.6250\n","learning rate 0.07\n","momentum 0.92\n","Found 5232 images belonging to 2 classes.\n","Model: \"sequential_2\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_10 (Conv2D)          (None, 532, 399, 8)       208       \n","                                                                 \n"," dropout_8 (Dropout)         (None, 532, 399, 8)       0         \n","                                                                 \n"," batch_normalization_2 (Batc  (None, 532, 399, 8)      32        \n"," hNormalization)                                                 \n","                                                                 \n"," conv2d_11 (Conv2D)          (None, 264, 198, 8)       1608      \n","                                                                 \n"," dropout_9 (Dropout)         (None, 264, 198, 8)       0         \n","                                                                 \n"," conv2d_12 (Conv2D)          (None, 130, 97, 8)        1608      \n","                                                                 \n"," conv2d_13 (Conv2D)          (None, 42, 31, 16)        6288      \n","                                                                 \n"," dropout_10 (Dropout)        (None, 42, 31, 16)        0         \n","                                                                 \n"," max_pooling2d_4 (MaxPooling  (None, 21, 15, 16)       0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_14 (Conv2D)          (None, 9, 6, 32)          12832     \n","                                                                 \n"," dropout_11 (Dropout)        (None, 9, 6, 32)          0         \n","                                                                 \n"," max_pooling2d_5 (MaxPooling  (None, 4, 3, 32)         0         \n"," 2D)                                                             \n","                                                                 \n"," flatten_2 (Flatten)         (None, 384)               0         \n","                                                                 \n"," dense_4 (Dense)             (None, 32)                12320     \n","                                                                 \n"," dense_5 (Dense)             (None, 2)                 66        \n","                                                                 \n","=================================================================\n","Total params: 34,962\n","Trainable params: 34,946\n","Non-trainable params: 16\n","_________________________________________________________________\n","Epoch 1/3\n","164/164 [==============================] - 183s 1s/step - loss: 0.5792 - accuracy: 0.7383\n","Epoch 2/3\n","164/164 [==============================] - 178s 1s/step - loss: 0.5713 - accuracy: 0.7429\n","Epoch 3/3\n","164/164 [==============================] - 176s 1s/step - loss: 0.5748 - accuracy: 0.7422\n","Found 624 images belonging to 2 classes.\n","20/20 [==============================] - 19s 918ms/step - loss: 0.7018 - accuracy: 0.6250\n","learning rate 0.07\n","momentum 0.94\n","Found 5232 images belonging to 2 classes.\n","Model: \"sequential_3\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_15 (Conv2D)          (None, 532, 399, 8)       208       \n","                                                                 \n"," dropout_12 (Dropout)        (None, 532, 399, 8)       0         \n","                                                                 \n"," batch_normalization_3 (Batc  (None, 532, 399, 8)      32        \n"," hNormalization)                                                 \n","                                                                 \n"," conv2d_16 (Conv2D)          (None, 264, 198, 8)       1608      \n","                                                                 \n"," dropout_13 (Dropout)        (None, 264, 198, 8)       0         \n","                                                                 \n"," conv2d_17 (Conv2D)          (None, 130, 97, 8)        1608      \n","                                                                 \n"," conv2d_18 (Conv2D)          (None, 42, 31, 16)        6288      \n","                                                                 \n"," dropout_14 (Dropout)        (None, 42, 31, 16)        0         \n","                                                                 \n"," max_pooling2d_6 (MaxPooling  (None, 21, 15, 16)       0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_19 (Conv2D)          (None, 9, 6, 32)          12832     \n","                                                                 \n"," dropout_15 (Dropout)        (None, 9, 6, 32)          0         \n","                                                                 \n"," max_pooling2d_7 (MaxPooling  (None, 4, 3, 32)         0         \n"," 2D)                                                             \n","                                                                 \n"," flatten_3 (Flatten)         (None, 384)               0         \n","                                                                 \n"," dense_6 (Dense)             (None, 32)                12320     \n","                                                                 \n"," dense_7 (Dense)             (None, 2)                 66        \n","                                                                 \n","=================================================================\n","Total params: 34,962\n","Trainable params: 34,946\n","Non-trainable params: 16\n","_________________________________________________________________\n","Epoch 1/3\n","164/164 [==============================] - 180s 1s/step - loss: 0.5837 - accuracy: 0.7366\n","Epoch 2/3\n","164/164 [==============================] - 178s 1s/step - loss: 959.0492 - accuracy: 0.7307\n","Epoch 3/3\n","164/164 [==============================] - 179s 1s/step - loss: 8.9063 - accuracy: 0.7422\n","Found 624 images belonging to 2 classes.\n","20/20 [==============================] - 19s 962ms/step - loss: 0.6780 - accuracy: 0.6250\n","learning rate 0.09\n","momentum 0.9\n","Found 5232 images belonging to 2 classes.\n","Model: \"sequential_4\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_20 (Conv2D)          (None, 532, 399, 8)       208       \n","                                                                 \n"," dropout_16 (Dropout)        (None, 532, 399, 8)       0         \n","                                                                 \n"," batch_normalization_4 (Batc  (None, 532, 399, 8)      32        \n"," hNormalization)                                                 \n","                                                                 \n"," conv2d_21 (Conv2D)          (None, 264, 198, 8)       1608      \n","                                                                 \n"," dropout_17 (Dropout)        (None, 264, 198, 8)       0         \n","                                                                 \n"," conv2d_22 (Conv2D)          (None, 130, 97, 8)        1608      \n","                                                                 \n"," conv2d_23 (Conv2D)          (None, 42, 31, 16)        6288      \n","                                                                 \n"," dropout_18 (Dropout)        (None, 42, 31, 16)        0         \n","                                                                 \n"," max_pooling2d_8 (MaxPooling  (None, 21, 15, 16)       0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_24 (Conv2D)          (None, 9, 6, 32)          12832     \n","                                                                 \n"," dropout_19 (Dropout)        (None, 9, 6, 32)          0         \n","                                                                 \n"," max_pooling2d_9 (MaxPooling  (None, 4, 3, 32)         0         \n"," 2D)                                                             \n","                                                                 \n"," flatten_4 (Flatten)         (None, 384)               0         \n","                                                                 \n"," dense_8 (Dense)             (None, 32)                12320     \n","                                                                 \n"," dense_9 (Dense)             (None, 2)                 66        \n","                                                                 \n","=================================================================\n","Total params: 34,962\n","Trainable params: 34,946\n","Non-trainable params: 16\n","_________________________________________________________________\n","Epoch 1/3\n","164/164 [==============================] - 179s 1s/step - loss: 0.6022 - accuracy: 0.7345\n","Epoch 2/3\n","164/164 [==============================] - 178s 1s/step - loss: 0.5739 - accuracy: 0.7422\n","Epoch 3/3\n","164/164 [==============================] - 176s 1s/step - loss: 0.5724 - accuracy: 0.7422\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-81473f08276e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     data_test = keras.preprocessing.image.ImageDataGenerator(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["lr_array = np.array([0.007, 0.009])\n","momentum_array = np.array([0.9, 0.92, 0.94])"],"metadata":{"id":"1Lo9imeGkkwE","executionInfo":{"status":"ok","timestamp":1686140820292,"user_tz":-180,"elapsed":506,"user":{"displayName":"YAKUP CAN KARACAOGLU","userId":"14801541947105482372"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["for lr in lr_array:\n","  for momentum_var in momentum_array:\n","    print(\"*\"*20)\n","    print(\"*\"*20)\n","    print(\"learning rate\",lr)\n","    print(\"momentum\", momentum_var)\n","    \n","\n","    data = keras.preprocessing.image.ImageDataGenerator(\n","    preprocessing_function=lambda x:x/255\n","    ).flow_from_directory(\n","        \"/content/gdrive/MyDrive/chest/datai2/chest_xray/train\",\n","        target_size=(1600, 1200),\n","        color_mode='grayscale',\n","        classes=None,\n","        class_mode='categorical',\n","        batch_size=32,\n","        shuffle=True,\n","        seed=None,\n","        save_to_dir=None,\n","        save_prefix='',\n","        save_format='png',\n","        follow_links=False,\n","        subset=None,\n","        interpolation='nearest',\n","        keep_aspect_ratio=False\n","    )\n","\n","\n","    # Calculate class weights\n","    class_weights = np.array([1, 1])\n","\n","    model = keras.Sequential([\n","        keras.layers.Input((1600, 1200, 1)),\n","        keras.layers.Conv2D(8, (5, 5), strides=(3, 3), activation='relu'),\n","        keras.layers.Dropout(0.1),\n","        keras.layers.BatchNormalization(),\n","        keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","        keras.layers.Dropout(0.2),\n","        keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","        keras.layers.Conv2D(16, (7, 7), strides=(3, 3), activation='relu'),\n","        keras.layers.Dropout(0.1),\n","        keras.layers.MaxPool2D((2, 2)),\n","        keras.layers.Conv2D(32, (5, 5), strides=(2, 2), activation='relu'),\n","        keras.layers.Dropout(0.2),\n","        keras.layers.MaxPool2D((2, 2)),\n","        keras.layers.Flatten(),\n","        keras.layers.Dense(32, activation=\"relu\"),\n","        keras.layers.Dense(2, activation=\"softmax\"),\n","    ])\n","\n","    model.compile(\n","        optimizer=keras.optimizers.SGD(learning_rate=lr, nesterov=True, momentum=momentum_var),\n","        loss=tf.keras.losses.CategoricalCrossentropy(),\n","        metrics=[\"accuracy\"]\n","    )\n","\n","    model.summary()\n","\n","    history = model.fit(data, verbose=1, epochs=3)\n","\n","    data_test = keras.preprocessing.image.ImageDataGenerator(\n","        preprocessing_function=lambda x: x / 255\n","    ).flow_from_directory(\n","        \"/content/gdrive/MyDrive/chest/datai2/chest_xray/test\",\n","        target_size=(1600, 1200),\n","        color_mode='grayscale',\n","        classes=None,\n","        class_mode='categorical',\n","        batch_size=32,\n","        shuffle=True,\n","        seed=None,\n","        save_to_dir=None,\n","        save_prefix='',\n","        save_format='png',\n","        follow_links=False,\n","        subset=None,\n","        interpolation='nearest',\n","        keep_aspect_ratio=False\n","    )\n","\n","    model.evaluate(data_test)\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bbRZ-AiA0uLb","executionInfo":{"status":"ok","timestamp":1686144435435,"user_tz":-180,"elapsed":3597838,"user":{"displayName":"YAKUP CAN KARACAOGLU","userId":"14801541947105482372"}},"outputId":"b95d1feb-e65b-4a92-bbed-dc1c89f141c7"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["********************\n","********************\n","learning rate 0.007\n","momentum 0.9\n","Found 5232 images belonging to 2 classes.\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d (Conv2D)             (None, 532, 399, 8)       208       \n","                                                                 \n"," dropout (Dropout)           (None, 532, 399, 8)       0         \n","                                                                 \n"," batch_normalization (BatchN  (None, 532, 399, 8)      32        \n"," ormalization)                                                   \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 264, 198, 8)       1608      \n","                                                                 \n"," dropout_1 (Dropout)         (None, 264, 198, 8)       0         \n","                                                                 \n"," conv2d_2 (Conv2D)           (None, 130, 97, 8)        1608      \n","                                                                 \n"," conv2d_3 (Conv2D)           (None, 42, 31, 16)        6288      \n","                                                                 \n"," dropout_2 (Dropout)         (None, 42, 31, 16)        0         \n","                                                                 \n"," max_pooling2d (MaxPooling2D  (None, 21, 15, 16)       0         \n"," )                                                               \n","                                                                 \n"," conv2d_4 (Conv2D)           (None, 9, 6, 32)          12832     \n","                                                                 \n"," dropout_3 (Dropout)         (None, 9, 6, 32)          0         \n","                                                                 \n"," max_pooling2d_1 (MaxPooling  (None, 4, 3, 32)         0         \n"," 2D)                                                             \n","                                                                 \n"," flatten (Flatten)           (None, 384)               0         \n","                                                                 \n"," dense (Dense)               (None, 32)                12320     \n","                                                                 \n"," dense_1 (Dense)             (None, 2)                 66        \n","                                                                 \n","=================================================================\n","Total params: 34,962\n","Trainable params: 34,946\n","Non-trainable params: 16\n","_________________________________________________________________\n","Epoch 1/3\n","164/164 [==============================] - 181s 1s/step - loss: 0.2785 - accuracy: 0.8809\n","Epoch 2/3\n","164/164 [==============================] - 177s 1s/step - loss: 0.1449 - accuracy: 0.9472\n","Epoch 3/3\n","164/164 [==============================] - 174s 1s/step - loss: 0.1347 - accuracy: 0.9507\n","Found 624 images belonging to 2 classes.\n","20/20 [==============================] - 19s 942ms/step - loss: 0.4593 - accuracy: 0.7692\n","********************\n","********************\n","learning rate 0.007\n","momentum 0.92\n","Found 5232 images belonging to 2 classes.\n","Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_5 (Conv2D)           (None, 532, 399, 8)       208       \n","                                                                 \n"," dropout_4 (Dropout)         (None, 532, 399, 8)       0         \n","                                                                 \n"," batch_normalization_1 (Batc  (None, 532, 399, 8)      32        \n"," hNormalization)                                                 \n","                                                                 \n"," conv2d_6 (Conv2D)           (None, 264, 198, 8)       1608      \n","                                                                 \n"," dropout_5 (Dropout)         (None, 264, 198, 8)       0         \n","                                                                 \n"," conv2d_7 (Conv2D)           (None, 130, 97, 8)        1608      \n","                                                                 \n"," conv2d_8 (Conv2D)           (None, 42, 31, 16)        6288      \n","                                                                 \n"," dropout_6 (Dropout)         (None, 42, 31, 16)        0         \n","                                                                 \n"," max_pooling2d_2 (MaxPooling  (None, 21, 15, 16)       0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_9 (Conv2D)           (None, 9, 6, 32)          12832     \n","                                                                 \n"," dropout_7 (Dropout)         (None, 9, 6, 32)          0         \n","                                                                 \n"," max_pooling2d_3 (MaxPooling  (None, 4, 3, 32)         0         \n"," 2D)                                                             \n","                                                                 \n"," flatten_1 (Flatten)         (None, 384)               0         \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                12320     \n","                                                                 \n"," dense_3 (Dense)             (None, 2)                 66        \n","                                                                 \n","=================================================================\n","Total params: 34,962\n","Trainable params: 34,946\n","Non-trainable params: 16\n","_________________________________________________________________\n","Epoch 1/3\n","164/164 [==============================] - 179s 1s/step - loss: 0.3048 - accuracy: 0.8779\n","Epoch 2/3\n","164/164 [==============================] - 177s 1s/step - loss: 0.1535 - accuracy: 0.9442\n","Epoch 3/3\n","164/164 [==============================] - 182s 1s/step - loss: 0.1237 - accuracy: 0.9539\n","Found 624 images belonging to 2 classes.\n","20/20 [==============================] - 19s 949ms/step - loss: 0.9055 - accuracy: 0.6651\n","********************\n","********************\n","learning rate 0.007\n","momentum 0.94\n","Found 5232 images belonging to 2 classes.\n","Model: \"sequential_2\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_10 (Conv2D)          (None, 532, 399, 8)       208       \n","                                                                 \n"," dropout_8 (Dropout)         (None, 532, 399, 8)       0         \n","                                                                 \n"," batch_normalization_2 (Batc  (None, 532, 399, 8)      32        \n"," hNormalization)                                                 \n","                                                                 \n"," conv2d_11 (Conv2D)          (None, 264, 198, 8)       1608      \n","                                                                 \n"," dropout_9 (Dropout)         (None, 264, 198, 8)       0         \n","                                                                 \n"," conv2d_12 (Conv2D)          (None, 130, 97, 8)        1608      \n","                                                                 \n"," conv2d_13 (Conv2D)          (None, 42, 31, 16)        6288      \n","                                                                 \n"," dropout_10 (Dropout)        (None, 42, 31, 16)        0         \n","                                                                 \n"," max_pooling2d_4 (MaxPooling  (None, 21, 15, 16)       0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_14 (Conv2D)          (None, 9, 6, 32)          12832     \n","                                                                 \n"," dropout_11 (Dropout)        (None, 9, 6, 32)          0         \n","                                                                 \n"," max_pooling2d_5 (MaxPooling  (None, 4, 3, 32)         0         \n"," 2D)                                                             \n","                                                                 \n"," flatten_2 (Flatten)         (None, 384)               0         \n","                                                                 \n"," dense_4 (Dense)             (None, 32)                12320     \n","                                                                 \n"," dense_5 (Dense)             (None, 2)                 66        \n","                                                                 \n","=================================================================\n","Total params: 34,962\n","Trainable params: 34,946\n","Non-trainable params: 16\n","_________________________________________________________________\n","Epoch 1/3\n","164/164 [==============================] - 181s 1s/step - loss: 0.3450 - accuracy: 0.8542\n","Epoch 2/3\n","164/164 [==============================] - 179s 1s/step - loss: 0.1542 - accuracy: 0.9423\n","Epoch 3/3\n","164/164 [==============================] - 183s 1s/step - loss: 0.1332 - accuracy: 0.9537\n","Found 624 images belonging to 2 classes.\n","20/20 [==============================] - 19s 950ms/step - loss: 0.3954 - accuracy: 0.8173\n","********************\n","********************\n","learning rate 0.009\n","momentum 0.9\n","Found 5232 images belonging to 2 classes.\n","Model: \"sequential_3\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_15 (Conv2D)          (None, 532, 399, 8)       208       \n","                                                                 \n"," dropout_12 (Dropout)        (None, 532, 399, 8)       0         \n","                                                                 \n"," batch_normalization_3 (Batc  (None, 532, 399, 8)      32        \n"," hNormalization)                                                 \n","                                                                 \n"," conv2d_16 (Conv2D)          (None, 264, 198, 8)       1608      \n","                                                                 \n"," dropout_13 (Dropout)        (None, 264, 198, 8)       0         \n","                                                                 \n"," conv2d_17 (Conv2D)          (None, 130, 97, 8)        1608      \n","                                                                 \n"," conv2d_18 (Conv2D)          (None, 42, 31, 16)        6288      \n","                                                                 \n"," dropout_14 (Dropout)        (None, 42, 31, 16)        0         \n","                                                                 \n"," max_pooling2d_6 (MaxPooling  (None, 21, 15, 16)       0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_19 (Conv2D)          (None, 9, 6, 32)          12832     \n","                                                                 \n"," dropout_15 (Dropout)        (None, 9, 6, 32)          0         \n","                                                                 \n"," max_pooling2d_7 (MaxPooling  (None, 4, 3, 32)         0         \n"," 2D)                                                             \n","                                                                 \n"," flatten_3 (Flatten)         (None, 384)               0         \n","                                                                 \n"," dense_6 (Dense)             (None, 32)                12320     \n","                                                                 \n"," dense_7 (Dense)             (None, 2)                 66        \n","                                                                 \n","=================================================================\n","Total params: 34,962\n","Trainable params: 34,946\n","Non-trainable params: 16\n","_________________________________________________________________\n","Epoch 1/3\n","164/164 [==============================] - 185s 1s/step - loss: 0.6010 - accuracy: 0.7458\n","Epoch 2/3\n","164/164 [==============================] - 180s 1s/step - loss: 0.4161 - accuracy: 0.8022\n","Epoch 3/3\n","164/164 [==============================] - 179s 1s/step - loss: 0.2004 - accuracy: 0.9283\n","Found 624 images belonging to 2 classes.\n","20/20 [==============================] - 19s 952ms/step - loss: 0.4437 - accuracy: 0.8333\n","********************\n","********************\n","learning rate 0.009\n","momentum 0.92\n","Found 5232 images belonging to 2 classes.\n","Model: \"sequential_4\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_20 (Conv2D)          (None, 532, 399, 8)       208       \n","                                                                 \n"," dropout_16 (Dropout)        (None, 532, 399, 8)       0         \n","                                                                 \n"," batch_normalization_4 (Batc  (None, 532, 399, 8)      32        \n"," hNormalization)                                                 \n","                                                                 \n"," conv2d_21 (Conv2D)          (None, 264, 198, 8)       1608      \n","                                                                 \n"," dropout_17 (Dropout)        (None, 264, 198, 8)       0         \n","                                                                 \n"," conv2d_22 (Conv2D)          (None, 130, 97, 8)        1608      \n","                                                                 \n"," conv2d_23 (Conv2D)          (None, 42, 31, 16)        6288      \n","                                                                 \n"," dropout_18 (Dropout)        (None, 42, 31, 16)        0         \n","                                                                 \n"," max_pooling2d_8 (MaxPooling  (None, 21, 15, 16)       0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_24 (Conv2D)          (None, 9, 6, 32)          12832     \n","                                                                 \n"," dropout_19 (Dropout)        (None, 9, 6, 32)          0         \n","                                                                 \n"," max_pooling2d_9 (MaxPooling  (None, 4, 3, 32)         0         \n"," 2D)                                                             \n","                                                                 \n"," flatten_4 (Flatten)         (None, 384)               0         \n","                                                                 \n"," dense_8 (Dense)             (None, 32)                12320     \n","                                                                 \n"," dense_9 (Dense)             (None, 2)                 66        \n","                                                                 \n","=================================================================\n","Total params: 34,962\n","Trainable params: 34,946\n","Non-trainable params: 16\n","_________________________________________________________________\n","Epoch 1/3\n","164/164 [==============================] - 181s 1s/step - loss: 0.4390 - accuracy: 0.8052\n","Epoch 2/3\n","164/164 [==============================] - 179s 1s/step - loss: 0.1683 - accuracy: 0.9394\n","Epoch 3/3\n","164/164 [==============================] - 176s 1s/step - loss: 0.1396 - accuracy: 0.9526\n","Found 624 images belonging to 2 classes.\n","20/20 [==============================] - 19s 945ms/step - loss: 0.5673 - accuracy: 0.7324\n","********************\n","********************\n","learning rate 0.009\n","momentum 0.94\n","Found 5232 images belonging to 2 classes.\n","Model: \"sequential_5\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_25 (Conv2D)          (None, 532, 399, 8)       208       \n","                                                                 \n"," dropout_20 (Dropout)        (None, 532, 399, 8)       0         \n","                                                                 \n"," batch_normalization_5 (Batc  (None, 532, 399, 8)      32        \n"," hNormalization)                                                 \n","                                                                 \n"," conv2d_26 (Conv2D)          (None, 264, 198, 8)       1608      \n","                                                                 \n"," dropout_21 (Dropout)        (None, 264, 198, 8)       0         \n","                                                                 \n"," conv2d_27 (Conv2D)          (None, 130, 97, 8)        1608      \n","                                                                 \n"," conv2d_28 (Conv2D)          (None, 42, 31, 16)        6288      \n","                                                                 \n"," dropout_22 (Dropout)        (None, 42, 31, 16)        0         \n","                                                                 \n"," max_pooling2d_10 (MaxPoolin  (None, 21, 15, 16)       0         \n"," g2D)                                                            \n","                                                                 \n"," conv2d_29 (Conv2D)          (None, 9, 6, 32)          12832     \n","                                                                 \n"," dropout_23 (Dropout)        (None, 9, 6, 32)          0         \n","                                                                 \n"," max_pooling2d_11 (MaxPoolin  (None, 4, 3, 32)         0         \n"," g2D)                                                            \n","                                                                 \n"," flatten_5 (Flatten)         (None, 384)               0         \n","                                                                 \n"," dense_10 (Dense)            (None, 32)                12320     \n","                                                                 \n"," dense_11 (Dense)            (None, 2)                 66        \n","                                                                 \n","=================================================================\n","Total params: 34,962\n","Trainable params: 34,946\n","Non-trainable params: 16\n","_________________________________________________________________\n","Epoch 1/3\n","164/164 [==============================] - 180s 1s/step - loss: 0.6661 - accuracy: 0.7404\n","Epoch 2/3\n","164/164 [==============================] - 176s 1s/step - loss: 0.5717 - accuracy: 0.7422\n","Epoch 3/3\n","164/164 [==============================] - 178s 1s/step - loss: 0.5713 - accuracy: 0.7422\n","Found 624 images belonging to 2 classes.\n","20/20 [==============================] - 19s 947ms/step - loss: 0.7032 - accuracy: 0.6250\n"]}]},{"cell_type":"code","source":["data = keras.preprocessing.image.ImageDataGenerator(\n","preprocessing_function=lambda x:x/255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/train\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","\n","# Calculate class weights\n","class_weights = np.array([1, 1])\n","\n","model = keras.Sequential([\n","    keras.layers.Input((1600, 1200, 1)),\n","    keras.layers.Conv2D(8, (5, 5), strides=(3, 3), activation='relu'),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.BatchNormalization(),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Dropout(0.2),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Conv2D(16, (7, 7), strides=(3, 3), activation='relu'),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Conv2D(32, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Dropout(0.2),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Flatten(),\n","    keras.layers.Dense(32, activation=\"relu\"),\n","    keras.layers.Dense(2, activation=\"softmax\"),\n","])\n","\n","model.compile(\n","    optimizer=keras.optimizers.SGD(learning_rate=0.014, nesterov=True, momentum=0.9),\n","    loss=tf.keras.losses.CategoricalCrossentropy(),\n","    metrics=[\"accuracy\"]\n",")\n","\n","model.summary()\n","\n","history = model.fit(data, verbose=1, epochs=3)\n","\n","data_test = keras.preprocessing.image.ImageDataGenerator(\n","    preprocessing_function=lambda x: x / 255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/test\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","model.evaluate(data_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OfGtRA0G01yU","executionInfo":{"status":"ok","timestamp":1686145784567,"user_tz":-180,"elapsed":584680,"user":{"displayName":"YAKUP CAN KARACAOGLU","userId":"14801541947105482372"}},"outputId":"8db77304-df60-42ce-d358-b046a3da2664"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 5232 images belonging to 2 classes.\n","Model: \"sequential_6\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_30 (Conv2D)          (None, 532, 399, 8)       208       \n","                                                                 \n"," dropout_24 (Dropout)        (None, 532, 399, 8)       0         \n","                                                                 \n"," batch_normalization_6 (Batc  (None, 532, 399, 8)      32        \n"," hNormalization)                                                 \n","                                                                 \n"," conv2d_31 (Conv2D)          (None, 264, 198, 8)       1608      \n","                                                                 \n"," dropout_25 (Dropout)        (None, 264, 198, 8)       0         \n","                                                                 \n"," conv2d_32 (Conv2D)          (None, 130, 97, 8)        1608      \n","                                                                 \n"," conv2d_33 (Conv2D)          (None, 42, 31, 16)        6288      \n","                                                                 \n"," dropout_26 (Dropout)        (None, 42, 31, 16)        0         \n","                                                                 \n"," max_pooling2d_12 (MaxPoolin  (None, 21, 15, 16)       0         \n"," g2D)                                                            \n","                                                                 \n"," conv2d_34 (Conv2D)          (None, 9, 6, 32)          12832     \n","                                                                 \n"," dropout_27 (Dropout)        (None, 9, 6, 32)          0         \n","                                                                 \n"," max_pooling2d_13 (MaxPoolin  (None, 4, 3, 32)         0         \n"," g2D)                                                            \n","                                                                 \n"," flatten_6 (Flatten)         (None, 384)               0         \n","                                                                 \n"," dense_12 (Dense)            (None, 32)                12320     \n","                                                                 \n"," dense_13 (Dense)            (None, 2)                 66        \n","                                                                 \n","=================================================================\n","Total params: 34,962\n","Trainable params: 34,946\n","Non-trainable params: 16\n","_________________________________________________________________\n","Epoch 1/3\n","164/164 [==============================] - 180s 1s/step - loss: 0.5728 - accuracy: 0.7412\n","Epoch 2/3\n","164/164 [==============================] - 178s 1s/step - loss: 0.4523 - accuracy: 0.7878\n","Epoch 3/3\n","164/164 [==============================] - 176s 1s/step - loss: 0.3729 - accuracy: 0.8368\n","Found 624 images belonging to 2 classes.\n","20/20 [==============================] - 19s 929ms/step - loss: 0.7635 - accuracy: 0.6538\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.7635278105735779, 0.6538461446762085]"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["data = keras.preprocessing.image.ImageDataGenerator(\n","preprocessing_function=lambda x:x/255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/train\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","\n","# Calculate class weights\n","class_weights = np.array([1, 1])\n","\n","model = keras.Sequential([\n","    keras.layers.Input((1600, 1200, 1)),\n","    keras.layers.Conv2D(8, (5, 5), strides=(3, 3), activation='relu'),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.BatchNormalization(),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Dropout(0.2),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Conv2D(16, (7, 7), strides=(3, 3), activation='relu'),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Conv2D(32, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Dropout(0.2),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Flatten(),\n","    keras.layers.Dense(32, activation=\"relu\"),\n","    keras.layers.Dense(2, activation=\"softmax\"),\n","])\n","\n","model.compile(\n","    optimizer=keras.optimizers.SGD(learning_rate=0.009, nesterov=True, momentum=0.9),\n","    loss=tf.keras.losses.CategoricalCrossentropy(),\n","    metrics=[\"accuracy\"]\n",")\n","\n","model.summary()\n","\n","history = model.fit(data, verbose=1, epochs=4)\n","\n","data_test = keras.preprocessing.image.ImageDataGenerator(\n","    preprocessing_function=lambda x: x / 255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/test\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","model.evaluate(data_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i8JPGVE8Feyn","executionInfo":{"status":"ok","timestamp":1686147401344,"user_tz":-180,"elapsed":785174,"user":{"displayName":"YAKUP CAN KARACAOGLU","userId":"14801541947105482372"}},"outputId":"28a332ec-78f0-4db4-b60a-09d560e7c37f"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 5232 images belonging to 2 classes.\n","Model: \"sequential_7\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_35 (Conv2D)          (None, 532, 399, 8)       208       \n","                                                                 \n"," dropout_28 (Dropout)        (None, 532, 399, 8)       0         \n","                                                                 \n"," batch_normalization_7 (Batc  (None, 532, 399, 8)      32        \n"," hNormalization)                                                 \n","                                                                 \n"," conv2d_36 (Conv2D)          (None, 264, 198, 8)       1608      \n","                                                                 \n"," dropout_29 (Dropout)        (None, 264, 198, 8)       0         \n","                                                                 \n"," conv2d_37 (Conv2D)          (None, 130, 97, 8)        1608      \n","                                                                 \n"," conv2d_38 (Conv2D)          (None, 42, 31, 16)        6288      \n","                                                                 \n"," dropout_30 (Dropout)        (None, 42, 31, 16)        0         \n","                                                                 \n"," max_pooling2d_14 (MaxPoolin  (None, 21, 15, 16)       0         \n"," g2D)                                                            \n","                                                                 \n"," conv2d_39 (Conv2D)          (None, 9, 6, 32)          12832     \n","                                                                 \n"," dropout_31 (Dropout)        (None, 9, 6, 32)          0         \n","                                                                 \n"," max_pooling2d_15 (MaxPoolin  (None, 4, 3, 32)         0         \n"," g2D)                                                            \n","                                                                 \n"," flatten_7 (Flatten)         (None, 384)               0         \n","                                                                 \n"," dense_14 (Dense)            (None, 32)                12320     \n","                                                                 \n"," dense_15 (Dense)            (None, 2)                 66        \n","                                                                 \n","=================================================================\n","Total params: 34,962\n","Trainable params: 34,946\n","Non-trainable params: 16\n","_________________________________________________________________\n","Epoch 1/4\n","164/164 [==============================] - 184s 1s/step - loss: 0.2913 - accuracy: 0.8796\n","Epoch 2/4\n","164/164 [==============================] - 176s 1s/step - loss: 0.1411 - accuracy: 0.9459\n","Epoch 3/4\n","164/164 [==============================] - 174s 1s/step - loss: 0.1277 - accuracy: 0.9509\n","Epoch 4/4\n","164/164 [==============================] - 176s 1s/step - loss: 0.1211 - accuracy: 0.9580\n","Found 624 images belonging to 2 classes.\n","20/20 [==============================] - 20s 991ms/step - loss: 0.4796 - accuracy: 0.7997\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.4795737564563751, 0.7996794581413269]"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["data = keras.preprocessing.image.ImageDataGenerator(\n","preprocessing_function=lambda x:x/255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/train\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","\n","# Calculate class weights\n","class_weights = np.array([1, 1])\n","\n","model = keras.Sequential([\n","    keras.layers.Input((1600, 1200, 1)),\n","    keras.layers.Conv2D(8, (5, 5), strides=(3, 3), activation='relu'),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.BatchNormalization(),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Dropout(0.2),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Conv2D(16, (7, 7), strides=(3, 3), activation='relu'),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Conv2D(32, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Dropout(0.2),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Flatten(),\n","    keras.layers.Dense(32, activation=\"relu\"),\n","    keras.layers.Dense(2, activation=\"softmax\"),\n","])\n","\n","model.compile(\n","    optimizer=keras.optimizers.SGD(learning_rate=0.09, nesterov=True, momentum=0.9),\n","    loss=tf.keras.losses.CategoricalCrossentropy(),\n","    metrics=[\"accuracy\"]\n",")\n","\n","model.summary()\n","\n","history = model.fit(data, verbose=1, epochs=3)\n","\n","data_test = keras.preprocessing.image.ImageDataGenerator(\n","    preprocessing_function=lambda x: x / 255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/test\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","model.evaluate(data_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OCdFu37UK4ZY","executionInfo":{"status":"ok","timestamp":1686148009021,"user_tz":-180,"elapsed":553961,"user":{"displayName":"YAKUP CAN KARACAOGLU","userId":"14801541947105482372"}},"outputId":"f9d32e18-d9d3-42d9-9544-2e71e1bd2813"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 5232 images belonging to 2 classes.\n","Model: \"sequential_8\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_40 (Conv2D)          (None, 532, 399, 8)       208       \n","                                                                 \n"," dropout_32 (Dropout)        (None, 532, 399, 8)       0         \n","                                                                 \n"," batch_normalization_8 (Batc  (None, 532, 399, 8)      32        \n"," hNormalization)                                                 \n","                                                                 \n"," conv2d_41 (Conv2D)          (None, 264, 198, 8)       1608      \n","                                                                 \n"," dropout_33 (Dropout)        (None, 264, 198, 8)       0         \n","                                                                 \n"," conv2d_42 (Conv2D)          (None, 130, 97, 8)        1608      \n","                                                                 \n"," conv2d_43 (Conv2D)          (None, 42, 31, 16)        6288      \n","                                                                 \n"," dropout_34 (Dropout)        (None, 42, 31, 16)        0         \n","                                                                 \n"," max_pooling2d_16 (MaxPoolin  (None, 21, 15, 16)       0         \n"," g2D)                                                            \n","                                                                 \n"," conv2d_44 (Conv2D)          (None, 9, 6, 32)          12832     \n","                                                                 \n"," dropout_35 (Dropout)        (None, 9, 6, 32)          0         \n","                                                                 \n"," max_pooling2d_17 (MaxPoolin  (None, 4, 3, 32)         0         \n"," g2D)                                                            \n","                                                                 \n"," flatten_8 (Flatten)         (None, 384)               0         \n","                                                                 \n"," dense_16 (Dense)            (None, 32)                12320     \n","                                                                 \n"," dense_17 (Dense)            (None, 2)                 66        \n","                                                                 \n","=================================================================\n","Total params: 34,962\n","Trainable params: 34,946\n","Non-trainable params: 16\n","_________________________________________________________________\n","Epoch 1/3\n","164/164 [==============================] - 181s 1s/step - loss: 0.7476 - accuracy: 0.7351\n","Epoch 2/3\n","164/164 [==============================] - 176s 1s/step - loss: 0.5743 - accuracy: 0.7422\n","Epoch 3/3\n","164/164 [==============================] - 175s 1s/step - loss: 0.5740 - accuracy: 0.7422\n","Found 624 images belonging to 2 classes.\n","20/20 [==============================] - 19s 936ms/step - loss: 0.6891 - accuracy: 0.6250\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.6891263127326965, 0.625]"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":[],"metadata":{"id":"2ecOXRw8OFYD"},"execution_count":null,"outputs":[]}]}