{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyN9lq/umXAaj5qdspWXM5R7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vm-Q_NO2nMni","executionInfo":{"status":"ok","timestamp":1686407271581,"user_tz":-180,"elapsed":59829,"user":{"displayName":"YAKUP CAN KARACAOGLU","userId":"14801541947105482372"}},"outputId":"d807d9ea-41ce-4459-f67d-9f35b0c4d953"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount(\"/content/gdrive\")"]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","#from imblearn.over_sampling import RandomOverSampler\n","import numpy as np\n","from sklearn.utils import class_weight\n","\n","data = keras.preprocessing.image.ImageDataGenerator(\n","preprocessing_function=lambda x:x/255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/train\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","\n","# Calculate class weights\n","#class_weights = np.array([1.3333, 0.8])\n","\n","model = keras.Sequential([\n","    keras.layers.Input((1600, 1200, 1)),\n","    keras.layers.Conv2D(8, (5, 5), strides=(3, 3), activation='relu'),\n","    keras.layers.BatchNormalization(),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Conv2D(16, (7, 7), strides=(3, 3), activation='relu'),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Conv2D(32, (9, 9), strides=(2, 2), activation='relu'),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Flatten(),\n","    keras.layers.Dense(32, activation=\"relu\"),\n","    keras.layers.Dense(2, activation=\"softmax\"),\n","])\n","\n","model.compile(\n","    optimizer=keras.optimizers.SGD(learning_rate=0.01, nesterov=True, momentum=0.8),\n","    loss=tf.keras.losses.CategoricalCrossentropy(),\n","    metrics=[\"accuracy\"]\n",")\n","\n","model.summary()\n","\n","history = model.fit(data, verbose=1, epochs=2)\n","\n","data_test = keras.preprocessing.image.ImageDataGenerator(\n","    preprocessing_function=lambda x: x / 255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/test\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","model.evaluate(data_test)\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":380},"id":"c58BITqznT-J","executionInfo":{"status":"error","timestamp":1686407279369,"user_tz":-180,"elapsed":7795,"user":{"displayName":"YAKUP CAN KARACAOGLU","userId":"14801541947105482372"}},"outputId":"a049dab7-8719-4109-9729-746049544abb"},"execution_count":2,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-8b9f108d2fa7>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m data = keras.preprocessing.image.ImageDataGenerator(\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mpreprocessing_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflow_from_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/preprocessing/image.py\u001b[0m in \u001b[0;36mflow_from_directory\u001b[0;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[1;32m   1646\u001b[0m                 \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0my\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0marray\u001b[0m \u001b[0mof\u001b[0m \u001b[0mcorresponding\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m         \"\"\"\n\u001b[0;32m-> 1648\u001b[0;31m         return DirectoryIterator(\n\u001b[0m\u001b[1;32m   1649\u001b[0m             \u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1650\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/preprocessing/image.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio, dtype)\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0mclasses_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m             \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    592\u001b[0m             \u001b[0mclasses_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilenames\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    769\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    605\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    608\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","#from imblearn.over_sampling import RandomOverSampler\n","import numpy as np\n","from sklearn.utils import class_weight\n","\n","data = keras.preprocessing.image.ImageDataGenerator(\n","preprocessing_function=lambda x:x/255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/train\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","\n","# Calculate class weights\n","#class_weights = np.array([1.3333, 0.8])\n","\n","model = keras.Sequential([\n","    keras.layers.Input((1600, 1200, 1)),\n","    keras.layers.Conv2D(8, (5, 5), strides=(3, 3), activation='relu'),\n","    keras.layers.BatchNormalization(),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Conv2D(16, (7, 7), strides=(3, 3), activation='relu'),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Conv2D(32, (9, 9), strides=(2, 2), activation='relu'),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Flatten(),\n","    keras.layers.Dense(32, activation=\"relu\"),\n","    keras.layers.Dense(2, activation=\"softmax\"),\n","])\n","\n","model.compile(\n","    optimizer=keras.optimizers.SGD(learning_rate=0.01, nesterov=True, momentum=0.9),\n","    loss=tf.keras.losses.CategoricalCrossentropy(),\n","    metrics=[\"accuracy\"]\n",")\n","\n","model.summary()\n","\n","history = model.fit(data, verbose=1, epochs=2)\n","\n","data_test = keras.preprocessing.image.ImageDataGenerator(\n","    preprocessing_function=lambda x: x / 255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/test\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","model.evaluate(data_test)\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hw0dtDylnst0","executionInfo":{"status":"ok","timestamp":1686039596756,"user_tz":-180,"elapsed":400874,"user":{"displayName":"YAKUP CAN KARACAOGLU","userId":"14801541947105482372"}},"outputId":"b402cf4d-5ba5-4ff9-c21e-9969a0ad3172"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 5232 images belonging to 2 classes.\n","Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_5 (Conv2D)           (None, 532, 399, 8)       208       \n","                                                                 \n"," batch_normalization_1 (Batc  (None, 532, 399, 8)      32        \n"," hNormalization)                                                 \n","                                                                 \n"," conv2d_6 (Conv2D)           (None, 264, 198, 8)       1608      \n","                                                                 \n"," dropout_1 (Dropout)         (None, 264, 198, 8)       0         \n","                                                                 \n"," conv2d_7 (Conv2D)           (None, 130, 97, 8)        1608      \n","                                                                 \n"," conv2d_8 (Conv2D)           (None, 42, 31, 16)        6288      \n","                                                                 \n"," max_pooling2d_2 (MaxPooling  (None, 21, 15, 16)       0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_9 (Conv2D)           (None, 7, 4, 32)          41504     \n","                                                                 \n"," dropout_2 (Dropout)         (None, 7, 4, 32)          0         \n","                                                                 \n"," max_pooling2d_3 (MaxPooling  (None, 3, 2, 32)         0         \n"," 2D)                                                             \n","                                                                 \n"," flatten_1 (Flatten)         (None, 192)               0         \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                6176      \n","                                                                 \n"," dense_3 (Dense)             (None, 2)                 66        \n","                                                                 \n","=================================================================\n","Total params: 57,490\n","Trainable params: 57,474\n","Non-trainable params: 16\n","_________________________________________________________________\n","Epoch 1/2\n","164/164 [==============================] - 177s 1s/step - loss: 0.3117 - accuracy: 0.8733\n","Epoch 2/2\n","164/164 [==============================] - 171s 1s/step - loss: 0.1334 - accuracy: 0.9501\n","Found 624 images belonging to 2 classes.\n","20/20 [==============================] - 18s 909ms/step - loss: 0.4848 - accuracy: 0.7404\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.48483335971832275, 0.7403846383094788]"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","#from imblearn.over_sampling import RandomOverSampler\n","import numpy as np\n","from sklearn.utils import class_weight\n","\n","data = keras.preprocessing.image.ImageDataGenerator(\n","preprocessing_function=lambda x:x/255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/train\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","\n","# Calculate class weights\n","#class_weights = np.array([1.3333, 0.8])\n","\n","model = keras.Sequential([\n","    keras.layers.Input((1600, 1200, 1)),\n","    keras.layers.Conv2D(8, (5, 5), strides=(3, 3), activation='relu'),\n","    keras.layers.BatchNormalization(),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Conv2D(16, (7, 7), strides=(3, 3), activation='relu'),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Conv2D(32, (9, 9), strides=(2, 2), activation='relu'),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Flatten(),\n","    keras.layers.Dense(32, activation=\"relu\"),\n","    keras.layers.Dense(2, activation=\"softmax\"),\n","])\n","\n","model.compile(\n","    optimizer=keras.optimizers.SGD(learning_rate=0.01, nesterov=True, momentum=0.99),\n","    loss=tf.keras.losses.CategoricalCrossentropy(),\n","    metrics=[\"accuracy\"]\n",")\n","\n","model.summary()\n","\n","history = model.fit(data, verbose=1, epochs=2)\n","\n","data_test = keras.preprocessing.image.ImageDataGenerator(\n","    preprocessing_function=lambda x: x / 255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/test\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","model.evaluate(data_test)\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vgqCcjfdxG55","executionInfo":{"status":"ok","timestamp":1686040015818,"user_tz":-180,"elapsed":401685,"user":{"displayName":"YAKUP CAN KARACAOGLU","userId":"14801541947105482372"}},"outputId":"58f006b0-1616-42c0-c8c6-454ec3834974"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 5232 images belonging to 2 classes.\n","Model: \"sequential_2\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_10 (Conv2D)          (None, 532, 399, 8)       208       \n","                                                                 \n"," batch_normalization_2 (Batc  (None, 532, 399, 8)      32        \n"," hNormalization)                                                 \n","                                                                 \n"," conv2d_11 (Conv2D)          (None, 264, 198, 8)       1608      \n","                                                                 \n"," dropout_3 (Dropout)         (None, 264, 198, 8)       0         \n","                                                                 \n"," conv2d_12 (Conv2D)          (None, 130, 97, 8)        1608      \n","                                                                 \n"," conv2d_13 (Conv2D)          (None, 42, 31, 16)        6288      \n","                                                                 \n"," max_pooling2d_4 (MaxPooling  (None, 21, 15, 16)       0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_14 (Conv2D)          (None, 7, 4, 32)          41504     \n","                                                                 \n"," dropout_4 (Dropout)         (None, 7, 4, 32)          0         \n","                                                                 \n"," max_pooling2d_5 (MaxPooling  (None, 3, 2, 32)         0         \n"," 2D)                                                             \n","                                                                 \n"," flatten_2 (Flatten)         (None, 192)               0         \n","                                                                 \n"," dense_4 (Dense)             (None, 32)                6176      \n","                                                                 \n"," dense_5 (Dense)             (None, 2)                 66        \n","                                                                 \n","=================================================================\n","Total params: 57,490\n","Trainable params: 57,474\n","Non-trainable params: 16\n","_________________________________________________________________\n","Epoch 1/2\n","164/164 [==============================] - 176s 1s/step - loss: 0.5760 - accuracy: 0.7468\n","Epoch 2/2\n","164/164 [==============================] - 172s 1s/step - loss: 0.5741 - accuracy: 0.7422\n","Found 624 images belonging to 2 classes.\n","20/20 [==============================] - 18s 905ms/step - loss: 0.6798 - accuracy: 0.6250\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.6797906160354614, 0.625]"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","#from imblearn.over_sampling import RandomOverSampler\n","import numpy as np\n","from sklearn.utils import class_weight\n","\n","data = keras.preprocessing.image.ImageDataGenerator(\n","preprocessing_function=lambda x:x/255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/train\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","\n","# Calculate class weights\n","#class_weights = np.array([1.3333, 0.8])\n","\n","model = keras.Sequential([\n","    keras.layers.Input((1600, 1200, 1)),\n","    keras.layers.Conv2D(8, (5, 5), strides=(3, 3), activation='relu'),\n","    keras.layers.BatchNormalization(),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Conv2D(16, (7, 7), strides=(3, 3), activation='relu'),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Conv2D(32, (9, 9), strides=(2, 2), activation='relu'),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Flatten(),\n","    keras.layers.Dense(32, activation=\"relu\"),\n","    keras.layers.Dense(2, activation=\"softmax\"),\n","])\n","\n","model.compile(\n","    optimizer=keras.optimizers.SGD(learning_rate=0.01, nesterov=True, momentum=0.95),\n","    loss=tf.keras.losses.CategoricalCrossentropy(),\n","    metrics=[\"accuracy\"]\n",")\n","\n","model.summary()\n","\n","history = model.fit(data, verbose=1, epochs=2)\n","\n","data_test = keras.preprocessing.image.ImageDataGenerator(\n","    preprocessing_function=lambda x: x / 255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/test\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","model.evaluate(data_test)\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TtrHVt7OytAi","executionInfo":{"status":"ok","timestamp":1686040487171,"user_tz":-180,"elapsed":402803,"user":{"displayName":"YAKUP CAN KARACAOGLU","userId":"14801541947105482372"}},"outputId":"68834393-88ac-4701-f0f3-32303978f877"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 5232 images belonging to 2 classes.\n","Model: \"sequential_3\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_15 (Conv2D)          (None, 532, 399, 8)       208       \n","                                                                 \n"," batch_normalization_3 (Batc  (None, 532, 399, 8)      32        \n"," hNormalization)                                                 \n","                                                                 \n"," conv2d_16 (Conv2D)          (None, 264, 198, 8)       1608      \n","                                                                 \n"," dropout_5 (Dropout)         (None, 264, 198, 8)       0         \n","                                                                 \n"," conv2d_17 (Conv2D)          (None, 130, 97, 8)        1608      \n","                                                                 \n"," conv2d_18 (Conv2D)          (None, 42, 31, 16)        6288      \n","                                                                 \n"," max_pooling2d_6 (MaxPooling  (None, 21, 15, 16)       0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_19 (Conv2D)          (None, 7, 4, 32)          41504     \n","                                                                 \n"," dropout_6 (Dropout)         (None, 7, 4, 32)          0         \n","                                                                 \n"," max_pooling2d_7 (MaxPooling  (None, 3, 2, 32)         0         \n"," 2D)                                                             \n","                                                                 \n"," flatten_3 (Flatten)         (None, 192)               0         \n","                                                                 \n"," dense_6 (Dense)             (None, 32)                6176      \n","                                                                 \n"," dense_7 (Dense)             (None, 2)                 66        \n","                                                                 \n","=================================================================\n","Total params: 57,490\n","Trainable params: 57,474\n","Non-trainable params: 16\n","_________________________________________________________________\n","Epoch 1/2\n","164/164 [==============================] - 177s 1s/step - loss: 0.3010 - accuracy: 0.8809\n","Epoch 2/2\n","164/164 [==============================] - 171s 1s/step - loss: 0.1814 - accuracy: 0.9318\n","Found 624 images belonging to 2 classes.\n","20/20 [==============================] - 19s 919ms/step - loss: 0.8094 - accuracy: 0.6987\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.8094497323036194, 0.6987179517745972]"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","#from imblearn.over_sampling import RandomOverSampler\n","import numpy as np\n","from sklearn.utils import class_weight\n","\n","data = keras.preprocessing.image.ImageDataGenerator(\n","preprocessing_function=lambda x:x/255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/train\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","\n","# Calculate class weights\n","#class_weights = np.array([1.3333, 0.8])\n","\n","model = keras.Sequential([\n","    keras.layers.Input((1600, 1200, 1)),\n","    keras.layers.Conv2D(8, (5, 5), strides=(3, 3), activation='relu'),\n","    keras.layers.BatchNormalization(),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Conv2D(16, (7, 7), strides=(3, 3), activation='relu'),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Conv2D(32, (9, 9), strides=(2, 2), activation='relu'),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Flatten(),\n","    keras.layers.Dense(32, activation=\"relu\"),\n","    keras.layers.Dense(2, activation=\"softmax\"),\n","])\n","\n","model.compile(\n","    optimizer=keras.optimizers.SGD(learning_rate=0.01, nesterov=True, momentum=0.92),\n","    loss=tf.keras.losses.CategoricalCrossentropy(),\n","    metrics=[\"accuracy\"]\n",")\n","\n","model.summary()\n","\n","history = model.fit(data, verbose=1, epochs=2)\n","\n","data_test = keras.preprocessing.image.ImageDataGenerator(\n","    preprocessing_function=lambda x: x / 255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/test\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","model.evaluate(data_test)\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V-geJtim0f1N","executionInfo":{"status":"ok","timestamp":1686040963249,"user_tz":-180,"elapsed":428306,"user":{"displayName":"YAKUP CAN KARACAOGLU","userId":"14801541947105482372"}},"outputId":"5cc95cfc-7791-4642-eb64-f5a2b127b693"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 5232 images belonging to 2 classes.\n","Model: \"sequential_4\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_20 (Conv2D)          (None, 532, 399, 8)       208       \n","                                                                 \n"," batch_normalization_4 (Batc  (None, 532, 399, 8)      32        \n"," hNormalization)                                                 \n","                                                                 \n"," conv2d_21 (Conv2D)          (None, 264, 198, 8)       1608      \n","                                                                 \n"," dropout_7 (Dropout)         (None, 264, 198, 8)       0         \n","                                                                 \n"," conv2d_22 (Conv2D)          (None, 130, 97, 8)        1608      \n","                                                                 \n"," conv2d_23 (Conv2D)          (None, 42, 31, 16)        6288      \n","                                                                 \n"," max_pooling2d_8 (MaxPooling  (None, 21, 15, 16)       0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_24 (Conv2D)          (None, 7, 4, 32)          41504     \n","                                                                 \n"," max_pooling2d_9 (MaxPooling  (None, 3, 2, 32)         0         \n"," 2D)                                                             \n","                                                                 \n"," flatten_4 (Flatten)         (None, 192)               0         \n","                                                                 \n"," dense_8 (Dense)             (None, 32)                6176      \n","                                                                 \n"," dense_9 (Dense)             (None, 2)                 66        \n","                                                                 \n","=================================================================\n","Total params: 57,490\n","Trainable params: 57,474\n","Non-trainable params: 16\n","_________________________________________________________________\n","Epoch 1/2\n","164/164 [==============================] - 176s 1s/step - loss: 0.2818 - accuracy: 0.8853\n","Epoch 2/2\n","164/164 [==============================] - 176s 1s/step - loss: 0.1495 - accuracy: 0.9459\n","Found 624 images belonging to 2 classes.\n","20/20 [==============================] - 20s 975ms/step - loss: 0.6111 - accuracy: 0.7244\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.6110696196556091, 0.7243589758872986]"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","#from imblearn.over_sampling import RandomOverSampler\n","import numpy as np\n","from sklearn.utils import class_weight\n","\n","data = keras.preprocessing.image.ImageDataGenerator(\n","preprocessing_function=lambda x:x/255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/train\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","\n","# Calculate class weights\n","#class_weights = np.array([1.3333, 0.8])\n","\n","model = keras.Sequential([\n","    keras.layers.Input((1600, 1200, 1)),\n","    keras.layers.Conv2D(8, (5, 5), strides=(3, 3), activation='relu'),\n","    keras.layers.BatchNormalization(),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Conv2D(16, (7, 7), strides=(3, 3), activation='relu'),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Conv2D(32, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Flatten(),\n","    keras.layers.Dense(32, activation=\"relu\"),\n","    keras.layers.Dense(2, activation=\"softmax\"),\n","])\n","\n","model.compile(\n","    optimizer=keras.optimizers.SGD(learning_rate=0.01, nesterov=True, momentum=0.92),\n","    loss=tf.keras.losses.CategoricalCrossentropy(),\n","    metrics=[\"accuracy\"]\n",")\n","\n","model.summary()\n","\n","history = model.fit(data, verbose=1, epochs=2)\n","\n","data_test = keras.preprocessing.image.ImageDataGenerator(\n","    preprocessing_function=lambda x: x / 255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/test\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","model.evaluate(data_test)\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PZLTNQ2b4Jb4","executionInfo":{"status":"ok","timestamp":1686041504626,"user_tz":-180,"elapsed":382423,"user":{"displayName":"YAKUP CAN KARACAOGLU","userId":"14801541947105482372"}},"outputId":"a648d909-f0f2-49a9-bcd5-6e64fe063b84"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 5232 images belonging to 2 classes.\n","Model: \"sequential_7\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_37 (Conv2D)          (None, 532, 399, 8)       208       \n","                                                                 \n"," batch_normalization_7 (Batc  (None, 532, 399, 8)      32        \n"," hNormalization)                                                 \n","                                                                 \n"," conv2d_38 (Conv2D)          (None, 264, 198, 8)       1608      \n","                                                                 \n"," dropout_12 (Dropout)        (None, 264, 198, 8)       0         \n","                                                                 \n"," conv2d_39 (Conv2D)          (None, 130, 97, 8)        1608      \n","                                                                 \n"," conv2d_40 (Conv2D)          (None, 42, 31, 16)        6288      \n","                                                                 \n"," max_pooling2d_14 (MaxPoolin  (None, 21, 15, 16)       0         \n"," g2D)                                                            \n","                                                                 \n"," conv2d_41 (Conv2D)          (None, 9, 6, 32)          12832     \n","                                                                 \n"," max_pooling2d_15 (MaxPoolin  (None, 4, 3, 32)         0         \n"," g2D)                                                            \n","                                                                 \n"," flatten_7 (Flatten)         (None, 384)               0         \n","                                                                 \n"," dense_14 (Dense)            (None, 32)                12320     \n","                                                                 \n"," dense_15 (Dense)            (None, 2)                 66        \n","                                                                 \n","=================================================================\n","Total params: 34,962\n","Trainable params: 34,946\n","Non-trainable params: 16\n","_________________________________________________________________\n","Epoch 1/2\n","164/164 [==============================] - 181s 1s/step - loss: 0.3680 - accuracy: 0.8479\n","Epoch 2/2\n","164/164 [==============================] - 177s 1s/step - loss: 0.1316 - accuracy: 0.9513\n","Found 624 images belonging to 2 classes.\n","20/20 [==============================] - 19s 935ms/step - loss: 0.5086 - accuracy: 0.7917\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.5085731744766235, 0.7916666865348816]"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","#from imblearn.over_sampling import RandomOverSampler\n","import numpy as np\n","from sklearn.utils import class_weight\n","\n","data = keras.preprocessing.image.ImageDataGenerator(\n","preprocessing_function=lambda x:x/255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/train\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","\n","# Calculate class weights\n","#class_weights = np.array([1.3333, 0.8])\n","\n","model = keras.Sequential([\n","    keras.layers.Input((1600, 1200, 1)),\n","    keras.layers.Conv2D(8, (5, 5), strides=(3, 3), activation='relu'),\n","    keras.layers.BatchNormalization(),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Conv2D(16, (3, 3), strides=(3, 3), activation='relu'),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Conv2D(32, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Flatten(),\n","    keras.layers.Dense(32, activation=\"relu\"),\n","    keras.layers.Dense(2, activation=\"softmax\"),\n","])\n","\n","model.compile(\n","    optimizer=keras.optimizers.SGD(learning_rate=0.01, nesterov=True, momentum=0.92),\n","    loss=tf.keras.losses.CategoricalCrossentropy(),\n","    metrics=[\"accuracy\"]\n",")\n","\n","model.summary()\n","\n","history = model.fit(data, verbose=1, epochs=2)\n","\n","data_test = keras.preprocessing.image.ImageDataGenerator(\n","    preprocessing_function=lambda x: x / 255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/test\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","model.evaluate(data_test)\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3sBjfO9B4dNf","executionInfo":{"status":"ok","timestamp":1686042433066,"user_tz":-180,"elapsed":404455,"user":{"displayName":"YAKUP CAN KARACAOGLU","userId":"14801541947105482372"}},"outputId":"0cfbb000-6ac5-4ea2-b353-4f1eead2ffaa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 5232 images belonging to 2 classes.\n","Model: \"sequential_8\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_42 (Conv2D)          (None, 532, 399, 8)       208       \n","                                                                 \n"," batch_normalization_8 (Batc  (None, 532, 399, 8)      32        \n"," hNormalization)                                                 \n","                                                                 \n"," conv2d_43 (Conv2D)          (None, 264, 198, 8)       1608      \n","                                                                 \n"," dropout_13 (Dropout)        (None, 264, 198, 8)       0         \n","                                                                 \n"," conv2d_44 (Conv2D)          (None, 130, 97, 8)        1608      \n","                                                                 \n"," conv2d_45 (Conv2D)          (None, 43, 32, 16)        1168      \n","                                                                 \n"," max_pooling2d_16 (MaxPoolin  (None, 21, 16, 16)       0         \n"," g2D)                                                            \n","                                                                 \n"," conv2d_46 (Conv2D)          (None, 9, 6, 32)          12832     \n","                                                                 \n"," max_pooling2d_17 (MaxPoolin  (None, 4, 3, 32)         0         \n"," g2D)                                                            \n","                                                                 \n"," flatten_8 (Flatten)         (None, 384)               0         \n","                                                                 \n"," dense_16 (Dense)            (None, 32)                12320     \n","                                                                 \n"," dense_17 (Dense)            (None, 2)                 66        \n","                                                                 \n","=================================================================\n","Total params: 29,842\n","Trainable params: 29,826\n","Non-trainable params: 16\n","_________________________________________________________________\n","Epoch 1/2\n","164/164 [==============================] - 178s 1s/step - loss: 0.2841 - accuracy: 0.8849\n","Epoch 2/2\n","164/164 [==============================] - 178s 1s/step - loss: 0.1246 - accuracy: 0.9545\n","Found 624 images belonging to 2 classes.\n","20/20 [==============================] - 20s 973ms/step - loss: 0.4861 - accuracy: 0.7885\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.4860967993736267, 0.7884615659713745]"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","#from imblearn.over_sampling import RandomOverSampler\n","import numpy as np\n","from sklearn.utils import class_weight\n","\n","data = keras.preprocessing.image.ImageDataGenerator(\n","preprocessing_function=lambda x:x/255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/train\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","\n","# Calculate class weights\n","#class_weights = np.array([1.3333, 0.8])\n","\n","model = keras.Sequential([\n","    keras.layers.Input((1600, 1200, 1)),\n","    keras.layers.Conv2D(8, (5, 5), strides=(3, 3), activation='relu'),\n","    keras.layers.BatchNormalization(),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Conv2D(16, (7, 7), strides=(3, 3), activation='relu'),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Conv2D(32, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Flatten(),\n","    keras.layers.Dense(32, activation=\"relu\"),\n","    keras.layers.Dense(2, activation=\"softmax\"),\n","])\n","\n","model.compile(\n","    optimizer=keras.optimizers.SGD(learning_rate=0.01, nesterov=True, momentum=0.92),\n","    loss=tf.keras.losses.CategoricalCrossentropy(),\n","    metrics=[\"accuracy\"]\n",")\n","\n","model.summary()\n","\n","history = model.fit(data, verbose=1, epochs=2)\n","\n","data_test = keras.preprocessing.image.ImageDataGenerator(\n","    preprocessing_function=lambda x: x / 255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/test\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","model.evaluate(data_test)\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vefLK2F376f6","executionInfo":{"status":"ok","timestamp":1686042882480,"user_tz":-180,"elapsed":404997,"user":{"displayName":"YAKUP CAN KARACAOGLU","userId":"14801541947105482372"}},"outputId":"21c79bec-3def-4219-8518-e2b395a85966"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 5232 images belonging to 2 classes.\n","Model: \"sequential_9\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_47 (Conv2D)          (None, 532, 399, 8)       208       \n","                                                                 \n"," batch_normalization_9 (Batc  (None, 532, 399, 8)      32        \n"," hNormalization)                                                 \n","                                                                 \n"," conv2d_48 (Conv2D)          (None, 264, 198, 8)       1608      \n","                                                                 \n"," dropout_14 (Dropout)        (None, 264, 198, 8)       0         \n","                                                                 \n"," conv2d_49 (Conv2D)          (None, 130, 97, 8)        1608      \n","                                                                 \n"," conv2d_50 (Conv2D)          (None, 42, 31, 16)        6288      \n","                                                                 \n"," max_pooling2d_18 (MaxPoolin  (None, 21, 15, 16)       0         \n"," g2D)                                                            \n","                                                                 \n"," conv2d_51 (Conv2D)          (None, 9, 6, 32)          12832     \n","                                                                 \n"," dropout_15 (Dropout)        (None, 9, 6, 32)          0         \n","                                                                 \n"," max_pooling2d_19 (MaxPoolin  (None, 4, 3, 32)         0         \n"," g2D)                                                            \n","                                                                 \n"," flatten_9 (Flatten)         (None, 384)               0         \n","                                                                 \n"," dense_18 (Dense)            (None, 32)                12320     \n","                                                                 \n"," dense_19 (Dense)            (None, 2)                 66        \n","                                                                 \n","=================================================================\n","Total params: 34,962\n","Trainable params: 34,946\n","Non-trainable params: 16\n","_________________________________________________________________\n","Epoch 1/2\n","164/164 [==============================] - 179s 1s/step - loss: 0.5859 - accuracy: 0.7410\n","Epoch 2/2\n","164/164 [==============================] - 174s 1s/step - loss: 0.5719 - accuracy: 0.7422\n","Found 624 images belonging to 2 classes.\n","20/20 [==============================] - 20s 967ms/step - loss: 0.6962 - accuracy: 0.6250\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.6962315440177917, 0.625]"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","#from imblearn.over_sampling import RandomOverSampler\n","import numpy as np\n","from sklearn.utils import class_weight\n","\n","data = keras.preprocessing.image.ImageDataGenerator(\n","preprocessing_function=lambda x:x/255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/train\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","\n","# Calculate class weights\n","#class_weights = np.array([1.3333, 0.8])\n","\n","model = keras.Sequential([\n","    keras.layers.Input((1600, 1200, 1)),\n","    keras.layers.Conv2D(8, (5, 5), strides=(3, 3), activation='relu'),\n","    keras.layers.BatchNormalization(),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Conv2D(16, (7, 7), strides=(3, 3), activation='relu'),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Conv2D(32, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Flatten(),\n","    keras.layers.Dense(32, activation=\"relu\"),\n","    keras.layers.Dense(2, activation=\"softmax\"),\n","])\n","\n","model.compile(\n","    optimizer=keras.optimizers.SGD(learning_rate=0.01, nesterov=True, momentum=0.92),\n","    loss=tf.keras.losses.CategoricalCrossentropy(),\n","    metrics=[\"accuracy\"]\n",")\n","\n","model.summary()\n","\n","history = model.fit(data, verbose=1, epochs=4)\n","\n","data_test = keras.preprocessing.image.ImageDataGenerator(\n","    preprocessing_function=lambda x: x / 255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/test\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","model.evaluate(data_test)\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NoU54d5C9oFq","executionInfo":{"status":"ok","timestamp":1686043662812,"user_tz":-180,"elapsed":780339,"user":{"displayName":"YAKUP CAN KARACAOGLU","userId":"14801541947105482372"}},"outputId":"59d193c7-525c-4754-9dd4-58b3eba4c28a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 5232 images belonging to 2 classes.\n","Model: \"sequential_10\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_52 (Conv2D)          (None, 532, 399, 8)       208       \n","                                                                 \n"," batch_normalization_10 (Bat  (None, 532, 399, 8)      32        \n"," chNormalization)                                                \n","                                                                 \n"," conv2d_53 (Conv2D)          (None, 264, 198, 8)       1608      \n","                                                                 \n"," dropout_16 (Dropout)        (None, 264, 198, 8)       0         \n","                                                                 \n"," conv2d_54 (Conv2D)          (None, 130, 97, 8)        1608      \n","                                                                 \n"," conv2d_55 (Conv2D)          (None, 42, 31, 16)        6288      \n","                                                                 \n"," max_pooling2d_20 (MaxPoolin  (None, 21, 15, 16)       0         \n"," g2D)                                                            \n","                                                                 \n"," conv2d_56 (Conv2D)          (None, 9, 6, 32)          12832     \n","                                                                 \n"," dropout_17 (Dropout)        (None, 9, 6, 32)          0         \n","                                                                 \n"," max_pooling2d_21 (MaxPoolin  (None, 4, 3, 32)         0         \n"," g2D)                                                            \n","                                                                 \n"," flatten_10 (Flatten)        (None, 384)               0         \n","                                                                 \n"," dense_20 (Dense)            (None, 32)                12320     \n","                                                                 \n"," dense_21 (Dense)            (None, 2)                 66        \n","                                                                 \n","=================================================================\n","Total params: 34,962\n","Trainable params: 34,946\n","Non-trainable params: 16\n","_________________________________________________________________\n","Epoch 1/4\n","164/164 [==============================] - 181s 1s/step - loss: 0.2925 - accuracy: 0.8838\n","Epoch 2/4\n","164/164 [==============================] - 176s 1s/step - loss: 0.1648 - accuracy: 0.9415\n","Epoch 3/4\n","164/164 [==============================] - 178s 1s/step - loss: 0.1303 - accuracy: 0.9515\n","Epoch 4/4\n","164/164 [==============================] - 179s 1s/step - loss: 0.1154 - accuracy: 0.9589\n","Found 624 images belonging to 2 classes.\n","20/20 [==============================] - 21s 1s/step - loss: 0.8870 - accuracy: 0.7196\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.886972188949585, 0.7195512652397156]"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","#from imblearn.over_sampling import RandomOverSampler\n","import numpy as np\n","from sklearn.utils import class_weight\n","\n","data = keras.preprocessing.image.ImageDataGenerator(\n","preprocessing_function=lambda x:x/255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/train\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","\n","# Calculate class weights\n","#class_weights = np.array([1.3333, 0.8])\n","\n","model = keras.Sequential([\n","    keras.layers.Input((1600, 1200, 1)),\n","    keras.layers.Conv2D(8, (5, 5), strides=(3, 3), activation='relu'),\n","    keras.layers.BatchNormalization(),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Conv2D(16, (7, 7), strides=(3, 3), activation='relu'),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Conv2D(32, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Flatten(),\n","    keras.layers.Dense(32, activation=\"relu\"),\n","    keras.layers.Dense(2, activation=\"softmax\"),\n","])\n","\n","model.compile(\n","    optimizer=keras.optimizers.SGD(learning_rate=0.01, nesterov=True, momentum=0.92),\n","    loss=tf.keras.losses.CategoricalCrossentropy(),\n","    metrics=[\"accuracy\"]\n",")\n","\n","model.summary()\n","\n","history = model.fit(data, verbose=1, epochs=10)\n","\n","data_test = keras.preprocessing.image.ImageDataGenerator(\n","    preprocessing_function=lambda x: x / 255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/test\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","model.evaluate(data_test)\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m2BYUgba9wjE","executionInfo":{"status":"ok","timestamp":1686047880926,"user_tz":-180,"elapsed":1848994,"user":{"displayName":"YAKUP CAN KARACAOGLU","userId":"14801541947105482372"}},"outputId":"1df1c658-415d-4900-9618-9cab39fddf2f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 5232 images belonging to 2 classes.\n","Model: \"sequential_11\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_57 (Conv2D)          (None, 532, 399, 8)       208       \n","                                                                 \n"," batch_normalization_11 (Bat  (None, 532, 399, 8)      32        \n"," chNormalization)                                                \n","                                                                 \n"," conv2d_58 (Conv2D)          (None, 264, 198, 8)       1608      \n","                                                                 \n"," dropout_18 (Dropout)        (None, 264, 198, 8)       0         \n","                                                                 \n"," conv2d_59 (Conv2D)          (None, 130, 97, 8)        1608      \n","                                                                 \n"," conv2d_60 (Conv2D)          (None, 42, 31, 16)        6288      \n","                                                                 \n"," max_pooling2d_22 (MaxPoolin  (None, 21, 15, 16)       0         \n"," g2D)                                                            \n","                                                                 \n"," conv2d_61 (Conv2D)          (None, 9, 6, 32)          12832     \n","                                                                 \n"," dropout_19 (Dropout)        (None, 9, 6, 32)          0         \n","                                                                 \n"," max_pooling2d_23 (MaxPoolin  (None, 4, 3, 32)         0         \n"," g2D)                                                            \n","                                                                 \n"," flatten_11 (Flatten)        (None, 384)               0         \n","                                                                 \n"," dense_22 (Dense)            (None, 32)                12320     \n","                                                                 \n"," dense_23 (Dense)            (None, 2)                 66        \n","                                                                 \n","=================================================================\n","Total params: 34,962\n","Trainable params: 34,946\n","Non-trainable params: 16\n","_________________________________________________________________\n","Epoch 1/10\n","164/164 [==============================] - 179s 1s/step - loss: 0.2694 - accuracy: 0.8851\n","Epoch 2/10\n","164/164 [==============================] - 175s 1s/step - loss: 0.1425 - accuracy: 0.9434\n","Epoch 3/10\n","164/164 [==============================] - 174s 1s/step - loss: 0.1238 - accuracy: 0.9536\n","Epoch 4/10\n","164/164 [==============================] - 174s 1s/step - loss: 0.1017 - accuracy: 0.9608\n","Epoch 5/10\n","164/164 [==============================] - 173s 1s/step - loss: 0.0849 - accuracy: 0.9696\n","Epoch 6/10\n","164/164 [==============================] - 174s 1s/step - loss: 0.0761 - accuracy: 0.9723\n","Epoch 7/10\n","164/164 [==============================] - 173s 1s/step - loss: 0.0709 - accuracy: 0.9721\n","Epoch 8/10\n","164/164 [==============================] - 172s 1s/step - loss: 0.0571 - accuracy: 0.9799\n","Epoch 9/10\n","164/164 [==============================] - 175s 1s/step - loss: 0.0536 - accuracy: 0.9820\n","Epoch 10/10\n","164/164 [==============================] - 175s 1s/step - loss: 0.0447 - accuracy: 0.9841\n","Found 624 images belonging to 2 classes.\n","20/20 [==============================] - 19s 923ms/step - loss: 1.2393 - accuracy: 0.7436\n"]},{"output_type":"execute_result","data":{"text/plain":["[1.23930025100708, 0.7435897588729858]"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","#from imblearn.over_sampling import RandomOverSampler\n","import numpy as np\n","from sklearn.utils import class_weight\n","\n","data = keras.preprocessing.image.ImageDataGenerator(\n","preprocessing_function=lambda x:x/255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/train\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","\n","# Calculate class weights\n","#class_weights = np.array([1.3333, 0.8])\n","\n","model = keras.Sequential([\n","    keras.layers.Input((1600, 1200, 1)),\n","    keras.layers.Conv2D(8, (5, 5), strides=(3, 3), activation='relu'),\n","    keras.layers.BatchNormalization(),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Conv2D(16, (7, 7), strides=(3, 3), activation='relu'),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Conv2D(32, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Flatten(),\n","    keras.layers.Dense(32, activation=\"relu\"),\n","    keras.layers.Dense(2, activation=\"softmax\"),\n","])\n","\n","model.compile(\n","    optimizer=keras.optimizers.SGD(learning_rate=0.01, nesterov=True, momentum=0.92),\n","    loss=tf.keras.losses.CategoricalCrossentropy(),\n","    metrics=[\"accuracy\"]\n",")\n","\n","model.summary()\n","\n","history = model.fit(data, verbose=1, epochs=6)\n","\n","data_test = keras.preprocessing.image.ImageDataGenerator(\n","    preprocessing_function=lambda x: x / 255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/test\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","model.evaluate(data_test)\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uCA3AwPGLL30","executionInfo":{"status":"ok","timestamp":1686049121125,"user_tz":-180,"elapsed":1161223,"user":{"displayName":"YAKUP CAN KARACAOGLU","userId":"14801541947105482372"}},"outputId":"9cc79b4d-3d1d-4d37-ffaa-7576d5bd03b1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 5232 images belonging to 2 classes.\n","Model: \"sequential_12\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_62 (Conv2D)          (None, 532, 399, 8)       208       \n","                                                                 \n"," batch_normalization_12 (Bat  (None, 532, 399, 8)      32        \n"," chNormalization)                                                \n","                                                                 \n"," conv2d_63 (Conv2D)          (None, 264, 198, 8)       1608      \n","                                                                 \n"," dropout_20 (Dropout)        (None, 264, 198, 8)       0         \n","                                                                 \n"," conv2d_64 (Conv2D)          (None, 130, 97, 8)        1608      \n","                                                                 \n"," conv2d_65 (Conv2D)          (None, 42, 31, 16)        6288      \n","                                                                 \n"," max_pooling2d_24 (MaxPoolin  (None, 21, 15, 16)       0         \n"," g2D)                                                            \n","                                                                 \n"," conv2d_66 (Conv2D)          (None, 9, 6, 32)          12832     \n","                                                                 \n"," dropout_21 (Dropout)        (None, 9, 6, 32)          0         \n","                                                                 \n"," max_pooling2d_25 (MaxPoolin  (None, 4, 3, 32)         0         \n"," g2D)                                                            \n","                                                                 \n"," flatten_12 (Flatten)        (None, 384)               0         \n","                                                                 \n"," dense_24 (Dense)            (None, 32)                12320     \n","                                                                 \n"," dense_25 (Dense)            (None, 2)                 66        \n","                                                                 \n","=================================================================\n","Total params: 34,962\n","Trainable params: 34,946\n","Non-trainable params: 16\n","_________________________________________________________________\n","Epoch 1/6\n","164/164 [==============================] - 185s 1s/step - loss: 0.3015 - accuracy: 0.8750\n","Epoch 2/6\n","164/164 [==============================] - 176s 1s/step - loss: 0.1398 - accuracy: 0.9482\n","Epoch 3/6\n","164/164 [==============================] - 175s 1s/step - loss: 0.1114 - accuracy: 0.9583\n","Epoch 4/6\n","164/164 [==============================] - 173s 1s/step - loss: 0.0936 - accuracy: 0.9673\n","Epoch 5/6\n","164/164 [==============================] - 174s 1s/step - loss: 0.0884 - accuracy: 0.9688\n","Epoch 6/6\n","164/164 [==============================] - 173s 1s/step - loss: 0.0752 - accuracy: 0.9738\n","Found 624 images belonging to 2 classes.\n","20/20 [==============================] - 18s 911ms/step - loss: 1.0222 - accuracy: 0.7212\n"]},{"output_type":"execute_result","data":{"text/plain":["[1.0221941471099854, 0.7211538553237915]"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","#from imblearn.over_sampling import RandomOverSampler\n","import numpy as np\n","from sklearn.utils import class_weight\n","\n","data = keras.preprocessing.image.ImageDataGenerator(\n","preprocessing_function=lambda x:x/255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/train\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","\n","# Calculate class weights\n","class_weights = np.array([1, 1])\n","\n","model = keras.Sequential([\n","    keras.layers.Input((1600, 1200, 1)),\n","    keras.layers.Conv2D(8, (5, 5), strides=(3, 3), activation='relu'),\n","    keras.layers.BatchNormalization(),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Conv2D(16, (7, 7), strides=(3, 3), activation='relu'),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Conv2D(32, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Flatten(),\n","    keras.layers.Dense(32, activation=\"relu\"),\n","    keras.layers.Dense(2, activation=\"softmax\"),\n","])\n","\n","model.compile(\n","    optimizer=keras.optimizers.SGD(learning_rate=0.01, nesterov=True, momentum=0.92),\n","    loss=tf.keras.losses.CategoricalCrossentropy(),\n","    metrics=[\"accuracy\"]\n",")\n","\n","model.summary()\n","\n","history = model.fit(data, verbose=1, epochs=6, class_weight=dict(enumerate(class_weights)))\n","\n","data_test = keras.preprocessing.image.ImageDataGenerator(\n","    preprocessing_function=lambda x: x / 255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/test\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","model.evaluate(data_test)\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_CP4cl-BSijv","executionInfo":{"status":"ok","timestamp":1686050232434,"user_tz":-180,"elapsed":1111318,"user":{"displayName":"YAKUP CAN KARACAOGLU","userId":"14801541947105482372"}},"outputId":"14a0a6c3-010d-4364-91da-ad3be61ce896"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 5232 images belonging to 2 classes.\n","Model: \"sequential_13\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_67 (Conv2D)          (None, 532, 399, 8)       208       \n","                                                                 \n"," batch_normalization_13 (Bat  (None, 532, 399, 8)      32        \n"," chNormalization)                                                \n","                                                                 \n"," conv2d_68 (Conv2D)          (None, 264, 198, 8)       1608      \n","                                                                 \n"," dropout_22 (Dropout)        (None, 264, 198, 8)       0         \n","                                                                 \n"," conv2d_69 (Conv2D)          (None, 130, 97, 8)        1608      \n","                                                                 \n"," conv2d_70 (Conv2D)          (None, 42, 31, 16)        6288      \n","                                                                 \n"," max_pooling2d_26 (MaxPoolin  (None, 21, 15, 16)       0         \n"," g2D)                                                            \n","                                                                 \n"," conv2d_71 (Conv2D)          (None, 9, 6, 32)          12832     \n","                                                                 \n"," dropout_23 (Dropout)        (None, 9, 6, 32)          0         \n","                                                                 \n"," max_pooling2d_27 (MaxPoolin  (None, 4, 3, 32)         0         \n"," g2D)                                                            \n","                                                                 \n"," flatten_13 (Flatten)        (None, 384)               0         \n","                                                                 \n"," dense_26 (Dense)            (None, 32)                12320     \n","                                                                 \n"," dense_27 (Dense)            (None, 2)                 66        \n","                                                                 \n","=================================================================\n","Total params: 34,962\n","Trainable params: 34,946\n","Non-trainable params: 16\n","_________________________________________________________________\n","Epoch 1/6\n","164/164 [==============================] - 175s 1s/step - loss: 0.3001 - accuracy: 0.8821\n","Epoch 2/6\n","164/164 [==============================] - 170s 1s/step - loss: 0.1799 - accuracy: 0.9373\n","Epoch 3/6\n","164/164 [==============================] - 171s 1s/step - loss: 0.1483 - accuracy: 0.9430\n","Epoch 4/6\n","164/164 [==============================] - 170s 1s/step - loss: 0.1281 - accuracy: 0.9516\n","Epoch 5/6\n","164/164 [==============================] - 169s 1s/step - loss: 0.1094 - accuracy: 0.9581\n","Epoch 6/6\n","164/164 [==============================] - 172s 1s/step - loss: 0.1112 - accuracy: 0.9576\n","Found 624 images belonging to 2 classes.\n","20/20 [==============================] - 19s 948ms/step - loss: 0.8036 - accuracy: 0.7708\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.8035848736763, 0.7708333134651184]"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","#from imblearn.over_sampling import RandomOverSampler\n","import numpy as np\n","from sklearn.utils import class_weight\n","\n","data = keras.preprocessing.image.ImageDataGenerator(\n","preprocessing_function=lambda x:x/255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/train\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","\n","# Calculate class weights\n","#class_weights = np.array([1.3333, 0.8])\n","\n","model = keras.Sequential([\n","    keras.layers.Input((1600, 1200, 1)),\n","    keras.layers.Conv2D(8, (5, 5), strides=(3, 3), activation='relu'),\n","    keras.layers.BatchNormalization(),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Conv2D(16, (7, 7), strides=(3, 3), activation='relu'),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Conv2D(32, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Flatten(),\n","    keras.layers.Dense(32, activation=\"relu\"),\n","    keras.layers.Dense(2, activation=\"softmax\"),\n","])\n","\n","model.compile(\n","    optimizer=keras.optimizers.SGD(learning_rate=0.01, nesterov=True, momentum=0.92),\n","    loss=tf.keras.losses.CategoricalCrossentropy(),\n","    metrics=[\"accuracy\"]\n",")\n","\n","model.summary()\n","\n","history = model.fit(data, verbose=1, epochs=3)\n","\n","data_test = keras.preprocessing.image.ImageDataGenerator(\n","    preprocessing_function=lambda x: x / 255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/test\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","model.evaluate(data_test)\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AWqTsaybSxPC","executionInfo":{"status":"ok","timestamp":1686050798933,"user_tz":-180,"elapsed":566525,"user":{"displayName":"YAKUP CAN KARACAOGLU","userId":"14801541947105482372"}},"outputId":"2bba6b77-6bda-4505-d120-e18126f74958"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 5232 images belonging to 2 classes.\n","Model: \"sequential_14\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_72 (Conv2D)          (None, 532, 399, 8)       208       \n","                                                                 \n"," batch_normalization_14 (Bat  (None, 532, 399, 8)      32        \n"," chNormalization)                                                \n","                                                                 \n"," conv2d_73 (Conv2D)          (None, 264, 198, 8)       1608      \n","                                                                 \n"," dropout_24 (Dropout)        (None, 264, 198, 8)       0         \n","                                                                 \n"," conv2d_74 (Conv2D)          (None, 130, 97, 8)        1608      \n","                                                                 \n"," conv2d_75 (Conv2D)          (None, 42, 31, 16)        6288      \n","                                                                 \n"," max_pooling2d_28 (MaxPoolin  (None, 21, 15, 16)       0         \n"," g2D)                                                            \n","                                                                 \n"," conv2d_76 (Conv2D)          (None, 9, 6, 32)          12832     \n","                                                                 \n"," dropout_25 (Dropout)        (None, 9, 6, 32)          0         \n","                                                                 \n"," max_pooling2d_29 (MaxPoolin  (None, 4, 3, 32)         0         \n"," g2D)                                                            \n","                                                                 \n"," flatten_14 (Flatten)        (None, 384)               0         \n","                                                                 \n"," dense_28 (Dense)            (None, 32)                12320     \n","                                                                 \n"," dense_29 (Dense)            (None, 2)                 66        \n","                                                                 \n","=================================================================\n","Total params: 34,962\n","Trainable params: 34,946\n","Non-trainable params: 16\n","_________________________________________________________________\n","Epoch 1/3\n","164/164 [==============================] - 174s 1s/step - loss: 0.3729 - accuracy: 0.8416\n","Epoch 2/3\n","164/164 [==============================] - 170s 1s/step - loss: 0.1761 - accuracy: 0.9312\n","Epoch 3/3\n","164/164 [==============================] - 171s 1s/step - loss: 0.1368 - accuracy: 0.9478\n","Found 624 images belonging to 2 classes.\n","20/20 [==============================] - 18s 909ms/step - loss: 0.4971 - accuracy: 0.7772\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.4970928132534027, 0.7772436141967773]"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","#from imblearn.over_sampling import RandomOverSampler\n","import numpy as np\n","from sklearn.utils import class_weight\n","\n","data = keras.preprocessing.image.ImageDataGenerator(\n","preprocessing_function=lambda x:x/255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/train\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","\n","# Calculate class weights\n","class_weights = np.array([1, 1])\n","\n","model = keras.Sequential([\n","    keras.layers.Input((1600, 1200, 1)),\n","    keras.layers.Conv2D(8, (5, 5), strides=(3, 3), activation='relu'),\n","    keras.layers.BatchNormalization(),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Conv2D(16, (7, 7), strides=(3, 3), activation='relu'),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Conv2D(32, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Flatten(),\n","    keras.layers.Dense(32, activation=\"relu\"),\n","    keras.layers.Dense(2, activation=\"softmax\"),\n","])\n","\n","model.compile(\n","    optimizer=keras.optimizers.SGD(learning_rate=0.01, nesterov=True, momentum=0.92),\n","    loss=tf.keras.losses.CategoricalCrossentropy(),\n","    metrics=[\"accuracy\"]\n",")\n","\n","model.summary()\n","\n","history = model.fit(data, verbose=1, epochs=3, class_weight=dict(enumerate(class_weights)))\n","\n","data_test = keras.preprocessing.image.ImageDataGenerator(\n","    preprocessing_function=lambda x: x / 255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/test\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","model.evaluate(data_test)\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sbvC7JN7T9jc","executionInfo":{"status":"ok","timestamp":1686051724984,"user_tz":-180,"elapsed":598699,"user":{"displayName":"YAKUP CAN KARACAOGLU","userId":"14801541947105482372"}},"outputId":"8f58f299-302a-4193-8dcf-31669b0d37db"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 5232 images belonging to 2 classes.\n","Model: \"sequential_15\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_77 (Conv2D)          (None, 532, 399, 8)       208       \n","                                                                 \n"," batch_normalization_15 (Bat  (None, 532, 399, 8)      32        \n"," chNormalization)                                                \n","                                                                 \n"," conv2d_78 (Conv2D)          (None, 264, 198, 8)       1608      \n","                                                                 \n"," dropout_26 (Dropout)        (None, 264, 198, 8)       0         \n","                                                                 \n"," conv2d_79 (Conv2D)          (None, 130, 97, 8)        1608      \n","                                                                 \n"," conv2d_80 (Conv2D)          (None, 42, 31, 16)        6288      \n","                                                                 \n"," max_pooling2d_30 (MaxPoolin  (None, 21, 15, 16)       0         \n"," g2D)                                                            \n","                                                                 \n"," conv2d_81 (Conv2D)          (None, 9, 6, 32)          12832     \n","                                                                 \n"," dropout_27 (Dropout)        (None, 9, 6, 32)          0         \n","                                                                 \n"," max_pooling2d_31 (MaxPoolin  (None, 4, 3, 32)         0         \n"," g2D)                                                            \n","                                                                 \n"," flatten_15 (Flatten)        (None, 384)               0         \n","                                                                 \n"," dense_30 (Dense)            (None, 32)                12320     \n","                                                                 \n"," dense_31 (Dense)            (None, 2)                 66        \n","                                                                 \n","=================================================================\n","Total params: 34,962\n","Trainable params: 34,946\n","Non-trainable params: 16\n","_________________________________________________________________\n","Epoch 1/3\n","164/164 [==============================] - 172s 1s/step - loss: 0.2373 - accuracy: 0.9033\n","Epoch 2/3\n","164/164 [==============================] - 169s 1s/step - loss: 0.1413 - accuracy: 0.9457\n","Epoch 3/3\n","164/164 [==============================] - 167s 1s/step - loss: 0.1038 - accuracy: 0.9623\n","Found 624 images belonging to 2 classes.\n","20/20 [==============================] - 18s 890ms/step - loss: 1.1118 - accuracy: 0.7276\n"]},{"output_type":"execute_result","data":{"text/plain":["[1.1117804050445557, 0.7275640964508057]"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","#from imblearn.over_sampling import RandomOverSampler\n","import numpy as np\n","from sklearn.utils import class_weight\n","\n","data = keras.preprocessing.image.ImageDataGenerator(\n","preprocessing_function=lambda x:x/255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/train\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","\n","# Calculate class weights\n","class_weights = np.array([1, 1])\n","\n","model = keras.Sequential([\n","    keras.layers.Input((1600, 1200, 1)),\n","    keras.layers.Conv2D(8, (5, 5), strides=(3, 3), activation='relu'),\n","    keras.layers.BatchNormalization(),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Conv2D(16, (7, 7), strides=(3, 3), activation='relu'),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Conv2D(32, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Flatten(),\n","    keras.layers.Dense(32, activation=\"relu\"),\n","    keras.layers.Dense(2, activation=\"softmax\"),\n","])\n","\n","model.compile(\n","    optimizer=keras.optimizers.SGD(learning_rate=0.01, nesterov=True, momentum=0.92),\n","    loss=tf.keras.losses.CategoricalCrossentropy(),\n","    metrics=[\"accuracy\"]\n",")\n","\n","model.summary()\n","\n","history = model.fit(data, verbose=1, epochs=1, class_weight=dict(enumerate(class_weights)))\n","\n","data_test = keras.preprocessing.image.ImageDataGenerator(\n","    preprocessing_function=lambda x: x / 255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/test\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","model.evaluate(data_test)\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_xZgqmnaenh0","executionInfo":{"status":"ok","timestamp":1686052246859,"user_tz":-180,"elapsed":198120,"user":{"displayName":"YAKUP CAN KARACAOGLU","userId":"14801541947105482372"}},"outputId":"5eddcd9f-5841-4c17-de87-ffaa298ec58d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 5232 images belonging to 2 classes.\n","Model: \"sequential_16\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_82 (Conv2D)          (None, 532, 399, 8)       208       \n","                                                                 \n"," batch_normalization_16 (Bat  (None, 532, 399, 8)      32        \n"," chNormalization)                                                \n","                                                                 \n"," conv2d_83 (Conv2D)          (None, 264, 198, 8)       1608      \n","                                                                 \n"," dropout_28 (Dropout)        (None, 264, 198, 8)       0         \n","                                                                 \n"," conv2d_84 (Conv2D)          (None, 130, 97, 8)        1608      \n","                                                                 \n"," conv2d_85 (Conv2D)          (None, 42, 31, 16)        6288      \n","                                                                 \n"," max_pooling2d_32 (MaxPoolin  (None, 21, 15, 16)       0         \n"," g2D)                                                            \n","                                                                 \n"," conv2d_86 (Conv2D)          (None, 9, 6, 32)          12832     \n","                                                                 \n"," dropout_29 (Dropout)        (None, 9, 6, 32)          0         \n","                                                                 \n"," max_pooling2d_33 (MaxPoolin  (None, 4, 3, 32)         0         \n"," g2D)                                                            \n","                                                                 \n"," flatten_16 (Flatten)        (None, 384)               0         \n","                                                                 \n"," dense_32 (Dense)            (None, 32)                12320     \n","                                                                 \n"," dense_33 (Dense)            (None, 2)                 66        \n","                                                                 \n","=================================================================\n","Total params: 34,962\n","Trainable params: 34,946\n","Non-trainable params: 16\n","_________________________________________________________________\n","164/164 [==============================] - 174s 1s/step - loss: 0.6297 - accuracy: 0.7460\n","Found 624 images belonging to 2 classes.\n","20/20 [==============================] - 20s 1s/step - loss: 0.6861 - accuracy: 0.6250\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.6860598921775818, 0.625]"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","#from imblearn.over_sampling import RandomOverSampler\n","import numpy as np\n","from sklearn.utils import class_weight\n","\n","data = keras.preprocessing.image.ImageDataGenerator(\n","preprocessing_function=lambda x:x/255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/train\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","\n","# Calculate class weights\n","class_weights = np.array([1, 1])\n","\n","model = keras.Sequential([\n","    keras.layers.Input((1600, 1200, 1)),\n","    keras.layers.Conv2D(8, (5, 5), strides=(3, 3), activation='relu'),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.BatchNormalization(),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Conv2D(16, (7, 7), strides=(3, 3), activation='relu'),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Conv2D(32, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Flatten(),\n","    keras.layers.Dense(32, activation=\"relu\"),\n","    keras.layers.Dense(2, activation=\"softmax\"),\n","])\n","\n","model.compile(\n","    optimizer=keras.optimizers.SGD(learning_rate=0.01, nesterov=True, momentum=0.92),\n","    loss=tf.keras.losses.CategoricalCrossentropy(),\n","    metrics=[\"accuracy\"]\n",")\n","\n","model.summary()\n","\n","history = model.fit(data, verbose=1, epochs=3, class_weight=dict(enumerate(class_weights)))\n","\n","data_test = keras.preprocessing.image.ImageDataGenerator(\n","    preprocessing_function=lambda x: x / 255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/test\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","model.evaluate(data_test)\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"caQFae5biIry","executionInfo":{"status":"ok","timestamp":1686052789920,"user_tz":-180,"elapsed":543085,"user":{"displayName":"YAKUP CAN KARACAOGLU","userId":"14801541947105482372"}},"outputId":"86aea10d-924b-44a1-e3ba-0b7afcbdaf0b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 5232 images belonging to 2 classes.\n","Model: \"sequential_17\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_87 (Conv2D)          (None, 532, 399, 8)       208       \n","                                                                 \n"," dropout_30 (Dropout)        (None, 532, 399, 8)       0         \n","                                                                 \n"," batch_normalization_17 (Bat  (None, 532, 399, 8)      32        \n"," chNormalization)                                                \n","                                                                 \n"," conv2d_88 (Conv2D)          (None, 264, 198, 8)       1608      \n","                                                                 \n"," dropout_31 (Dropout)        (None, 264, 198, 8)       0         \n","                                                                 \n"," conv2d_89 (Conv2D)          (None, 130, 97, 8)        1608      \n","                                                                 \n"," conv2d_90 (Conv2D)          (None, 42, 31, 16)        6288      \n","                                                                 \n"," dropout_32 (Dropout)        (None, 42, 31, 16)        0         \n","                                                                 \n"," max_pooling2d_34 (MaxPoolin  (None, 21, 15, 16)       0         \n"," g2D)                                                            \n","                                                                 \n"," conv2d_91 (Conv2D)          (None, 9, 6, 32)          12832     \n","                                                                 \n"," dropout_33 (Dropout)        (None, 9, 6, 32)          0         \n","                                                                 \n"," max_pooling2d_35 (MaxPoolin  (None, 4, 3, 32)         0         \n"," g2D)                                                            \n","                                                                 \n"," flatten_17 (Flatten)        (None, 384)               0         \n","                                                                 \n"," dense_34 (Dense)            (None, 32)                12320     \n","                                                                 \n"," dense_35 (Dense)            (None, 2)                 66        \n","                                                                 \n","=================================================================\n","Total params: 34,962\n","Trainable params: 34,946\n","Non-trainable params: 16\n","_________________________________________________________________\n","Epoch 1/3\n","164/164 [==============================] - 174s 1s/step - loss: 0.4850 - accuracy: 0.7970\n","Epoch 2/3\n","164/164 [==============================] - 170s 1s/step - loss: 0.1585 - accuracy: 0.9425\n","Epoch 3/3\n","164/164 [==============================] - 176s 1s/step - loss: 0.1287 - accuracy: 0.9547\n","Found 624 images belonging to 2 classes.\n","20/20 [==============================] - 19s 946ms/step - loss: 0.5851 - accuracy: 0.7708\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.5851203203201294, 0.7708333134651184]"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","#from imblearn.over_sampling import RandomOverSampler\n","import numpy as np\n","from sklearn.utils import class_weight\n","\n","data = keras.preprocessing.image.ImageDataGenerator(\n","preprocessing_function=lambda x:x/255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/train\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","\n","# Calculate class weights\n","class_weights = np.array([1, 1])\n","\n","model = keras.Sequential([\n","    keras.layers.Input((1600, 1200, 1)),\n","    keras.layers.Conv2D(8, (5, 5), strides=(3, 3), activation='relu'),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.BatchNormalization(),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Conv2D(16, (7, 7), strides=(3, 3), activation='relu'),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Conv2D(32, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Flatten(),\n","    keras.layers.Dense(32, activation=\"relu\"),\n","    keras.layers.Dense(2, activation=\"softmax\"),\n","])\n","\n","model.compile(\n","    optimizer=keras.optimizers.SGD(learning_rate=0.01, nesterov=True, momentum=0.90),\n","    loss=tf.keras.losses.CategoricalCrossentropy(),\n","    metrics=[\"accuracy\"]\n",")\n","\n","model.summary()\n","\n","history = model.fit(data, verbose=1, epochs=3, class_weight=dict(enumerate(class_weights)))\n","\n","data_test = keras.preprocessing.image.ImageDataGenerator(\n","    preprocessing_function=lambda x: x / 255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/test\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","model.evaluate(data_test)\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vrcQncZQiNFh","executionInfo":{"status":"ok","timestamp":1686053464154,"user_tz":-180,"elapsed":629755,"user":{"displayName":"YAKUP CAN KARACAOGLU","userId":"14801541947105482372"}},"outputId":"8a998c7d-c8fc-41ff-f85c-365185246f31"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 5232 images belonging to 2 classes.\n","Model: \"sequential_18\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_92 (Conv2D)          (None, 532, 399, 8)       208       \n","                                                                 \n"," dropout_34 (Dropout)        (None, 532, 399, 8)       0         \n","                                                                 \n"," batch_normalization_18 (Bat  (None, 532, 399, 8)      32        \n"," chNormalization)                                                \n","                                                                 \n"," conv2d_93 (Conv2D)          (None, 264, 198, 8)       1608      \n","                                                                 \n"," dropout_35 (Dropout)        (None, 264, 198, 8)       0         \n","                                                                 \n"," conv2d_94 (Conv2D)          (None, 130, 97, 8)        1608      \n","                                                                 \n"," conv2d_95 (Conv2D)          (None, 42, 31, 16)        6288      \n","                                                                 \n"," dropout_36 (Dropout)        (None, 42, 31, 16)        0         \n","                                                                 \n"," max_pooling2d_36 (MaxPoolin  (None, 21, 15, 16)       0         \n"," g2D)                                                            \n","                                                                 \n"," conv2d_96 (Conv2D)          (None, 9, 6, 32)          12832     \n","                                                                 \n"," dropout_37 (Dropout)        (None, 9, 6, 32)          0         \n","                                                                 \n"," max_pooling2d_37 (MaxPoolin  (None, 4, 3, 32)         0         \n"," g2D)                                                            \n","                                                                 \n"," flatten_18 (Flatten)        (None, 384)               0         \n","                                                                 \n"," dense_36 (Dense)            (None, 32)                12320     \n","                                                                 \n"," dense_37 (Dense)            (None, 2)                 66        \n","                                                                 \n","=================================================================\n","Total params: 34,962\n","Trainable params: 34,946\n","Non-trainable params: 16\n","_________________________________________________________________\n","Epoch 1/3\n","164/164 [==============================] - 181s 1s/step - loss: 0.3335 - accuracy: 0.8612\n","Epoch 2/3\n","164/164 [==============================] - 180s 1s/step - loss: 0.1504 - accuracy: 0.9442\n","Epoch 3/3\n","164/164 [==============================] - 173s 1s/step - loss: 0.1125 - accuracy: 0.9578\n","Found 624 images belonging to 2 classes.\n","20/20 [==============================] - 19s 959ms/step - loss: 0.5223 - accuracy: 0.8109\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.522262454032898, 0.8108974099159241]"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","#from imblearn.over_sampling import RandomOverSampler\n","import numpy as np\n","from sklearn.utils import class_weight\n","\n","data = keras.preprocessing.image.ImageDataGenerator(\n","preprocessing_function=lambda x:x/255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/train\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","\n","# Calculate class weights\n","class_weights = np.array([1, 1])\n","\n","model = keras.Sequential([\n","    keras.layers.Input((1600, 1200, 1)),\n","    keras.layers.Conv2D(8, (5, 5), strides=(3, 3), activation='relu'),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.BatchNormalization(),\n","    #keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    #keras.layers.Dropout(0.1),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Conv2D(16, (7, 7), strides=(3, 3), activation='relu'),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Conv2D(32, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Flatten(),\n","    keras.layers.Dense(32, activation=\"relu\"),\n","    keras.layers.Dense(2, activation=\"softmax\"),\n","])\n","\n","model.compile(\n","    optimizer=keras.optimizers.SGD(learning_rate=0.01, nesterov=True, momentum=0.90),\n","    loss=tf.keras.losses.CategoricalCrossentropy(),\n","    metrics=[\"accuracy\"]\n",")\n","\n","model.summary()\n","\n","history = model.fit(data, verbose=1, epochs=3, class_weight=dict(enumerate(class_weights)))\n","\n","data_test = keras.preprocessing.image.ImageDataGenerator(\n","    preprocessing_function=lambda x: x / 255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/test\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","model.evaluate(data_test)\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-6gwGIzulIol","executionInfo":{"status":"ok","timestamp":1686054151333,"user_tz":-180,"elapsed":605117,"user":{"displayName":"YAKUP CAN KARACAOGLU","userId":"14801541947105482372"}},"outputId":"0fbc5f64-b57b-4e11-f1d9-b172f3bda52a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 5232 images belonging to 2 classes.\n","Model: \"sequential_19\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_97 (Conv2D)          (None, 532, 399, 8)       208       \n","                                                                 \n"," dropout_38 (Dropout)        (None, 532, 399, 8)       0         \n","                                                                 \n"," batch_normalization_19 (Bat  (None, 532, 399, 8)      32        \n"," chNormalization)                                                \n","                                                                 \n"," conv2d_98 (Conv2D)          (None, 264, 198, 8)       1608      \n","                                                                 \n"," conv2d_99 (Conv2D)          (None, 86, 64, 16)        6288      \n","                                                                 \n"," dropout_39 (Dropout)        (None, 86, 64, 16)        0         \n","                                                                 \n"," max_pooling2d_38 (MaxPoolin  (None, 43, 32, 16)       0         \n"," g2D)                                                            \n","                                                                 \n"," conv2d_100 (Conv2D)         (None, 20, 14, 32)        12832     \n","                                                                 \n"," dropout_40 (Dropout)        (None, 20, 14, 32)        0         \n","                                                                 \n"," max_pooling2d_39 (MaxPoolin  (None, 10, 7, 32)        0         \n"," g2D)                                                            \n","                                                                 \n"," flatten_19 (Flatten)        (None, 2240)              0         \n","                                                                 \n"," dense_38 (Dense)            (None, 32)                71712     \n","                                                                 \n"," dense_39 (Dense)            (None, 2)                 66        \n","                                                                 \n","=================================================================\n","Total params: 92,746\n","Trainable params: 92,730\n","Non-trainable params: 16\n","_________________________________________________________________\n","Epoch 1/3\n","164/164 [==============================] - 183s 1s/step - loss: 0.2724 - accuracy: 0.8870\n","Epoch 2/3\n","164/164 [==============================] - 176s 1s/step - loss: 0.1216 - accuracy: 0.9574\n","Epoch 3/3\n","164/164 [==============================] - 179s 1s/step - loss: 0.0979 - accuracy: 0.9667\n","Found 624 images belonging to 2 classes.\n","20/20 [==============================] - 20s 978ms/step - loss: 0.6917 - accuracy: 0.7612\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.6917171478271484, 0.7612179517745972]"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","#from imblearn.over_sampling import RandomOverSampler\n","import numpy as np\n","from sklearn.utils import class_weight\n","\n","data = keras.preprocessing.image.ImageDataGenerator(\n","preprocessing_function=lambda x:x/255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/train\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","\n","# Calculate class weights\n","class_weights = np.array([1, 1])\n","\n","model = keras.Sequential([\n","    keras.layers.Input((1600, 1200, 1)),\n","    keras.layers.Conv2D(8, (5, 5), strides=(3, 3), activation='relu'),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.BatchNormalization(),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Dropout(0.2),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Conv2D(16, (7, 7), strides=(3, 3), activation='relu'),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Conv2D(32, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Dropout(0.2),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Flatten(),\n","    keras.layers.Dense(32, activation=\"relu\"),\n","    keras.layers.Dense(2, activation=\"softmax\"),\n","])\n","\n","model.compile(\n","    optimizer=keras.optimizers.SGD(learning_rate=0.01, nesterov=True, momentum=0.90),\n","    loss=tf.keras.losses.CategoricalCrossentropy(),\n","    metrics=[\"accuracy\"]\n",")\n","\n","model.summary()\n","\n","history = model.fit(data, verbose=1, epochs=3, class_weight=dict(enumerate(class_weights)))\n","\n","data_test = keras.preprocessing.image.ImageDataGenerator(\n","    preprocessing_function=lambda x: x / 255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/test\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","model.evaluate(data_test)\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3G3PsIUdn2aE","executionInfo":{"status":"ok","timestamp":1686054730565,"user_tz":-180,"elapsed":579237,"user":{"displayName":"YAKUP CAN KARACAOGLU","userId":"14801541947105482372"}},"outputId":"d9163136-f447-4e18-afcd-0dbb0b1f4963"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 5232 images belonging to 2 classes.\n","Model: \"sequential_20\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_101 (Conv2D)         (None, 532, 399, 8)       208       \n","                                                                 \n"," dropout_41 (Dropout)        (None, 532, 399, 8)       0         \n","                                                                 \n"," batch_normalization_20 (Bat  (None, 532, 399, 8)      32        \n"," chNormalization)                                                \n","                                                                 \n"," conv2d_102 (Conv2D)         (None, 264, 198, 8)       1608      \n","                                                                 \n"," dropout_42 (Dropout)        (None, 264, 198, 8)       0         \n","                                                                 \n"," conv2d_103 (Conv2D)         (None, 130, 97, 8)        1608      \n","                                                                 \n"," conv2d_104 (Conv2D)         (None, 42, 31, 16)        6288      \n","                                                                 \n"," dropout_43 (Dropout)        (None, 42, 31, 16)        0         \n","                                                                 \n"," max_pooling2d_40 (MaxPoolin  (None, 21, 15, 16)       0         \n"," g2D)                                                            \n","                                                                 \n"," conv2d_105 (Conv2D)         (None, 9, 6, 32)          12832     \n","                                                                 \n"," dropout_44 (Dropout)        (None, 9, 6, 32)          0         \n","                                                                 \n"," max_pooling2d_41 (MaxPoolin  (None, 4, 3, 32)         0         \n"," g2D)                                                            \n","                                                                 \n"," flatten_20 (Flatten)        (None, 384)               0         \n","                                                                 \n"," dense_40 (Dense)            (None, 32)                12320     \n","                                                                 \n"," dense_41 (Dense)            (None, 2)                 66        \n","                                                                 \n","=================================================================\n","Total params: 34,962\n","Trainable params: 34,946\n","Non-trainable params: 16\n","_________________________________________________________________\n","Epoch 1/3\n","164/164 [==============================] - 180s 1s/step - loss: 0.3577 - accuracy: 0.8461\n","Epoch 2/3\n","164/164 [==============================] - 174s 1s/step - loss: 0.1595 - accuracy: 0.9386\n","Epoch 3/3\n","164/164 [==============================] - 172s 1s/step - loss: 0.1398 - accuracy: 0.9509\n","Found 624 images belonging to 2 classes.\n","20/20 [==============================] - 20s 970ms/step - loss: 0.3744 - accuracy: 0.8317\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.37443816661834717, 0.8317307829856873]"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","#from imblearn.over_sampling import RandomOverSampler\n","import numpy as np\n","from sklearn.utils import class_weight\n","\n","data = keras.preprocessing.image.ImageDataGenerator(\n","preprocessing_function=lambda x:x/255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/train\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","\n","# Calculate class weights\n","class_weights = np.array([1, 1])\n","\n","model = keras.Sequential([\n","    keras.layers.Input((1600, 1200, 1)),\n","    keras.layers.Conv2D(8, (5, 5), strides=(3, 3), activation='relu'),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.BatchNormalization(),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Dropout(0.2),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Conv2D(16, (7, 7), strides=(3, 3), activation='relu'),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Conv2D(32, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Dropout(0.2),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Flatten(),\n","    keras.layers.Dense(32, activation=\"relu\"),\n","    keras.layers.Dense(2, activation=\"softmax\"),\n","])\n","\n","model.compile(\n","    optimizer=keras.optimizers.SGD(learning_rate=0.01, nesterov=True, momentum=0.90),\n","    loss=tf.keras.losses.CategoricalCrossentropy(),\n","    metrics=[\"accuracy\"]\n",")\n","\n","model.summary()\n","\n","history = model.fit(data, verbose=1, epochs=3)\n","\n","data_test = keras.preprocessing.image.ImageDataGenerator(\n","    preprocessing_function=lambda x: x / 255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/test\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","model.evaluate(data_test)\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CLPdIdqpqI2U","executionInfo":{"status":"ok","timestamp":1686055348290,"user_tz":-180,"elapsed":571274,"user":{"displayName":"YAKUP CAN KARACAOGLU","userId":"14801541947105482372"}},"outputId":"f0e5f402-6a90-411b-a51a-0cc36bbae687"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 5232 images belonging to 2 classes.\n","Model: \"sequential_21\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_106 (Conv2D)         (None, 532, 399, 8)       208       \n","                                                                 \n"," dropout_45 (Dropout)        (None, 532, 399, 8)       0         \n","                                                                 \n"," batch_normalization_21 (Bat  (None, 532, 399, 8)      32        \n"," chNormalization)                                                \n","                                                                 \n"," conv2d_107 (Conv2D)         (None, 264, 198, 8)       1608      \n","                                                                 \n"," dropout_46 (Dropout)        (None, 264, 198, 8)       0         \n","                                                                 \n"," conv2d_108 (Conv2D)         (None, 130, 97, 8)        1608      \n","                                                                 \n"," conv2d_109 (Conv2D)         (None, 42, 31, 16)        6288      \n","                                                                 \n"," dropout_47 (Dropout)        (None, 42, 31, 16)        0         \n","                                                                 \n"," max_pooling2d_42 (MaxPoolin  (None, 21, 15, 16)       0         \n"," g2D)                                                            \n","                                                                 \n"," conv2d_110 (Conv2D)         (None, 9, 6, 32)          12832     \n","                                                                 \n"," dropout_48 (Dropout)        (None, 9, 6, 32)          0         \n","                                                                 \n"," max_pooling2d_43 (MaxPoolin  (None, 4, 3, 32)         0         \n"," g2D)                                                            \n","                                                                 \n"," flatten_21 (Flatten)        (None, 384)               0         \n","                                                                 \n"," dense_42 (Dense)            (None, 32)                12320     \n","                                                                 \n"," dense_43 (Dense)            (None, 2)                 66        \n","                                                                 \n","=================================================================\n","Total params: 34,962\n","Trainable params: 34,946\n","Non-trainable params: 16\n","_________________________________________________________________\n","Epoch 1/3\n","164/164 [==============================] - 174s 1s/step - loss: 0.3466 - accuracy: 0.8490\n","Epoch 2/3\n","164/164 [==============================] - 172s 1s/step - loss: 0.1621 - accuracy: 0.9398\n","Epoch 3/3\n","164/164 [==============================] - 172s 1s/step - loss: 0.1440 - accuracy: 0.9450\n","Found 624 images belonging to 2 classes.\n","20/20 [==============================] - 20s 1s/step - loss: 0.3451 - accuracy: 0.8494\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.3450576961040497, 0.8493589758872986]"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","#from imblearn.over_sampling import RandomOverSampler\n","import numpy as np\n","from sklearn.utils import class_weight\n","\n","data = keras.preprocessing.image.ImageDataGenerator(\n","preprocessing_function=lambda x:x/255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/train\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","\n","# Calculate class weights\n","class_weights = np.array([1, 1])\n","\n","model = keras.Sequential([\n","    keras.layers.Input((1600, 1200, 1)),\n","    keras.layers.Conv2D(8, (5, 5), strides=(3, 3), activation='relu'),\n","    keras.layers.Dropout(0.3),\n","    keras.layers.BatchNormalization(),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Dropout(0.2),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Conv2D(16, (7, 7), strides=(3, 3), activation='relu'),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Conv2D(32, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Dropout(0.2),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Flatten(),\n","    keras.layers.Dense(32, activation=\"relu\"),\n","    keras.layers.Dense(2, activation=\"softmax\"),\n","])\n","\n","model.compile(\n","    optimizer=keras.optimizers.SGD(learning_rate=0.01, nesterov=True, momentum=0.90),\n","    loss=tf.keras.losses.CategoricalCrossentropy(),\n","    metrics=[\"accuracy\"]\n",")\n","\n","model.summary()\n","\n","history = model.fit(data, verbose=1, epochs=3)\n","\n","data_test = keras.preprocessing.image.ImageDataGenerator(\n","    preprocessing_function=lambda x: x / 255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/test\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","model.evaluate(data_test)\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"24wnmuxxsi5v","executionInfo":{"status":"ok","timestamp":1686056100963,"user_tz":-180,"elapsed":604531,"user":{"displayName":"YAKUP CAN KARACAOGLU","userId":"14801541947105482372"}},"outputId":"24cbc551-f5a9-4198-b60a-9c0b765191cb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 5232 images belonging to 2 classes.\n","Model: \"sequential_22\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_111 (Conv2D)         (None, 532, 399, 8)       208       \n","                                                                 \n"," dropout_49 (Dropout)        (None, 532, 399, 8)       0         \n","                                                                 \n"," batch_normalization_22 (Bat  (None, 532, 399, 8)      32        \n"," chNormalization)                                                \n","                                                                 \n"," conv2d_112 (Conv2D)         (None, 264, 198, 8)       1608      \n","                                                                 \n"," dropout_50 (Dropout)        (None, 264, 198, 8)       0         \n","                                                                 \n"," conv2d_113 (Conv2D)         (None, 130, 97, 8)        1608      \n","                                                                 \n"," conv2d_114 (Conv2D)         (None, 42, 31, 16)        6288      \n","                                                                 \n"," dropout_51 (Dropout)        (None, 42, 31, 16)        0         \n","                                                                 \n"," max_pooling2d_44 (MaxPoolin  (None, 21, 15, 16)       0         \n"," g2D)                                                            \n","                                                                 \n"," conv2d_115 (Conv2D)         (None, 9, 6, 32)          12832     \n","                                                                 \n"," dropout_52 (Dropout)        (None, 9, 6, 32)          0         \n","                                                                 \n"," max_pooling2d_45 (MaxPoolin  (None, 4, 3, 32)         0         \n"," g2D)                                                            \n","                                                                 \n"," flatten_22 (Flatten)        (None, 384)               0         \n","                                                                 \n"," dense_44 (Dense)            (None, 32)                12320     \n","                                                                 \n"," dense_45 (Dense)            (None, 2)                 66        \n","                                                                 \n","=================================================================\n","Total params: 34,962\n","Trainable params: 34,946\n","Non-trainable params: 16\n","_________________________________________________________________\n","Epoch 1/3\n","164/164 [==============================] - 179s 1s/step - loss: 0.4911 - accuracy: 0.7892\n","Epoch 2/3\n","164/164 [==============================] - 173s 1s/step - loss: 0.1790 - accuracy: 0.9354\n","Epoch 3/3\n","164/164 [==============================] - 172s 1s/step - loss: 0.1507 - accuracy: 0.9455\n","Found 624 images belonging to 2 classes.\n","20/20 [==============================] - 18s 907ms/step - loss: 0.4447 - accuracy: 0.7901\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.4447028338909149, 0.7900640964508057]"]},"metadata":{},"execution_count":25}]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","#from imblearn.over_sampling import RandomOverSampler\n","import numpy as np\n","from sklearn.utils import class_weight\n","\n","data = keras.preprocessing.image.ImageDataGenerator(\n","preprocessing_function=lambda x:x/255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/train\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","\n","# Calculate class weights\n","class_weights = np.array([1, 1])\n","\n","model = keras.Sequential([\n","    keras.layers.Input((1600, 1200, 1)),\n","    keras.layers.Conv2D(8, (5, 5), strides=(3, 3), activation='relu'),\n","    keras.layers.Dropout(0.2),\n","    keras.layers.BatchNormalization(),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Dropout(0.2),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Conv2D(16, (7, 7), strides=(3, 3), activation='relu'),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Conv2D(32, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Dropout(0.2),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Flatten(),\n","    keras.layers.Dense(32, activation=\"relu\"),\n","    keras.layers.Dense(2, activation=\"softmax\"),\n","])\n","\n","model.compile(\n","    optimizer=keras.optimizers.SGD(learning_rate=0.01, nesterov=True, momentum=0.90),\n","    loss=tf.keras.losses.CategoricalCrossentropy(),\n","    metrics=[\"accuracy\"]\n",")\n","\n","model.summary()\n","\n","history = model.fit(data, verbose=1, epochs=3)\n","\n","data_test = keras.preprocessing.image.ImageDataGenerator(\n","    preprocessing_function=lambda x: x / 255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/test\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","model.evaluate(data_test)\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6FK0rcEXvSih","executionInfo":{"status":"ok","timestamp":1686056865036,"user_tz":-180,"elapsed":545415,"user":{"displayName":"YAKUP CAN KARACAOGLU","userId":"14801541947105482372"}},"outputId":"0b30f4d8-2e95-4faf-df76-a219070b95eb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 5232 images belonging to 2 classes.\n","Model: \"sequential_23\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_116 (Conv2D)         (None, 532, 399, 8)       208       \n","                                                                 \n"," dropout_53 (Dropout)        (None, 532, 399, 8)       0         \n","                                                                 \n"," batch_normalization_23 (Bat  (None, 532, 399, 8)      32        \n"," chNormalization)                                                \n","                                                                 \n"," conv2d_117 (Conv2D)         (None, 264, 198, 8)       1608      \n","                                                                 \n"," dropout_54 (Dropout)        (None, 264, 198, 8)       0         \n","                                                                 \n"," conv2d_118 (Conv2D)         (None, 130, 97, 8)        1608      \n","                                                                 \n"," conv2d_119 (Conv2D)         (None, 42, 31, 16)        6288      \n","                                                                 \n"," dropout_55 (Dropout)        (None, 42, 31, 16)        0         \n","                                                                 \n"," max_pooling2d_46 (MaxPoolin  (None, 21, 15, 16)       0         \n"," g2D)                                                            \n","                                                                 \n"," conv2d_120 (Conv2D)         (None, 9, 6, 32)          12832     \n","                                                                 \n"," dropout_56 (Dropout)        (None, 9, 6, 32)          0         \n","                                                                 \n"," max_pooling2d_47 (MaxPoolin  (None, 4, 3, 32)         0         \n"," g2D)                                                            \n","                                                                 \n"," flatten_23 (Flatten)        (None, 384)               0         \n","                                                                 \n"," dense_46 (Dense)            (None, 32)                12320     \n","                                                                 \n"," dense_47 (Dense)            (None, 2)                 66        \n","                                                                 \n","=================================================================\n","Total params: 34,962\n","Trainable params: 34,946\n","Non-trainable params: 16\n","_________________________________________________________________\n","Epoch 1/3\n","164/164 [==============================] - 177s 1s/step - loss: 0.3758 - accuracy: 0.8379\n","Epoch 2/3\n","164/164 [==============================] - 173s 1s/step - loss: 0.1605 - accuracy: 0.9400\n","Epoch 3/3\n","164/164 [==============================] - 172s 1s/step - loss: 0.1421 - accuracy: 0.9465\n","Found 624 images belonging to 2 classes.\n","20/20 [==============================] - 20s 945ms/step - loss: 0.4971 - accuracy: 0.7837\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.49708494544029236, 0.7836538553237915]"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","#from imblearn.over_sampling import RandomOverSampler\n","import numpy as np\n","from sklearn.utils import class_weight\n","\n","data = keras.preprocessing.image.ImageDataGenerator(\n","preprocessing_function=lambda x:x/255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/train\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","\n","# Calculate class weights\n","class_weights = np.array([1, 1])\n","\n","model = keras.Sequential([\n","    keras.layers.Input((1600, 1200, 1)),\n","    keras.layers.Conv2D(8, (5, 5), strides=(3, 3), activation='relu'),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.BatchNormalization(),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Dropout(0.2),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Conv2D(16, (7, 7), strides=(3, 3), activation='relu'),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Conv2D(32, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Dropout(0.2),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Flatten(),\n","    keras.layers.Dense(32, activation=\"relu\"),\n","    keras.layers.Dense(2, activation=\"softmax\"),\n","])\n","\n","model.compile(\n","    optimizer=keras.optimizers.SGD(learning_rate=0.01, nesterov=True, momentum=0.90),\n","    loss=tf.keras.losses.CategoricalCrossentropy(),\n","    metrics=[\"accuracy\"]\n",")\n","\n","model.summary()\n","\n","history = model.fit(data, verbose=1, epochs=2)\n","\n","data_test = keras.preprocessing.image.ImageDataGenerator(\n","    preprocessing_function=lambda x: x / 255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/test\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","model.evaluate(data_test)\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"irucOvqBybgp","executionInfo":{"status":"ok","timestamp":1686057359113,"user_tz":-180,"elapsed":430996,"user":{"displayName":"YAKUP CAN KARACAOGLU","userId":"14801541947105482372"}},"outputId":"04e24d00-0948-4f1e-8320-77f8ff1d8148"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 5232 images belonging to 2 classes.\n","Model: \"sequential_24\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_121 (Conv2D)         (None, 532, 399, 8)       208       \n","                                                                 \n"," dropout_57 (Dropout)        (None, 532, 399, 8)       0         \n","                                                                 \n"," batch_normalization_24 (Bat  (None, 532, 399, 8)      32        \n"," chNormalization)                                                \n","                                                                 \n"," conv2d_122 (Conv2D)         (None, 264, 198, 8)       1608      \n","                                                                 \n"," dropout_58 (Dropout)        (None, 264, 198, 8)       0         \n","                                                                 \n"," conv2d_123 (Conv2D)         (None, 130, 97, 8)        1608      \n","                                                                 \n"," conv2d_124 (Conv2D)         (None, 42, 31, 16)        6288      \n","                                                                 \n"," dropout_59 (Dropout)        (None, 42, 31, 16)        0         \n","                                                                 \n"," max_pooling2d_48 (MaxPoolin  (None, 21, 15, 16)       0         \n"," g2D)                                                            \n","                                                                 \n"," conv2d_125 (Conv2D)         (None, 9, 6, 32)          12832     \n","                                                                 \n"," dropout_60 (Dropout)        (None, 9, 6, 32)          0         \n","                                                                 \n"," max_pooling2d_49 (MaxPoolin  (None, 4, 3, 32)         0         \n"," g2D)                                                            \n","                                                                 \n"," flatten_24 (Flatten)        (None, 384)               0         \n","                                                                 \n"," dense_48 (Dense)            (None, 32)                12320     \n","                                                                 \n"," dense_49 (Dense)            (None, 2)                 66        \n","                                                                 \n","=================================================================\n","Total params: 34,962\n","Trainable params: 34,946\n","Non-trainable params: 16\n","_________________________________________________________________\n","Epoch 1/2\n","164/164 [==============================] - 177s 1s/step - loss: 0.3225 - accuracy: 0.8609\n","Epoch 2/2\n","164/164 [==============================] - 183s 1s/step - loss: 0.3606 - accuracy: 0.8588\n","Found 624 images belonging to 2 classes.\n","20/20 [==============================] - 21s 1s/step - loss: 0.6845 - accuracy: 0.7340\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.6845448613166809, 0.7339743375778198]"]},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","#from imblearn.over_sampling import RandomOverSampler\n","import numpy as np\n","from sklearn.utils import class_weight\n","\n","data = keras.preprocessing.image.ImageDataGenerator(\n","preprocessing_function=lambda x:x/255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/train\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","\n","# Calculate class weights\n","class_weights = np.array([1, 1])\n","\n","model = keras.Sequential([\n","    keras.layers.Input((1600, 1200, 1)),\n","    keras.layers.Conv2D(8, (5, 5), strides=(3, 3), activation='relu'),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.BatchNormalization(),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Dropout(0.2),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Conv2D(16, (7, 7), strides=(3, 3), activation='relu'),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Conv2D(32, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Dropout(0.2),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Flatten(),\n","    keras.layers.Dense(32, activation=\"relu\"),\n","    keras.layers.Dense(2, activation=\"softmax\"),\n","])\n","\n","model.compile(\n","    optimizer=keras.optimizers.SGD(learning_rate=0.01, nesterov=True, momentum=0.90),\n","    loss=tf.keras.losses.CategoricalCrossentropy(),\n","    metrics=[\"accuracy\"]\n",")\n","\n","model.summary()\n","\n","history = model.fit(data, verbose=1, epochs=4)\n","\n","data_test = keras.preprocessing.image.ImageDataGenerator(\n","    preprocessing_function=lambda x: x / 255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/test\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","model.evaluate(data_test)\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CRLInCCj0wEQ","outputId":"e66d603f-a784-472d-ffaa-813a3786e050"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 5232 images belonging to 2 classes.\n","Model: \"sequential_25\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_126 (Conv2D)         (None, 532, 399, 8)       208       \n","                                                                 \n"," dropout_61 (Dropout)        (None, 532, 399, 8)       0         \n","                                                                 \n"," batch_normalization_25 (Bat  (None, 532, 399, 8)      32        \n"," chNormalization)                                                \n","                                                                 \n"," conv2d_127 (Conv2D)         (None, 264, 198, 8)       1608      \n","                                                                 \n"," dropout_62 (Dropout)        (None, 264, 198, 8)       0         \n","                                                                 \n"," conv2d_128 (Conv2D)         (None, 130, 97, 8)        1608      \n","                                                                 \n"," conv2d_129 (Conv2D)         (None, 42, 31, 16)        6288      \n","                                                                 \n"," dropout_63 (Dropout)        (None, 42, 31, 16)        0         \n","                                                                 \n"," max_pooling2d_50 (MaxPoolin  (None, 21, 15, 16)       0         \n"," g2D)                                                            \n","                                                                 \n"," conv2d_130 (Conv2D)         (None, 9, 6, 32)          12832     \n","                                                                 \n"," dropout_64 (Dropout)        (None, 9, 6, 32)          0         \n","                                                                 \n"," max_pooling2d_51 (MaxPoolin  (None, 4, 3, 32)         0         \n"," g2D)                                                            \n","                                                                 \n"," flatten_25 (Flatten)        (None, 384)               0         \n","                                                                 \n"," dense_50 (Dense)            (None, 32)                12320     \n","                                                                 \n"," dense_51 (Dense)            (None, 2)                 66        \n","                                                                 \n","=================================================================\n","Total params: 34,962\n","Trainable params: 34,946\n","Non-trainable params: 16\n","_________________________________________________________________\n","Epoch 1/4\n","164/164 [==============================] - 186s 1s/step - loss: 0.3388 - accuracy: 0.8542\n","Epoch 2/4\n","164/164 [==============================] - 179s 1s/step - loss: 0.1507 - accuracy: 0.9427\n"]}]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","#from imblearn.over_sampling import RandomOverSampler\n","import numpy as np\n","from sklearn.utils import class_weight\n","\n","data = keras.preprocessing.image.ImageDataGenerator(\n","preprocessing_function=lambda x:x/255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/train\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","\n","# Calculate class weights\n","class_weights = np.array([1, 1])\n","\n","model = keras.Sequential([\n","    keras.layers.Input((1600, 1200, 1)),\n","    keras.layers.Conv2D(8, (5, 5), strides=(3, 3), activation='relu'),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.BatchNormalization(),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Dropout(0.2),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Conv2D(16, (7, 7), strides=(3, 3), activation='relu'),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Conv2D(32, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Dropout(0.2),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Flatten(),\n","    keras.layers.Dense(32, activation=\"relu\"),\n","    keras.layers.Dense(2, activation=\"softmax\"),\n","])\n","\n","model.compile(\n","    optimizer=keras.optimizers.SGD(learning_rate=0.01, nesterov=True, momentum=0.90),\n","    loss=tf.keras.losses.CategoricalCrossentropy(),\n","    metrics=[\"accuracy\"]\n",")\n","\n","model.summary()\n","\n","history = model.fit(data, verbose=1, epochs=3)\n","\n","data_test = keras.preprocessing.image.ImageDataGenerator(\n","    preprocessing_function=lambda x: x / 255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/test\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","model.evaluate(data_test)\n","\n","\n","\n"],"metadata":{"id":"c5rQiiqg0yGi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686408933956,"user_tz":-180,"elapsed":1644109,"user":{"displayName":"YAKUP CAN KARACAOGLU","userId":"14801541947105482372"}},"outputId":"3d3485ac-aa5b-4ca9-a0f4-02be0936d531"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 5232 images belonging to 2 classes.\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d (Conv2D)             (None, 532, 399, 8)       208       \n","                                                                 \n"," dropout (Dropout)           (None, 532, 399, 8)       0         \n","                                                                 \n"," batch_normalization (BatchN  (None, 532, 399, 8)      32        \n"," ormalization)                                                   \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 264, 198, 8)       1608      \n","                                                                 \n"," dropout_1 (Dropout)         (None, 264, 198, 8)       0         \n","                                                                 \n"," conv2d_2 (Conv2D)           (None, 130, 97, 8)        1608      \n","                                                                 \n"," conv2d_3 (Conv2D)           (None, 42, 31, 16)        6288      \n","                                                                 \n"," dropout_2 (Dropout)         (None, 42, 31, 16)        0         \n","                                                                 \n"," max_pooling2d (MaxPooling2D  (None, 21, 15, 16)       0         \n"," )                                                               \n","                                                                 \n"," conv2d_4 (Conv2D)           (None, 9, 6, 32)          12832     \n","                                                                 \n"," dropout_3 (Dropout)         (None, 9, 6, 32)          0         \n","                                                                 \n"," max_pooling2d_1 (MaxPooling  (None, 4, 3, 32)         0         \n"," 2D)                                                             \n","                                                                 \n"," flatten (Flatten)           (None, 384)               0         \n","                                                                 \n"," dense (Dense)               (None, 32)                12320     \n","                                                                 \n"," dense_1 (Dense)             (None, 2)                 66        \n","                                                                 \n","=================================================================\n","Total params: 34,962\n","Trainable params: 34,946\n","Non-trainable params: 16\n","_________________________________________________________________\n","Epoch 1/3\n","164/164 [==============================] - 1066s 6s/step - loss: 0.4252 - accuracy: 0.8167\n","Epoch 2/3\n","164/164 [==============================] - 163s 991ms/step - loss: 0.1752 - accuracy: 0.9360\n","Epoch 3/3\n","164/164 [==============================] - 161s 981ms/step - loss: 0.1362 - accuracy: 0.9476\n","Found 624 images belonging to 2 classes.\n","20/20 [==============================] - 160s 8s/step - loss: 0.5927 - accuracy: 0.7933\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.5926761031150818, 0.7932692170143127]"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","#from imblearn.over_sampling import RandomOverSampler\n","import numpy as np\n","from sklearn.utils import class_weight\n","\n","data = keras.preprocessing.image.ImageDataGenerator(\n","preprocessing_function=lambda x:x/255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/train\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","\n","# Calculate class weights\n","class_weights = np.array([1, 1])\n","\n","model = keras.Sequential([\n","    keras.layers.Input((1600, 1200, 1)),\n","    keras.layers.Conv2D(8, (5, 5), strides=(3, 3), activation='relu'),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.BatchNormalization(),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Dropout(0.2),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Conv2D(16, (7, 7), strides=(3, 3), activation='relu'),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Conv2D(32, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Dropout(0.2),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Flatten(),\n","    keras.layers.Dense(32, activation=\"relu\"),\n","    keras.layers.Dense(2, activation=\"softmax\"),\n","])\n","\n","model.compile(\n","    optimizer=keras.optimizers.SGD(learning_rate=0.01, nesterov=True, momentum=0.90),\n","    loss=tf.keras.losses.CategoricalCrossentropy(),\n","    metrics=[\"accuracy\"]\n",")\n","\n","model.summary()\n","\n","history = model.fit(data, verbose=1, epochs=3)\n","\n","data_test = keras.preprocessing.image.ImageDataGenerator(\n","    preprocessing_function=lambda x: x / 255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/test\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","model.evaluate(data_test)\n","\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GEvubbyYRuJl","executionInfo":{"status":"ok","timestamp":1686409567078,"user_tz":-180,"elapsed":512739,"user":{"displayName":"YAKUP CAN KARACAOGLU","userId":"14801541947105482372"}},"outputId":"7bf59100-610f-42d4-c8ff-27960c1abb6e"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 5232 images belonging to 2 classes.\n","Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_5 (Conv2D)           (None, 532, 399, 8)       208       \n","                                                                 \n"," dropout_4 (Dropout)         (None, 532, 399, 8)       0         \n","                                                                 \n"," batch_normalization_1 (Batc  (None, 532, 399, 8)      32        \n"," hNormalization)                                                 \n","                                                                 \n"," conv2d_6 (Conv2D)           (None, 264, 198, 8)       1608      \n","                                                                 \n"," dropout_5 (Dropout)         (None, 264, 198, 8)       0         \n","                                                                 \n"," conv2d_7 (Conv2D)           (None, 130, 97, 8)        1608      \n","                                                                 \n"," conv2d_8 (Conv2D)           (None, 42, 31, 16)        6288      \n","                                                                 \n"," dropout_6 (Dropout)         (None, 42, 31, 16)        0         \n","                                                                 \n"," max_pooling2d_2 (MaxPooling  (None, 21, 15, 16)       0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_9 (Conv2D)           (None, 9, 6, 32)          12832     \n","                                                                 \n"," dropout_7 (Dropout)         (None, 9, 6, 32)          0         \n","                                                                 \n"," max_pooling2d_3 (MaxPooling  (None, 4, 3, 32)         0         \n"," 2D)                                                             \n","                                                                 \n"," flatten_1 (Flatten)         (None, 384)               0         \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                12320     \n","                                                                 \n"," dense_3 (Dense)             (None, 2)                 66        \n","                                                                 \n","=================================================================\n","Total params: 34,962\n","Trainable params: 34,946\n","Non-trainable params: 16\n","_________________________________________________________________\n","Epoch 1/3\n","164/164 [==============================] - 163s 970ms/step - loss: 0.3350 - accuracy: 0.8605\n","Epoch 2/3\n","164/164 [==============================] - 162s 989ms/step - loss: 0.1563 - accuracy: 0.9407\n","Epoch 3/3\n","164/164 [==============================] - 164s 998ms/step - loss: 0.1334 - accuracy: 0.9520\n","Found 624 images belonging to 2 classes.\n","20/20 [==============================] - 18s 905ms/step - loss: 0.4490 - accuracy: 0.7821\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.4489918351173401, 0.7820512652397156]"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","#from imblearn.over_sampling import RandomOverSampler\n","import numpy as np\n","from sklearn.utils import class_weight\n","\n","data = keras.preprocessing.image.ImageDataGenerator(\n","preprocessing_function=lambda x:x/255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/train\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","\n","# Calculate class weights\n","class_weights = np.array([1, 1])\n","\n","model = keras.Sequential([\n","    keras.layers.Input((1600, 1200, 1)),\n","    keras.layers.Conv2D(8, (5, 5), strides=(3, 3), activation='relu'),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.BatchNormalization(),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Dropout(0.2),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Conv2D(16, (7, 7), strides=(3, 3), activation='relu'),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Conv2D(32, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Dropout(0.2),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Flatten(),\n","    keras.layers.Dense(32, activation=\"relu\"),\n","    keras.layers.Dense(2, activation=\"softmax\"),\n","])\n","\n","model.compile(\n","    optimizer=keras.optimizers.SGD(learning_rate=0.01, nesterov=True, momentum=0.90),\n","    loss=tf.keras.losses.CategoricalCrossentropy(),\n","    metrics=[\"accuracy\"]\n",")\n","\n","model.summary()\n","\n","history = model.fit(data, verbose=1, epochs=3)\n","\n","data_test = keras.preprocessing.image.ImageDataGenerator(\n","    preprocessing_function=lambda x: x / 255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/test\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=False,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","model.evaluate(data_test)\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fv0ShR-i2AEU","executionInfo":{"status":"ok","timestamp":1686410143722,"user_tz":-180,"elapsed":511549,"user":{"displayName":"YAKUP CAN KARACAOGLU","userId":"14801541947105482372"}},"outputId":"23e86a57-83ef-49af-cb79-35ac0b834381"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 5232 images belonging to 2 classes.\n","Model: \"sequential_3\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_15 (Conv2D)          (None, 532, 399, 8)       208       \n","                                                                 \n"," dropout_12 (Dropout)        (None, 532, 399, 8)       0         \n","                                                                 \n"," batch_normalization_3 (Batc  (None, 532, 399, 8)      32        \n"," hNormalization)                                                 \n","                                                                 \n"," conv2d_16 (Conv2D)          (None, 264, 198, 8)       1608      \n","                                                                 \n"," dropout_13 (Dropout)        (None, 264, 198, 8)       0         \n","                                                                 \n"," conv2d_17 (Conv2D)          (None, 130, 97, 8)        1608      \n","                                                                 \n"," conv2d_18 (Conv2D)          (None, 42, 31, 16)        6288      \n","                                                                 \n"," dropout_14 (Dropout)        (None, 42, 31, 16)        0         \n","                                                                 \n"," max_pooling2d_6 (MaxPooling  (None, 21, 15, 16)       0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_19 (Conv2D)          (None, 9, 6, 32)          12832     \n","                                                                 \n"," dropout_15 (Dropout)        (None, 9, 6, 32)          0         \n","                                                                 \n"," max_pooling2d_7 (MaxPooling  (None, 4, 3, 32)         0         \n"," 2D)                                                             \n","                                                                 \n"," flatten_3 (Flatten)         (None, 384)               0         \n","                                                                 \n"," dense_6 (Dense)             (None, 32)                12320     \n","                                                                 \n"," dense_7 (Dense)             (None, 2)                 66        \n","                                                                 \n","=================================================================\n","Total params: 34,962\n","Trainable params: 34,946\n","Non-trainable params: 16\n","_________________________________________________________________\n","Epoch 1/3\n","164/164 [==============================] - 164s 987ms/step - loss: 0.5795 - accuracy: 0.7403\n","Epoch 2/3\n","164/164 [==============================] - 164s 1s/step - loss: 0.5721 - accuracy: 0.7443\n","Epoch 3/3\n","164/164 [==============================] - 163s 992ms/step - loss: 0.5687 - accuracy: 0.7422\n","Found 624 images belonging to 2 classes.\n","20/20 [==============================] - 17s 821ms/step - loss: 0.6757 - accuracy: 0.6250\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.6757498979568481, 0.625]"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","#from imblearn.over_sampling import RandomOverSampler\n","import numpy as np\n","from sklearn.utils import class_weight\n","\n","data = keras.preprocessing.image.ImageDataGenerator(\n","preprocessing_function=lambda x:x/255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/train\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=False,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","\n","# Calculate class weights\n","class_weights = np.array([1, 1])\n","\n","model = keras.Sequential([\n","    keras.layers.Input((1600, 1200, 1)),\n","    keras.layers.Conv2D(8, (5, 5), strides=(3, 3), activation='relu'),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.BatchNormalization(),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Dropout(0.2),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Conv2D(16, (7, 7), strides=(3, 3), activation='relu'),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Conv2D(32, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Dropout(0.2),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Flatten(),\n","    keras.layers.Dense(32, activation=\"relu\"),\n","    keras.layers.Dense(2, activation=\"softmax\"),\n","])\n","\n","model.compile(\n","    optimizer=keras.optimizers.SGD(learning_rate=0.01, nesterov=True, momentum=0.90),\n","    loss=tf.keras.losses.CategoricalCrossentropy(),\n","    metrics=[\"accuracy\"]\n",")\n","\n","model.summary()\n","\n","history = model.fit(data, verbose=1, epochs=3)\n","\n","data_test = keras.preprocessing.image.ImageDataGenerator(\n","    preprocessing_function=lambda x: x / 255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/test\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=False,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","model.evaluate(data_test)\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y36Lc9uo2NUi","executionInfo":{"status":"ok","timestamp":1686410652496,"user_tz":-180,"elapsed":507470,"user":{"displayName":"YAKUP CAN KARACAOGLU","userId":"14801541947105482372"}},"outputId":"ce10c854-75f6-41ca-cfd6-ca6a46d7baea"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 5232 images belonging to 2 classes.\n","Model: \"sequential_4\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_20 (Conv2D)          (None, 532, 399, 8)       208       \n","                                                                 \n"," dropout_16 (Dropout)        (None, 532, 399, 8)       0         \n","                                                                 \n"," batch_normalization_4 (Batc  (None, 532, 399, 8)      32        \n"," hNormalization)                                                 \n","                                                                 \n"," conv2d_21 (Conv2D)          (None, 264, 198, 8)       1608      \n","                                                                 \n"," dropout_17 (Dropout)        (None, 264, 198, 8)       0         \n","                                                                 \n"," conv2d_22 (Conv2D)          (None, 130, 97, 8)        1608      \n","                                                                 \n"," conv2d_23 (Conv2D)          (None, 42, 31, 16)        6288      \n","                                                                 \n"," dropout_18 (Dropout)        (None, 42, 31, 16)        0         \n","                                                                 \n"," max_pooling2d_8 (MaxPooling  (None, 21, 15, 16)       0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_24 (Conv2D)          (None, 9, 6, 32)          12832     \n","                                                                 \n"," dropout_19 (Dropout)        (None, 9, 6, 32)          0         \n","                                                                 \n"," max_pooling2d_9 (MaxPooling  (None, 4, 3, 32)         0         \n"," 2D)                                                             \n","                                                                 \n"," flatten_4 (Flatten)         (None, 384)               0         \n","                                                                 \n"," dense_8 (Dense)             (None, 32)                12320     \n","                                                                 \n"," dense_9 (Dense)             (None, 2)                 66        \n","                                                                 \n","=================================================================\n","Total params: 34,962\n","Trainable params: 34,946\n","Non-trainable params: 16\n","_________________________________________________________________\n","Epoch 1/3\n","164/164 [==============================] - 161s 971ms/step - loss: 0.6717 - accuracy: 0.7422\n","Epoch 2/3\n","164/164 [==============================] - 159s 972ms/step - loss: 0.5772 - accuracy: 0.7422\n","Epoch 3/3\n","164/164 [==============================] - 162s 988ms/step - loss: 0.5979 - accuracy: 0.7422\n","Found 624 images belonging to 2 classes.\n","20/20 [==============================] - 17s 838ms/step - loss: 0.6910 - accuracy: 0.6250\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.6909792423248291, 0.625]"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","#from imblearn.over_sampling import RandomOverSampler\n","import numpy as np\n","from sklearn.utils import class_weight\n","\n","data = keras.preprocessing.image.ImageDataGenerator(\n","preprocessing_function=lambda x:x/255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/train\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","\n","# Calculate class weights\n","class_weights = np.array([1, 1])\n","\n","model = keras.Sequential([\n","    keras.layers.Input((1600, 1200, 1)),\n","    keras.layers.Conv2D(8, (5, 5), strides=(3, 3), activation='relu'),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.BatchNormalization(),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Dropout(0.2),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Conv2D(16, (7, 7), strides=(3, 3), activation='relu'),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Conv2D(32, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Dropout(0.2),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Flatten(),\n","    keras.layers.Dense(32, activation=\"relu\"),\n","    keras.layers.Dense(2, activation=\"softmax\"),\n","])\n","\n","model.compile(\n","    optimizer=keras.optimizers.SGD(learning_rate=0.01, nesterov=True, momentum=0.90),\n","    loss=tf.keras.losses.CategoricalCrossentropy(),\n","    metrics=[\"accuracy\"]\n",")\n","\n","model.summary()\n","\n","history = model.fit(data, verbose=1, epochs=3)\n","\n","data_test = keras.preprocessing.image.ImageDataGenerator(\n","    preprocessing_function=lambda x: x / 255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/test\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","model.evaluate(data_test)\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0ZY8-ai-33w4","executionInfo":{"status":"ok","timestamp":1686411180080,"user_tz":-180,"elapsed":501677,"user":{"displayName":"YAKUP CAN KARACAOGLU","userId":"14801541947105482372"}},"outputId":"333a7abb-e6b5-4cea-8870-6e0ed85b384c"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 5232 images belonging to 2 classes.\n","Model: \"sequential_5\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_25 (Conv2D)          (None, 532, 399, 8)       208       \n","                                                                 \n"," dropout_20 (Dropout)        (None, 532, 399, 8)       0         \n","                                                                 \n"," batch_normalization_5 (Batc  (None, 532, 399, 8)      32        \n"," hNormalization)                                                 \n","                                                                 \n"," conv2d_26 (Conv2D)          (None, 264, 198, 8)       1608      \n","                                                                 \n"," dropout_21 (Dropout)        (None, 264, 198, 8)       0         \n","                                                                 \n"," conv2d_27 (Conv2D)          (None, 130, 97, 8)        1608      \n","                                                                 \n"," conv2d_28 (Conv2D)          (None, 42, 31, 16)        6288      \n","                                                                 \n"," dropout_22 (Dropout)        (None, 42, 31, 16)        0         \n","                                                                 \n"," max_pooling2d_10 (MaxPoolin  (None, 21, 15, 16)       0         \n"," g2D)                                                            \n","                                                                 \n"," conv2d_29 (Conv2D)          (None, 9, 6, 32)          12832     \n","                                                                 \n"," dropout_23 (Dropout)        (None, 9, 6, 32)          0         \n","                                                                 \n"," max_pooling2d_11 (MaxPoolin  (None, 4, 3, 32)         0         \n"," g2D)                                                            \n","                                                                 \n"," flatten_5 (Flatten)         (None, 384)               0         \n","                                                                 \n"," dense_10 (Dense)            (None, 32)                12320     \n","                                                                 \n"," dense_11 (Dense)            (None, 2)                 66        \n","                                                                 \n","=================================================================\n","Total params: 34,962\n","Trainable params: 34,946\n","Non-trainable params: 16\n","_________________________________________________________________\n","Epoch 1/3\n","164/164 [==============================] - 163s 979ms/step - loss: 0.4925 - accuracy: 0.7789\n","Epoch 2/3\n","164/164 [==============================] - 159s 970ms/step - loss: 0.2372 - accuracy: 0.9079\n","Epoch 3/3\n","164/164 [==============================] - 158s 962ms/step - loss: 0.1751 - accuracy: 0.9316\n","Found 624 images belonging to 2 classes.\n","20/20 [==============================] - 18s 853ms/step - loss: 0.8070 - accuracy: 0.6939\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.8070391416549683, 0.6939102411270142]"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","#from imblearn.over_sampling import RandomOverSampler\n","import numpy as np\n","from sklearn.utils import class_weight\n","\n","data = keras.preprocessing.image.ImageDataGenerator(\n","preprocessing_function=lambda x:x/255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/train\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","\n","# Calculate class weights\n","class_weights = np.array([1, 1])\n","\n","model = keras.Sequential([\n","    keras.layers.Input((1600, 1200, 1)),\n","    keras.layers.Conv2D(8, (5, 5), strides=(3, 3), activation='relu'),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.BatchNormalization(),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Dropout(0.2),\n","    keras.layers.Conv2D(8, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Conv2D(16, (7, 7), strides=(3, 3), activation='relu'),\n","    keras.layers.Dropout(0.1),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Conv2D(32, (5, 5), strides=(2, 2), activation='relu'),\n","    keras.layers.Dropout(0.2),\n","    keras.layers.MaxPool2D((2, 2)),\n","    keras.layers.Flatten(),\n","    keras.layers.Dense(32, activation=\"relu\"),\n","    keras.layers.Dense(2, activation=\"softmax\"),\n","])\n","\n","model.compile(\n","    optimizer=keras.optimizers.SGD(learning_rate=0.01, nesterov=True, momentum=0.90),\n","    loss=tf.keras.losses.CategoricalCrossentropy(),\n","    metrics=[\"accuracy\"]\n",")\n","\n","model.summary()\n","\n","history = model.fit(data, verbose=1, epochs=4)\n","\n","data_test = keras.preprocessing.image.ImageDataGenerator(\n","    preprocessing_function=lambda x: x / 255\n",").flow_from_directory(\n","    \"/content/gdrive/MyDrive/chest/datai2/chest_xray/test\",\n","    target_size=(1600, 1200),\n","    color_mode='grayscale',\n","    classes=None,\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix='',\n","    save_format='png',\n","    follow_links=False,\n","    subset=None,\n","    interpolation='nearest',\n","    keep_aspect_ratio=False\n",")\n","\n","model.evaluate(data_test)\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eDIzd11G6Mxk","executionInfo":{"status":"ok","timestamp":1686411953488,"user_tz":-180,"elapsed":708231,"user":{"displayName":"YAKUP CAN KARACAOGLU","userId":"14801541947105482372"}},"outputId":"34e27d3e-4b30-497a-c3c1-81ecee8a878f"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 5232 images belonging to 2 classes.\n","Model: \"sequential_6\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_30 (Conv2D)          (None, 532, 399, 8)       208       \n","                                                                 \n"," dropout_24 (Dropout)        (None, 532, 399, 8)       0         \n","                                                                 \n"," batch_normalization_6 (Batc  (None, 532, 399, 8)      32        \n"," hNormalization)                                                 \n","                                                                 \n"," conv2d_31 (Conv2D)          (None, 264, 198, 8)       1608      \n","                                                                 \n"," dropout_25 (Dropout)        (None, 264, 198, 8)       0         \n","                                                                 \n"," conv2d_32 (Conv2D)          (None, 130, 97, 8)        1608      \n","                                                                 \n"," conv2d_33 (Conv2D)          (None, 42, 31, 16)        6288      \n","                                                                 \n"," dropout_26 (Dropout)        (None, 42, 31, 16)        0         \n","                                                                 \n"," max_pooling2d_12 (MaxPoolin  (None, 21, 15, 16)       0         \n"," g2D)                                                            \n","                                                                 \n"," conv2d_34 (Conv2D)          (None, 9, 6, 32)          12832     \n","                                                                 \n"," dropout_27 (Dropout)        (None, 9, 6, 32)          0         \n","                                                                 \n"," max_pooling2d_13 (MaxPoolin  (None, 4, 3, 32)         0         \n"," g2D)                                                            \n","                                                                 \n"," flatten_6 (Flatten)         (None, 384)               0         \n","                                                                 \n"," dense_12 (Dense)            (None, 32)                12320     \n","                                                                 \n"," dense_13 (Dense)            (None, 2)                 66        \n","                                                                 \n","=================================================================\n","Total params: 34,962\n","Trainable params: 34,946\n","Non-trainable params: 16\n","_________________________________________________________________\n","Epoch 1/4\n","164/164 [==============================] - 162s 966ms/step - loss: 0.4207 - accuracy: 0.8230\n","Epoch 2/4\n","164/164 [==============================] - 160s 977ms/step - loss: 0.1571 - accuracy: 0.9400\n","Epoch 3/4\n","164/164 [==============================] - 160s 978ms/step - loss: 0.1377 - accuracy: 0.9513\n","Epoch 4/4\n","164/164 [==============================] - 160s 975ms/step - loss: 0.1334 - accuracy: 0.9507\n","Found 624 images belonging to 2 classes.\n","20/20 [==============================] - 18s 883ms/step - loss: 0.3936 - accuracy: 0.8333\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.39356017112731934, 0.8333333134651184]"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["model.save('/content/gdrive/MyDrive/chest/model2/model.h5')"],"metadata":{"id":"nop8DZiy8XHN","executionInfo":{"status":"ok","timestamp":1686412418587,"user_tz":-180,"elapsed":9,"user":{"displayName":"YAKUP CAN KARACAOGLU","userId":"14801541947105482372"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"xY6IIBQg_n6T"},"execution_count":null,"outputs":[]}]}